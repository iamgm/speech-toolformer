{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VGR2chCPm0s"
      },
      "source": [
        "# Magic Paste\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –û–ø–∏—Å–∞–Ω–∏–µ"
      ],
      "metadata": {
        "id": "cbu7FWxpKI5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **–ö–æ–Ω—Ü–µ–ø—Ü–∏—è**  \n",
        "–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è \"–±–µ—Å—à–æ–≤–Ω–æ–π\" —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º, —Ä–∞—Å—à–∏—Ä—è—é—â–∏–π  \n",
        "–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –≤–≤–æ–¥–∞ –≤ –ª—é–±–æ–π —Å—Ä–µ–¥–µ –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã.  \n",
        "\n",
        "### **–ü—Ä–æ–±–ª–µ–º–∞**  \n",
        "–ü—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –≤–Ω–µ—à–Ω–∏—Ö LLM-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ (ChatGPT, –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–∏, –∫–æ—Ä—Ä–µ–∫—Ç–æ—Ä—ã)   \n",
        "–ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å—Ç–∞–ª–∫–∏–≤–∞–µ—Ç—Å—è —Å –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–º –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ü—Ä–æ—Ü–µ—Å—Å  \n",
        "\"–≤—ã–¥–µ–ª–∏—Ç—å - —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å - –ø–µ—Ä–µ–∫–ª—é—á–∏—Ç—å –æ–∫–Ω–æ - –≤—Å—Ç–∞–≤–∏—Ç—å - –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç -  \n",
        "—Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å - –≤–µ—Ä–Ω—É—Ç—å—Å—è - –≤—Å—Ç–∞–≤–∏—Ç—å\" –Ω–∞—Ä—É—à–∞–µ—Ç —Ä–∞–±–æ—á–∏–π –ø–æ—Ç–æ–∫ –∏ —Å–Ω–∏–∂–∞–µ—Ç  \n",
        "–∫–æ–Ω—Ü–µ–Ω—Ç—Ä–∞—Ü–∏—é.\n",
        "\n",
        "### **–¢–µ—Ö–Ω–∏—á–µ—Å–∫–æ–µ —Ä–µ—à–µ–Ω–∏–µ**  \n",
        "Magic Paste –∏–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏ `Gemma-3n E4B Instruct`  \n",
        "–Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –∞–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —á–µ—Ä–µ–∑ **–ø–ª–∞–≤–∞—é—â–∏–π –æ–≤–µ—Ä–ª–µ–π–Ω—ã–π**  \n",
        "**–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å**.  \n",
        "–ö–ª—é—á–µ–≤–∞—è –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ - –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ **–±–µ–∑ –∑–∞—Ö–≤–∞—Ç–∞ —Ñ–æ–∫—É—Å–∞**: –æ–∫–Ω–æ  \n",
        "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤–∏–∑—É–∞–ª—å–Ω–æ –¥–æ—Å—Ç—É–ø–Ω–æ, –Ω–æ —Å–∏—Å—Ç–µ–º–Ω—ã–π –∫—É—Ä—Å–æ—Ä –∏ —Ñ–æ–∫—É—Å –≤–≤–æ–¥–∞ –æ—Å—Ç–∞—é—Ç—Å—è –≤  \n",
        "–æ—Å–Ω–æ–≤–Ω–æ–º —Ä–∞–±–æ—á–µ–º –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏.\n",
        "\n",
        "### **–°—Ü–µ–Ω–∞—Ä–∏–π –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è**  \n",
        "1.  **–ê–∫—Ç–∏–≤–∞—Ü–∏—è:** –†–∞–±–æ—Ç–∞—è –≤ IDE, –±—Ä–∞—É–∑–µ—Ä–µ –∏–ª–∏ –º–µ—Å—Å–µ–Ω–¥–∂–µ—Ä–µ, –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å  \n",
        "–≤—ã–∑—ã–≤–∞–µ—Ç –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≥–æ–ª–æ—Å–æ–≤–æ–π –∫–æ–º–∞–Ω–¥–æ–π –∏–ª–∏ —Ö–æ—Ç–∫–µ–µ–º. –ü–æ—è–≤–ª—è–µ—Ç—Å—è –Ω–µ–Ω–∞–≤—è–∑—á–∏–≤–æ–µ  \n",
        "–¥–∏–∞–ª–æ–≥–æ–≤–æ–µ –æ–∫–Ω–æ –ø–æ–≤–µ—Ä—Ö –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å–∞.  \n",
        "2.  **–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –í–≤–æ–¥:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –Ω–∞–¥–∏–∫—Ç–æ–≤—ã–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –≥–æ–ª–æ—Å–æ–º *–ª–∏–±–æ*  \n",
        "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ—Ç —É–∂–µ –≤—ã–¥–µ–ª–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ –∞–∫—Ç–∏–≤–Ω–æ–º –æ–∫–Ω–µ.  \n",
        "3.  **–û–±—Ä–∞–±–æ—Ç–∫–∞:** –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≥–æ–ª–æ—Å–æ–º –æ—Ç–¥–∞–µ—Ç –∫–æ–º–∞–Ω–¥—É –Ω–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—é  \n",
        "(–Ω–∞–ø—Ä–∏–º–µ—Ä: *\"–°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º\"*, *\"–ò—Å–ø—Ä–∞–≤—å –±–∞–≥–∏ –≤ –∫–æ–¥–µ\"*,  \n",
        "*\"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\"*).  \n",
        "4.  **–†–µ–∑—É–ª—å—Ç–∞—Ç:** Magic Paste –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–ø—Ä–æ—Å –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤—Å—Ç–∞–≤–ª—è–µ—Ç  \n",
        "–≥–æ—Ç–æ–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –ø–æ–∑–∏—Ü–∏—é –∫—É—Ä—Å–æ—Ä–∞ –∞–∫—Ç–∏–≤–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è.  \n",
        "\n",
        "\n",
        "### **–ò—Ç–æ–≥:**\n",
        "–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —É—Å—Ç—Ä–∞–Ω—è–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç—å —Ä—É—á–Ω–æ–≥–æ –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è  \n",
        "–æ–∫–æ–Ω, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—è –µ–¥–∏–Ω—ã–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ò–ò –≤ –ª—é–±–æ–π —Ç–æ—á–∫–µ —Å–∏—Å—Ç–µ–º—ã.  \n",
        "\n"
      ],
      "metadata": {
        "id": "B4NcSsJBKDRS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duBX6ZuEzqPU"
      },
      "source": [
        "## ‚ö° –ë–∏–±–ª–∏–æ—Ç–µ–∫–∏, –º–æ–¥–µ–ª—å, –¥–∞–Ω–Ω—ã–µ, —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kp8SmJ_Q4ODb"
      },
      "source": [
        "### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MhUPvKJD4JmY"
      },
      "outputs": [],
      "source": [
        "# ‚ö†Ô∏è –†–∞–±–æ—Ç–∞—é—â–∏–π –∫–æ–Ω—Ñ–∏–≥. –ù–µ –º–µ–Ω—è—Ç—å!\n",
        "\n",
        "%%capture\n",
        "import os, re\n",
        "# –¥–ª—è torchcodec –Ω—É–∂–µ–Ω FFmpeg 7+\n",
        "!add-apt-repository -y ppa:ubuntuhandbook1/ffmpeg8\n",
        "!apt-get update && apt-get install -y ffmpeg libavutil-dev libavcodec-dev libavformat-dev\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    import torch; v = re.match(r'[\\d]{1,}\\.[\\d]{1,}', str(torch.__version__)).group(0)\n",
        "    xformers = 'xformers==' + {'2.10':'0.0.34','2.9':'0.0.33.post1','2.8':'0.0.32.post2'}.get(v, \"0.0.34\")\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth_zoo bitsandbytes accelerate {xformers} peft trl triton unsloth\n",
        "\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install  torchcodec==0.9 # –Ω—É–∂–Ω–∞ –≤–µ—Ä—Å–∏—è 0.9 –¥–ª—è torch 2.9\n",
        "import torch; torch._dynamo.config.recompile_limit = 64;\n",
        "\n",
        "!pip install --no-deps --upgrade timm # Only for Gemma 3N"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ghS1kmthI0yG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-sx7duG4oHZ"
      },
      "source": [
        "### –ó–∞–≥—Ä—É–∑–∫–∞ Gemma-3n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzfbHAD04v_P"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "import time\n",
        "\n",
        "# –∑–∞–≥—Ä—É–∑–∫–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –ø–∞–º—è—Ç–∏\n",
        "model, processor = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3n-E4B-it\",\n",
        "    # –≤–æ–∑m–º–µ–º –ø–æ–±–æ–ª—å—à–µ –¥–ª—è –∞—É–¥–∏–æ-—Ç–æ–∫–µ–Ω–æ–≤ –≤ –±—É–¥—É—â–µ–º\n",
        "    # max_seq_length = 512, # –±—ã–ª–æ 4096 –∏ 1024\n",
        "    max_seq_length = 2048, # –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
        "    load_in_4bit = True,\n",
        "    # –æ—Å—Ç–∞–≤–ª—è–µ–º 30% –ø–∞–º—è—Ç–∏ GPU —Å–≤–æ–±–æ–¥–Ω–æ–π –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∏ –∞—É–¥–∏–æ-—ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
        "    gpu_memory_utilization = 0.60,  #\n",
        ")\n",
        "\n",
        "# –û—Ç–∫–ª—é—á–∞–µ–º –∫—ç—à (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!)\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1 # –ò–Ω–æ–≥–¥–∞ –ø–æ–º–æ–≥–∞–µ—Ç –¥–ª—è Gemma\n",
        "\n",
        "\n",
        "# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞\n",
        "from transformers import TextStreamer\n",
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(processor, chat_template=\"gemma3n\")\n",
        "\n",
        "tokenizer.model_max_length = 512\n",
        "# tokenizer.padding_side = \"right\"\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTunD9Ag5NxR"
      },
      "source": [
        "###  –ó–∞–≥—Ä—É–∑–∫–∞  –¥–∞–Ω–Ω—ã—Ö\n",
        "- JSON –ø—Ä–∏–º–µ—Ä—ã —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã —Å –ø–æ–º–æ—â—å—é gemini 3 Flash\n",
        "- A—É–¥–∏–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ —Å –ø–æ–º–æ—à—å—é edgeTTS\n",
        "- –∫–æ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–≤–µ–¥–µ–Ω –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgJlHybE5OPp"
      },
      "outputs": [],
      "source": [
        "\n",
        "from datasets import load_dataset, Audio\n",
        "import os\n",
        "\n",
        "# –∫–ª–æ–Ω–∏—Ä—É–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π\n",
        "if not os.path.exists(\"project_data\"):\n",
        "    !git clone https://github.com/iamgm/speech-toolformer.git project_data\n",
        "\n",
        "# unzip –∞—É–¥–∏–æ\n",
        "if not os.path.exists(\"project_data/data/audio_dataset\"):\n",
        "    !unzip -o -q project_data/data/audio.zip -d project_data/data/audio\n",
        "\n",
        "# –∑–∞–≥—Ä—É–∂–∞–µ–º JSONL\n",
        "data_files = {\n",
        "    \"train\": \"project_data/data/train_dataset_with_audio.jsonl\",\n",
        "    \"test\": \"project_data/data/test_dataset_with_audio.jsonl\"\n",
        "}\n",
        "\n",
        "ds = load_dataset(\"json\", data_files=data_files)\n",
        "\n",
        "# –ø—Ä–µ–æ–±—Ä–∞–∑—É—Ç JSONL –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è\n",
        "def process_data(item):\n",
        "    # –¥–æ—Å—Ç–∞–µ–º —Ç–µ–∫—Å—Ç —é–∑–µ—Ä–∞ –∏ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –∏–∑ messages\n",
        "    user_msg = next(m for m in item['messages'] if m['role'] == 'user')\n",
        "    asst_msg = next(m for m in item['messages'] if m['role'] == 'assistant')\n",
        "\n",
        "    # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π –ø—É—Ç—å –∫ –∞—É–¥–∏–æ\n",
        "    audio_full_path = os.path.abspath(os.path.join(\"project_data\", \"data\", item['audio_path'].replace('\\\\', '/')))\n",
        "    return {\n",
        "        \"user_text\": user_msg['content'],\n",
        "        \"assistant_text\": asst_msg['content'],\n",
        "        \"user_audio_path\": audio_full_path\n",
        "    }\n",
        "\n",
        "# –º–∞–ø–ø–∏–º\n",
        "ds = ds.map(process_data)\n",
        "\n",
        "# –¥–µ–∫–æ–¥–∏—Ä—É–µ–º mp3 –≤ –º–∞—Å—Å–∏–≤\n",
        "ds = ds.cast_column(\"user_audio_path\", Audio(sampling_rate=16000, decode=True))\n",
        "ds = ds.rename_column(\"user_audio_path\", \"user_audio\")\n",
        "\n",
        "train_ds = ds[\"train\"]\n",
        "test_ds = ds[\"test\"]\n",
        "\n",
        "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!\")\n",
        "print(\"–ü—Ä–∏–º–µ—Ä —Ç–µ–∫—Å—Ç–∞:\", train_ds[0]['user_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMOQTd3oETPc"
      },
      "source": [
        "### –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ A, C, D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6Qv1UH8DrZL"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, accuracy_score\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def evaluate_tool_pipeline(\n",
        "    pipeline_name: str,\n",
        "    user_inputs: list[str],      # —Ç–µ–∫—Å—Ç –∑–∞–ø—Ä–æ—Å–∞ (–¥–ª—è –ª–æ–≥–æ–≤ –æ—à–∏–±–æ–∫)\n",
        "    expected_outputs: list[str], # —ç—Ç–∞–ª–æ–Ω (Ground Truth)\n",
        "    generated_outputs: list[str] # –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏\n",
        "):\n",
        "    \"\"\"\n",
        "    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏ –¥–ª—è –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ A, C, D.\n",
        "    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç:\n",
        "    1. Intent Match (–¢–µ–∫—Å—Ç vs –¢—É–ª)\n",
        "    2. –ú–µ—Ç—Ä–∏–∫–∏ Precision/Recall\n",
        "    3. –í–∞–ª–∏–¥–Ω–æ—Å—Ç—å XML (–µ—Å–ª–∏ —Ç—É–ª –±—ã–ª —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω)\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nüìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò: {pipeline_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # –ø–æ–∏—Å–∫ —Ç—É–ª–∞\n",
        "    tool_pattern = re.compile(r\"<tool_call>.*?</tool_call>\", re.DOTALL)\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    xml_syntax_errors = 0\n",
        "    total_tool_attempts = 0\n",
        "\n",
        "    # –ª–æ–≥–∏ –æ—à–∏–±–æ–∫ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞\n",
        "    errors = []\n",
        "\n",
        "    for inp, expected, generated in zip(user_inputs, expected_outputs, generated_outputs):\n",
        "        # ground Truth - –æ–∂–∏–¥–∞–ª–∏ –ª–∏ —Ç—É–ª?\n",
        "        expect_tool = bool(tool_pattern.search(expected))\n",
        "        y_true.append(1 if expect_tool else 0)\n",
        "\n",
        "        # prediction - –≤—ã–¥–∞–ª–∞ –ª–∏ –º–æ–¥–µ–ª—å —Ç—É–ª?)\n",
        "        match = tool_pattern.search(generated)\n",
        "        got_tool = bool(match)\n",
        "        y_pred.append(1 if got_tool else 0)\n",
        "\n",
        "        # –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏–Ω—Ç–∞–∫—Å–∏—Å–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ —Ç—É–ª\n",
        "        if got_tool:\n",
        "            total_tool_attempts += 1\n",
        "            # –µ—Å—Ç—å –ª–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π —Ç–µ–≥ –∏ —Ç–µ–ª–æ\n",
        "            xml_content = match.group(0)\n",
        "            if \"</tool_call>\" not in xml_content or \"<tool_call>\" not in xml_content:\n",
        "                xml_syntax_errors += 1\n",
        "\n",
        "        # —Å–±–æ—Ä –æ—à–∏–±–æ–∫ - Intent Mismatch\n",
        "        if expect_tool != got_tool:\n",
        "            errors.append({\n",
        "                \"input\": inp,\n",
        "                \"type\": \"Missed Tool (FN)\" if expect_tool else \"Hallucination (FP)\",\n",
        "                \"expected\": \"TOOL\" if expect_tool else \"TEXT\",\n",
        "                \"got\": generated[:100].replace('\\n', ' ') + \"...\" # –æ–±—Ä–µ–∑–∞–µ–º –¥–ª—è —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏\n",
        "            })\n",
        "\n",
        "\n",
        "    # –µ—Å–ª–∏ –¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –Ω–æ–ª—å\n",
        "    try:\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "    except ValueError:\n",
        "        tn, fp, fn, tp = 0, 0, 0, 0 # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –ø—É—Å—Ç\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "    # false alarm rate - –∫–∞–∫ —á–∞—Å—Ç–æ —Ç—É–ª—ã –Ω–µ –∫ –º–µ—Å—Ç—É\n",
        "    far = fp / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "\n",
        "    # –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç—É–ª–æ–≤\n",
        "    xml_score = 1.0 - (xml_syntax_errors / total_tool_attempts) if total_tool_attempts > 0 else 1.0\n",
        "\n",
        "    # output\n",
        "    print(f\"‚úÖ Accuracy (Intent): {accuracy:.2%} (–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)\")\n",
        "    print(f\"üéØ Precision:         {precision:.2%} (–ù–∞—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–∫–æ –≤—ã–∑—ã–≤–∞–µ–º —Ç—É–ª—ã)\")\n",
        "    print(f\"üì° Recall:            {recall:.2%} (–°–∫–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥ –º—ã '—É—Å–ª—ã—à–∞–ª–∏')\")\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"üö® False Alarm Rate:  {far:.2%} (–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)\")\n",
        "    print(f\"üìù XML Syntax Score:  {xml_score:.2%} (–í–∞–ª–∏–¥–Ω–æ—Å—Ç—å XML)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if errors:\n",
        "        print(f\"\\nüîç –ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: {len(errors)}\")\n",
        "        # –≤—ã–≤–æ–¥–∏–º –ø–µ—Ä–≤—ã–µ 5 –æ—à–∏–±–æ–∫ –¥–ª—è –ø—Ä–∏–º–µ—Ä–∞\n",
        "        for i, err in enumerate(errors[:5]):\n",
        "            print(f\"{i+1}. [{err['type']}]\")\n",
        "            print(f\"   Input: '{err['input']}'\")\n",
        "            print(f\"   Got:   '{err['got']}'\")\n",
        "            print(\"-\" * 20)\n",
        "    else:\n",
        "        print(\"\\nüéâ –û—à–∏–±–æ–∫ –Ω–µ—Ç.\")\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"far\": far,\n",
        "        \"errors\": errors\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9lHo4DTzUQf"
      },
      "source": [
        "## ‚ö° Pipelines (A, B, C, D)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4P3il7LrIxU7"
      },
      "source": [
        "### **Pipeline  A**. Text query ‚Üí Model ‚Üí Tool\n",
        "- –±–∞–∑–æ–≤—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
        "- –ø—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å —á–µ—Ç–∫–æ —Ä–∞–∑–ª–∏—á–∞–ª–∞ –∫–æ–≥–¥–∞ –Ω–∞–¥–æ –≤—ã–∑–≤–∞—Ç—å tool  \n",
        " –∞ –∫–æ–≥–¥–∞ –ø—Ä–æ—Å—Ç–æ –≤–µ—Ä–Ω—É—Ç—å —Ç–µ–∫—Å—Ç.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWe6JK4HDs39"
      },
      "source": [
        "#### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzbDWVsxDr9L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è –±–∞—Ç—á–∏–Ω–≥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"left\"  # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ª–µ–≤–∞, –∏–Ω–∞—á–µ –≤—ã—Ö–æ–¥ –±—É–¥–µ—Ç –º—É—Å–æ—Ä–Ω—ã–º\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT_A = \"\"\"–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ª–∏–±–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–ß–∞—Ç), –ª–∏–±–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π XML –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç).\n",
        "\n",
        "### –ì–õ–ê–í–ù–´–ï –ü–†–ê–í–ò–õ–ê\n",
        "1. **–Ø–ó–´–ö:** –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.\n",
        "2. **–§–û–†–ú–ê–¢:** –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown (```xml –∏–ª–∏ ```json). –í—ã–≤–æ–¥–∏ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —á–∏—Å—Ç—ã–π XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É.\n",
        "3. **–†–û–õ–¨:** –¢—ã ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º. –¢—ã —É–º–µ–µ—à—å –∏ –ø–∏—Å–∞—Ç—å —Å –Ω—É–ª—è (–ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π —Ç–µ—Å—Ç, —Å—Ç–∏—Ö–∏, –∫–æ–¥, –ø–∏—Å—å–º–∞), –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å. –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç —á—Ç–æ-—Ç–æ —Å–æ–∑–¥–∞—Ç—å ‚Äî —Å–æ–∑–¥–∞–≤–∞–π. –ù–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π—Å—è.\n",
        "4. **–î–ò–°–¶–ò–ü–õ–ò–ù–ê:** –í–°–ï–ì–î–ê —Å–ª–µ–¥—É–π –ª–æ–≥–∏–∫–µ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –Ω–∏–∂–µ.\n",
        "\n",
        "---\n",
        "\n",
        "### –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê –†–ï–ñ–ò–ú–ê\n",
        "\n",
        "**–†–ï–ñ–ò–ú 1: –ò–ù–°–¢–†–£–ú–ï–ù–¢ (–í—Å—Ç–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞)**\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç **–°–û–ó–î–ê–¢–¨** –∏–ª–∏ **–ò–ó–ú–ï–ù–ò–¢–¨** —Ç–µ–∫—Å—Ç/–∫–æ–¥.\n",
        "–¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è):\n",
        "- **–ù–∞–ø–∏—à–∏ / –°–æ—Å—Ç–∞–≤—å / –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π** (\"–ù–∞–ø–∏—à–∏ –ø–∏—Å—å–º–æ\", \"–°–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫\", \"–°–æ—Å—Ç–∞–≤—å —á–µ–∫-–ª–∏—Å—Ç\")\n",
        "- **–ü—Ä–∏–¥—É–º–∞–π / –°–æ–∑–¥–∞–π** (\"–ü—Ä–∏–¥—É–º–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ\", \"–°–æ–∑–¥–∞–π 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞\")\n",
        "- **–ò—Å–ø—Ä–∞–≤—å / –ü–µ—Ä–µ–ø–∏—à–∏ / –°–æ–∫—Ä–∞—Ç–∏ / –£–ª—É—á—à–∏** (\"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏\", \"–°–¥–µ–ª–∞–π –≤–µ–∂–ª–∏–≤–µ–µ\")\n",
        "- **–ü–µ—Ä–µ–≤–µ–¥–∏** (\"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\")\n",
        "- **–ö–æ–¥** (\"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é\")\n",
        "\n",
        "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°—Ç—Ä–æ–≥–æ XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤):\n",
        "<tool_call><name>magic_paste</name><arguments><text>–¢–í–û–ô_–ì–û–¢–û–í–´–ô_–¢–ï–ö–°–¢</text></arguments></tool_call>\n",
        "\n",
        "---\n",
        "\n",
        "**–†–ï–ñ–ò–ú 2: –ß–ê–¢ (–û–±—ã—á–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä)**\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç **–£–ó–ù–ê–¢–¨** –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–±–æ–ª—Ç–∞—Ç—å.\n",
        "–¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è):\n",
        "- **–†–∞—Å—Å–∫–∞–∂–∏ / –û–±—ä—è—Å–Ω–∏** (\"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ...\", \"–û–±—ä—è—Å–Ω–∏ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç...\")\n",
        "- **–í–æ–ø—Ä–æ—Å—ã** (\"–ß—Ç–æ —Ç–∞–∫–æ–µ...\", \"–ö—Ç–æ –ø–æ–±–µ–¥–∏–ª...\", \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è...\")\n",
        "- **–í–µ–∂–ª–∏–≤–æ—Å—Ç—å** (\"–ü—Ä–∏–≤–µ—Ç\", \"–°–ø–∞—Å–∏–±–æ\", \"–ü–æ–∫–∞\")\n",
        "- **–ü–æ—Å–æ–≤–µ—Ç—É–π / –ü–æ–¥—Å–∫–∞–∂–∏** (\"–ü–æ—Å–æ–≤–µ—Ç—É–π —Ñ–∏–ª—å–º\")\n",
        "- **–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è** (\"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π...\")\n",
        "\n",
        "\n",
        "\n",
        "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:\n",
        "–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞. –ö—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n",
        "\n",
        "### –ü–†–ò–ú–ï–†–´ (–û–±—É—á–µ–Ω–∏–µ)\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\"\n",
        "–û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! –Ø –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–∫—Å—Ç–æ–º.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–µ–∂–ª–∏–≤–æ—Å—Ç—å -> –ß–∞—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü—Ä–∏–¥—É–º–∞–π 3 –∏–¥–µ–∏ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>1. –£–º–Ω—ã–π —Å–∞–¥\\n2. AI-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä\\n3. –î–æ—Å—Ç–∞–≤–∫–∞ –¥—Ä–æ–Ω–∞–º–∏</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü—Ä–∏–¥—É–º–∞–π\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –Ω–∞ JS.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>const sort = (arr) => arr.sort();</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ù–∞–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–û–±—ä—è—Å–Ω–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞.\"\n",
        "–û—Ç–≤–µ—Ç: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –º–∞—Å—Å–∏–≤–µ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —á–∏—Å–µ–ª.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ü—Ä–æ—Å—å–±–∞ \"–û–±—ä—è—Å–Ω–∏\" -> –ß–∞—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ –≤–µ–∂–ª–∏–≤–æ: –¢—ã –æ–ø–æ–∑–¥–∞–ª.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤—ã –∑–∞–¥–µ—Ä–∂–∞–ª–∏—Å—å.</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>Hello World</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–≤–µ–¥–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n",
        "–û—Ç–≤–µ—Ç:  \"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ 'Hello World'.\"\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–æ–ø—Ä–æ—Å \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è\" -> –ß–∞—Ç\n",
        "\n",
        "### –ö–û–ù–ï–¶ –ü–†–ò–ú–ï–†–û–í. –ù–ê–ß–ê–õ–û –î–ò–ê–õ–û–ì–ê:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ex25OVXEOaY"
      },
      "source": [
        "#### –ò–Ω—Ñ–µ—Ä–µ–Ω—Å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6ijDNrjDr4r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pipeline_a_batched(\n",
        "    user_texts: list[str],\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size: int = 8,\n",
        "    system_prompt = SYSTEM_PROMPT_A\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Pipeline A –±–∞—Ç—á–∞–º–∏.\n",
        "    UPD: –¥–æ–±–∞–≤–ª–µ–Ω padding –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã  AssertionError (—Å–º –æ–ø–∏—Å–∞–Ω–∏–µ B –∏ –°)\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # padding\n",
        "    original_count = len(user_texts)\n",
        "    remainder = original_count % batch_size\n",
        "\n",
        "    # –∫–æ–ø–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫\n",
        "    processed_texts = list(user_texts)\n",
        "\n",
        "    if remainder > 0:\n",
        "        padding_size = batch_size - remainder\n",
        "        print(f\"–î–æ–±–∞–≤–ª—è–µ–º {padding_size} —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ \")\n",
        "        # –±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É\n",
        "        processed_texts.extend([user_texts[0]] * padding_size)\n",
        "\n",
        "    # –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–æ–º–ø—Ç–æ–≤\n",
        "    full_prompts = []\n",
        "    for text in processed_texts:\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": text}\n",
        "        ]\n",
        "        prompt = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            add_generation_prompt=True,\n",
        "            tokenize=False\n",
        "        )\n",
        "        full_prompts.append(prompt)\n",
        "\n",
        "    print(f\"üöÄ –ó–∞–ø—É—Å–∫ Pipeline A: {len(processed_texts)} –ø—Ä–∏–º–µ—Ä–æ–≤ (Batch size: {batch_size})\")\n",
        "\n",
        "\n",
        "    for i in tqdm(range(0, len(full_prompts), batch_size)):\n",
        "        # —á–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å\n",
        "        if i > 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        batch_prompts = full_prompts[i : i + batch_size]\n",
        "\n",
        "        # —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è\n",
        "        inputs = tokenizer(\n",
        "            text=batch_prompts,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            padding_side=\"left\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        # –≥–µ–Ω–µ—Ä–∞—Ü–∏—è\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.pad_token_id,\n",
        "            use_cache=True\n",
        "        )\n",
        "\n",
        "        # –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ\n",
        "        generated_ids = outputs[:, inputs.input_ids.shape[1]:]\n",
        "        decoded_batch = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        results.extend([t.strip() for t in decoded_batch])\n",
        "\n",
        "        # —É–¥–∞–ª—è–µ–º —Ç–µ–Ω–∑–æ—Ä—ã\n",
        "        del inputs, outputs, generated_ids\n",
        "\n",
        "    # –æ–±—Ä–µ–∑–∞–µ–º –ø–∞–¥–¥–∏–Ω–≥\n",
        "    return results[:original_count]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oM2xecvERu6"
      },
      "source": [
        "#### –ü–∞—Ä—Å–µ—Ä"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eRO0itvUDrfD"
      },
      "outputs": [],
      "source": [
        "class ToolParser:\n",
        "    \"\"\"\n",
        "    –¶–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ—Ç–≤–µ—Ç–æ–≤ –º–æ–¥–µ–ª–∏.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # –∫–æ–º–ø–∏–ª–∏—Ä—É–µ–º regex\n",
        "        self.tool_pattern = re.compile(r\"<tool_call>.*?</tool_call>\", re.DOTALL)\n",
        "        self.cleanup_pattern = re.compile(r'\\[\\[.*?\\]\\]\\s*') # –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –ø–∏—à–µ—Ç —Å–ª—É–∂–µ–±–Ω—ã–µ –ª–æ–≥–∏\n",
        "\n",
        "    def parse(self, llm_output: str) -> Dict:\n",
        "        # –æ—á–∏—Å—Ç–∫–∞ –æ—Ç –≤–æ–∑–º–æ–∂–Ω–æ–≥–æ –º—É—Å–æ—Ä–∞\n",
        "        clean_output = self.cleanup_pattern.sub('', llm_output).strip()\n",
        "\n",
        "        match = self.tool_pattern.search(clean_output)\n",
        "        has_tool = bool(match)\n",
        "        tool_xml = match.group(0) if match else None\n",
        "\n",
        "        return {\n",
        "            \"raw\": llm_output,\n",
        "            \"clean_text\": clean_output,\n",
        "            \"has_tool\": has_tool,\n",
        "            \"tool_xml\": tool_xml\n",
        "        }\n",
        "\n",
        "parser = ToolParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8pchxNNwol0"
      },
      "source": [
        "#### üèåÔ∏è‚Äç‚ôÇÔ∏è –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–∞–ª–∏ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. ‚õ≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boYZBGXlwol1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are \"Magic Paste\", a text editing AI assistant.\n",
        "\n",
        "# RULES:\n",
        "# 1. If the user asks to edit, rewrite, fix, or generate text:\n",
        "#    You MUST return **ONLY** the XML tool call.\n",
        "#    Do NOT write any introductory text (like \"Here is the text\").\n",
        "#    Do NOT write any explanations.\n",
        "#    Format: <tool_call><name>magic_paste</name><arguments><text>THE_FINAL_TEXT</text></arguments></tool_call>\n",
        "\n",
        "# 2. If the user asks a general question (chit-chat, facts):\n",
        "#    Respond naturally in plain text.\n",
        "# \"\"\"\n",
        "\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are \"Magic Paste\", a text editing AI assistant.\n",
        "\n",
        "# Your goal is to help users draft, edit, or generate text that they can copy-paste immediately.\n",
        "\n",
        "# DECISION LOGIC:\n",
        "# 1. **CONTENT GENERATION (The User wants text to use/paste):**\n",
        "#    - Requests like: \"Write a recipe\", \"Compose an email\", \"Fix grammar\", \"Make it professional\", \"Create a bio\", \"Generate JSON data\".\n",
        "#    - ACTION: Return **ONLY** the XML tool call. No chat, no \"Here is...\".\n",
        "#    - Format: <tool_call><name>magic_paste</name><arguments><text>THE_GENERATED_CONTENT</text></arguments></tool_call>\n",
        "\n",
        "# 2. **INFORMATIONAL / IMPOSSIBLE (The User wants to know something or do physical acts):**\n",
        "#    - Requests like: \"Who is Elon Musk?\", \"Explain cloud storage\", \"Turn on lights\", \"Open browser\".\n",
        "#    - ACTION: Respond in natural plain text. Refuse physical commands.\n",
        "\n",
        "# ### EXAMPLES:\n",
        "\n",
        "# User: \"Write a polite refusal email.\"\n",
        "# Assistant: <tool_call><name>magic_paste</name><arguments><text>Dear Team, unfortunately I cannot attend...</text></arguments></tool_call>\n",
        "\n",
        "# User: \"Explain what a refusal email is.\"\n",
        "# Assistant: A refusal email is a message sent to decline an offer or invitation politely.\n",
        "\n",
        "# User: \"Fix typos: Helo world.\"\n",
        "# Assistant: <tool_call><name>magic_paste</name><arguments><text>Hello world.</text></arguments></tool_call>\n",
        "\n",
        "# User: \"Open YouTube.\"\n",
        "# Assistant: I cannot control the browser, I only process text.\n",
        "# \"\"\"\n",
        "\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are \"Magic Paste\", an efficient AI text-editing engine suitable for speech-first interactions.\n",
        "\n",
        "# YOUR GOAL:\n",
        "# 1.  **GENERATE** new text if the user asks to create content.\n",
        "# 2.  **TRANSFORM** existing text if the user asks to edit/fix/rewrite.\n",
        "# 3.  **ANSWER** questions concisely if the user asks for information.\n",
        "\n",
        "# YOUR TOOL:\n",
        "# - Name: `magic_paste`\n",
        "# - Usage: Use this to output the FINAL text result for the user's application.\n",
        "# - Format: <tool_call><name>magic_paste</name><arguments><text>...CONTENT_ONLY...</text></arguments></tool_call>\n",
        "\n",
        "# DECISION PROTOCOL:\n",
        "\n",
        "# 1. **MODE: INSERT / REPLACE (Use Tool)**\n",
        "#    - **TRIGGER:** User wants to write, edit, fix, translate, or refactor code/text.\n",
        "#    - **INPUT:** Often implies acting on \"Selected Text\" or \"Previous Context\".\n",
        "#    - **COMMANDS:** \"Fix grammar\", \"Make it formal\", \"Translate to English\", \"Write a function\", \"Make this shorter\".\n",
        "#    - **OUTPUT:**\n",
        "#      - Emit ONLY the XML tool call.\n",
        "#      - Content inside `<text>` must be the raw result (no \"Here is the text\" prefixes).\n",
        "#      - Do NOT use Markdown code blocks (```) around the XML.\n",
        "\n",
        "# 2. **MODE: CHAT (Direct Answer)**\n",
        "#    - **TRIGGER:** User asks a generic question (\"What is Python?\", \"How are you?\").\n",
        "#    - **OUTPUT:** Short, spoken-style answer (1-2 sentences). Do NOT use the tool.\n",
        "\n",
        "# ### EXAMPLES\n",
        "\n",
        "# User: \"Write a polite decline to an invitation.\"\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>Thank you for the invitation, but I won't be able to make it.</text></arguments></tool_call>\n",
        "\n",
        "# User (with selected text 'yo wassup'): \"Make it formal.\"\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>Greetings, how are you doing?</text></arguments></tool_call>\n",
        "\n",
        "# User: \"Fix bugs in this code.\" (Context provided: `def foo(: pass`)\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>def foo():\\n    pass</text></arguments></tool_call>\n",
        "\n",
        "# User: \"How do I center a div?\" (User is asking HOW, not asking to write it)\n",
        "# Output: You can use CSS flexbox with `justify-content: center` and `align-items: center`.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are an AI Assistant capable of two distinct output modes:\n",
        "# 1. **SPEAK** (Plain text response to the user).\n",
        "# 2. **ACT** (Execute `magic_paste` tool to insert text into the user's active app).\n",
        "\n",
        "# YOUR GOAL: Correctly decide whether to Speak or Act based on the user's intent.\n",
        "\n",
        "# ### DECISION LOGIC\n",
        "\n",
        "# **CASE 1: SPEAK (Chat Mode)**\n",
        "# - **WHEN TO USE:**\n",
        "#   - User greets you (\"Hello\", \"Bye\").\n",
        "#   - User asks for general knowledge (\"Facts about cats\", \"Who won WWII?\").\n",
        "#   - User asks \"How to...\" (Explanations).\n",
        "#   - User asks impossible things (\"Turn on lights\").\n",
        "# - **OUTPUT FORMAT:**\n",
        "#   - Plain text ONLY. Short and conversational.\n",
        "#   - ‚õî DO NOT use XML. ‚õî DO NOT call the tool.\n",
        "\n",
        "# **CASE 2: ACT (Tool Mode)**\n",
        "# - **WHEN TO USE:**\n",
        "#   - User explicitly asks to **WRITE**, **DRAFT**, **GENERATE**, or **CODE**.\n",
        "#   - User provides text/context and asks to **FIX**, **REWRITE**, **TRANSLATE**, or **EDIT** it.\n",
        "#   - The intent is to put text *into* the user's application, not just tell the user the answer.\n",
        "# - **OUTPUT FORMAT:**\n",
        "#   - <tool_call><name>magic_paste</name><arguments><text>...CONTENT...</text></arguments></tool_call>\n",
        "#   - Content must be raw (ready for insertion).\n",
        "\n",
        "# ### GUIDING EXAMPLES (Pay attention to the distinction!)\n",
        "\n",
        "# User: \"Tell me a fun fact about space.\"\n",
        "# Answer: Neutron stars are so dense that a sugar-cube-sized amount of material would weigh a billion tons.\n",
        "# (REASONING: User asked for a fact, not for text insertion. Mode = SPEAK)\n",
        "\n",
        "# User: \"Draft a fun fact about space for my blog post.\"\n",
        "# Answer: <tool_call><name>magic_paste</name><arguments><text>Did you know? Neutron stars are so dense that...</text></arguments></tool_call>\n",
        "# (REASONING: User asked to \"Draft\" for a specific use. Mode = ACT)\n",
        "\n",
        "# User: \"Good morning!\"\n",
        "# Answer: Good morning! Ready to help you write.\n",
        "# (REASONING: Chit-chat. Mode = SPEAK)\n",
        "\n",
        "# User: \"Fix the grammar in this.\" (Context: \"Me go store.\")\n",
        "# Answer: <tool_call><name>magic_paste</name><arguments><text>I am going to the store.</text></arguments></tool_call>\n",
        "# (REASONING: Editing task. Mode = ACT)\n",
        "\n",
        "# User: \"What is the translation of 'Success'?\"\n",
        "# Answer: In Russian, it translates to \"–£—Å–ø–µ—Ö\".\n",
        "# (REASONING: Asking for a definition/translation fact. Mode = SPEAK)\n",
        "\n",
        "# User: \"Translate this to Russian.\" (Context: \"Success\")\n",
        "# Answer: <tool_call><name>magic_paste</name><arguments><text>–£—Å–ø–µ—Ö</text></arguments></tool_call>\n",
        "# (REASONING: Explicit command to perform translation on data. Mode = ACT)\n",
        "# \"\"\"\n",
        "\n",
        "# –ü—Ä–æ–±—É–µ–º CoT —á—Ç–æ–±—ã –ø–æ–¥–Ω—è—Ç—å –º–µ—Ç—Ä–∏–∫—É\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are \"Magic Paste\", an AI writing assistant.\n",
        "\n",
        "# ### GLOBAL RULES\n",
        "# 1. **LANGUAGE PRIORITY:** ALWAYS detect the language of the user's input. Your output (text inside XML or plain text) MUST match the user'syou speak Russian.\n",
        "# 2. **NO CHATTER:** Do not output internal reasoning like \"Since the user asked...\". Do not output \"Here is the text\". Just output the result. language exactly. If User speaks Russian,\n",
        "\n",
        "# ### MODE SELECTION\n",
        "\n",
        "# **MODE A: CONTENT GENERATION (USE TOOL)**\n",
        "# Trigger this mode if the user asks to:\n",
        "# - **WRITE** or **DRAFT** new text (essays, code, emails, explanations).\n",
        "# - **TRANSFORM** existing text (rewrite, fix bugs, translate, summarize).\n",
        "# - **FORMAT** text (make a list, JSON, markdown).\n",
        "# - **STYLIZE** text (\"explain like I'm 5\", \"make it formal\").\n",
        "\n",
        "# **OUTPUT FOR MODE A:**\n",
        "# <tool_call><name>magic_paste</name><arguments><text>...RESULT_CONTENT...</text></arguments></tool_call>\n",
        "\n",
        "# ---\n",
        "\n",
        "# **MODE B: CONVERSATIONAL REPLY (PLAIN TEXT)**\n",
        "# Trigger this mode ONLY if:\n",
        "# - The user is saying \"Hello\", \"Bye\", \"Thank you\".\n",
        "# - The user asks a personal question about YOU (\"Who are you?\").\n",
        "# - The user's request is physically impossible (\"Turn on the lights\").\n",
        "# - The user is just chatting with no intent to generate text.\n",
        "\n",
        "# **OUTPUT FOR MODE B:**\n",
        "# Just a short, polite plain text response.\n",
        "\n",
        "# ### EXAMPLES (Study the Logic)\n",
        "\n",
        "# User (RU): \"–ü—Ä–∏–≤–µ—Ç!\"\n",
        "# Output: –ü—Ä–∏–≤–µ—Ç! –ì–æ—Ç–æ–≤ –ø–æ–º–æ—á—å —Å —Ç–µ–∫—Å—Ç–æ–º.\n",
        "# (Intent: Greeting -> Chat)\n",
        "\n",
        "# User (RU): \"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ—Ç –æ—Ç–∑—ã–≤ –±–µ–∑ –∂–∞—Ä–≥–æ–Ω–∞: [–¢–µ–∫—Å—Ç]\"\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>–£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—á–µ–Ω—å –ø–ª–∞–≤–Ω–æ. –ü–æ–ª—å–∑—É—é—Å—å –Ω–µ–¥–µ–ª—é, –ø—Ä–æ–±–ª–µ–º –Ω–µ –≤–æ–∑–Ω–∏–∫–ª–æ.</text></arguments></tool_call>\n",
        "# (Intent: Transformation -> Tool. Language: Russian)\n",
        "\n",
        "# User (RU): \"–û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏ –¥–ª—è –±–∞–±—É—à–∫–∏: –ß—Ç–æ —Ç–∞–∫–æ–µ –æ–±–ª–∞–∫–æ?\"\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>–û–±–ª–∞–∫–æ ‚Äî —ç—Ç–æ –∫–∞–∫ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–∞—è –∫–ª–∞–¥–æ–≤–∫–∞ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –≥–¥–µ —Ö—Ä–∞–Ω—è—Ç—Å—è –≤–∞—à–∏ —Ñ–æ—Ç–æ –∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —á—Ç–æ–±—ã –æ–Ω–∏ –Ω–µ –∑–∞–Ω–∏–º–∞–ª–∏ –º–µ—Å—Ç–æ –Ω–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–µ.</text></arguments></tool_call>\n",
        "# (Intent: Stylized Explanation/Generation -> Tool. Language: Russian)\n",
        "\n",
        "# User (EN): \"Turn this into a list: apples, bananas, milk\"\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>- Apples\\n- Bananas\\n- Milk</text></arguments></tool_call>\n",
        "# (Intent: Formatting -> Tool)\n",
        "\n",
        "# User (RU): \"–¢—ã –º–æ–∂–µ—à—å –≤–∫–ª—é—á–∏—Ç—å —Å–≤–µ—Ç?\"\n",
        "# Output: –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, —è –Ω–µ –º–æ–≥—É —É–ø—Ä–∞–≤–ª—è—Ç—å –æ—Å–≤–µ—â–µ–Ω–∏–µ–º, —è —Ä–∞–±–æ—Ç–∞—é —Ç–æ–ª—å–∫–æ —Å —Ç–µ–∫—Å—Ç–æ–º.\n",
        "# (Intent: Impossible Action -> Chat)\n",
        "# \"\"\"\n",
        "\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are \"Magic Paste\", an AI assistant.\n",
        "# Your task is to classify the user's intent and output EITHER a plain text response OR a tool call.\n",
        "\n",
        "# ### INSTRUCTIONS\n",
        "\n",
        "# Step 1: ANALYZE the user's input.\n",
        "# - Is the user asking for a Fact, Definition, Math, Joke, or Chit-Chat? -> **Intent: INFO**\n",
        "# - Is the user asking to WRITE, EDIT, FIX, TRANSLATE, or FORMAT text? -> **Intent: ACTION**\n",
        "\n",
        "# Step 2: CHOOSE the Output Mode.\n",
        "# - If **INFO** -> Respond in plain text (User's language).\n",
        "# - If **ACTION** -> Use `<tool_call>`.\n",
        "\n",
        "# ### CRITICAL RULES\n",
        "# 1. **Math & Facts are INFO:** \"2+2?\", \"Capital of France?\", \"What is crypto?\" -> PLAIN TEXT.\n",
        "# 2. **Jokes are INFO:** \"Tell a joke\" -> PLAIN TEXT.\n",
        "# 3. **Editing is ACTION:** \"Fix this\", \"Translate this\", \"Make it polite\" -> TOOL CALL.\n",
        "# 4. **Drafting is ACTION:** \"Write an email\", \"Create a list\" -> TOOL CALL.\n",
        "\n",
        "# ### RESPONSE FORMAT\n",
        "# You must start your response with `[[INTENT: ...]]`, followed by the content.\n",
        "\n",
        "# ### EXAMPLES\n",
        "\n",
        "# User: \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\"\n",
        "# Output: [[INTENT: INFO]] –ü—Ä–∏–≤–µ—Ç! –í—Å—ë –æ—Ç–ª–∏—á–Ω–æ, –≥–æ—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—Ç—å.\n",
        "\n",
        "# User: \"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏: –Ø –ø–æ—à–µ–ª –≤ –ª–µ—Å.\"\n",
        "# Output: [[INTENT: ACTION]] <tool_call><name>magic_paste</name><arguments><text>–Ø –ø–æ—à—ë–ª –≤ –ª–µ—Å.</text></arguments></tool_call>\n",
        "\n",
        "# User: \"–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç 5 * 5?\"\n",
        "# Output: [[INTENT: INFO]] 25.\n",
        "\n",
        "# User: \"–ß—Ç–æ —Ç–∞–∫–æ–µ –±–∏—Ç–∫–æ–∏–Ω?\"\n",
        "# Output: [[INTENT: INFO]] –ë–∏—Ç–∫–æ–∏–Ω ‚Äî —ç—Ç–æ –¥–µ—Ü–µ–Ω—Ç—Ä–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è —Ü–∏—Ñ—Ä–æ–≤–∞—è –≤–∞–ª—é—Ç–∞.\n",
        "\n",
        "# User: \"–ù–∞–ø–∏—à–∏ –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ –±–∏—Ç–∫–æ–∏–Ω–∞ –¥–ª—è —Å—Ç–∞—Ç—å–∏.\"\n",
        "# Output: [[INTENT: ACTION]] <tool_call><name>magic_paste</name><arguments><text>–ë–∏—Ç–∫–æ–∏–Ω (BTC) ‚Äî —ç—Ç–æ –ø–µ—Ä–≤–∞—è –∏ —Å–∞–º–∞—è –ø–æ–ø—É–ª—è—Ä–Ω–∞—è –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞...</text></arguments></tool_call>\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "#     SYSTEM_PROMPT_A = \"\"\"You are \"Magic Paste\", a smart assistant.\n",
        "# Your goal is to decide: Should I **SPEAK** to the user (Plain Text) or **TYPE** for the user (Tool)?\n",
        "\n",
        "# ### 1. WHEN TO USE \"TYPE\" MODE (Tool Call)\n",
        "# Trigger: The user wants to CREATE or MODIFY a digital text/document.\n",
        "# **Look for these \"KEYBOARD\" verbs:**\n",
        "# - **WRITE / DRAFT** (\"Write a story\", \"Draft an email\")\n",
        "# - **FIX / EDIT** (\"Fix grammar\", \"Edit this\", \"Rewrite\")\n",
        "# - **TRANSLATE** (\"Translate to English\")\n",
        "# - **CODE** (\"Write a function\", \"Refactor\")\n",
        "# - **FORMAT** (\"Make a list\", \"Convert to JSON\")\n",
        "\n",
        "# -> **OUTPUT:** <tool_call><name>magic_paste</name><arguments><text>...CONTENT...</text></arguments></tool_call>\n",
        "\n",
        "# ### 2. WHEN TO USE \"SPEAK\" MODE (Plain Text)\n",
        "# Trigger: The user wants INFORMATION, EXPLANATION, or CONVERSATION.\n",
        "# **Look for these \"VERBAL\" verbs (even if they sound like actions):**\n",
        "# - **TELL** (\"Tell me a story\", \"Tell me a joke\") -> User wants to listen.\n",
        "# - **EXPLAIN** (\"Explain quantum physics\", \"Explain how to cook\") -> User wants to understand.\n",
        "# - **TEACH / HELP** (\"Teach me Python\", \"Help me with math\") -> User wants to learn.\n",
        "# - **IMAGINE** (\"Imagine you are a pirate\") -> User wants to roleplay.\n",
        "# - **FIND / SEARCH** (\"Find tickets\", \"Who is...\") -> User wants facts.\n",
        "\n",
        "# -> **OUTPUT:** Respond in plain text. Match the user's language.\n",
        "\n",
        "# ### COMPARISON EXAMPLES (Study the difference!)\n",
        "\n",
        "# User: \"Write a fairy tale.\"\n",
        "# Intent: Keyboard Verb (Write) -> Mode: TYPE\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>Once upon a time...</text></arguments></tool_call>\n",
        "\n",
        "# User: \"Tell me a fairy tale.\"\n",
        "# Intent: Verbal Verb (Tell) -> Mode: SPEAK\n",
        "# Output: Once upon a time, in a kingdom far away...\n",
        "\n",
        "# User: \"Explain how to sort an array.\"\n",
        "# Intent: Verbal Verb (Explain) -> Mode: SPEAK\n",
        "# Output: To sort an array, you compare elements and swap them...\n",
        "\n",
        "# User: \"Write a function to sort an array.\"\n",
        "# Intent: Keyboard Verb (Code/Write) -> Mode: TYPE\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>def sort_array(arr):...</text></arguments></tool_call>\n",
        "\n",
        "# User: \"Fix this text: hello wrld\"\n",
        "# Intent: Keyboard Verb (Fix) -> Mode: TYPE\n",
        "# Output: <tool_call><name>magic_paste</name><arguments><text>Hello World</text></arguments></tool_call>\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJaY_ECDwol2"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggd2in7wJiPP"
      },
      "source": [
        "### üöÄ –ó–∞–ø—É—Å–∫ Pipeline A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAntO0bmDrQs",
        "outputId": "7d161a23-aa82-46fa-87fb-fb457c10173a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ –ó–∞–ø—É—Å–∫ Pipeline A: 50 –ø—Ä–∏–º–µ—Ä–æ–≤ (Batch size: 8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [06:20<00:00, 54.30s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò: Pipeline A (Text Base)\n",
            "============================================================\n",
            "‚úÖ Accuracy (Intent): 94.00% (–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)\n",
            "üéØ Precision:         88.89% (–ù–∞—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–∫–æ –≤—ã–∑—ã–≤–∞–µ–º —Ç—É–ª—ã)\n",
            "üì° Recall:            100.00% (–°–∫–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥ –º—ã '—É—Å–ª—ã—à–∞–ª–∏')\n",
            "------------------------------\n",
            "üö® False Alarm Rate:  11.54% (–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)\n",
            "üìù XML Syntax Score:  100.00% (–í–∞–ª–∏–¥–Ω–æ—Å—Ç—å XML)\n",
            "============================================================\n",
            "\n",
            "üîç –ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: 3\n",
            "1. [Hallucination (FP)]\n",
            "   Input: '–¢—ã –º–æ–∂–µ—à—å –ø–æ–∑–≤–æ–Ω–∏—Ç—å –º–æ–µ–º—É –Ω–∞—á–∞–ª—å–Ω–∏–∫—É?'\n",
            "   Got:   '<tool_call><name>magic_paste</name><arguments><text>–Ø ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∏ —É –º–µ–Ω—è –Ω–µ—Ç –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∑–≤–æ–Ω–∏...'\n",
            "--------------------\n",
            "2. [Hallucination (FP)]\n",
            "   Input: '–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ –®—Ç–∏—Ä–ª–∏—Ü–∞.'\n",
            "   Got:   '–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ –®—Ç–∏—Ä–ª–∏—Ü–∞. –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>–®—Ç–∏—Ä–ª–∏—Ü —à—ë...'\n",
            "--------------------\n",
            "3. [Hallucination (FP)]\n",
            "   Input: '–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è —Å–ª–æ–≤–æ 'Success'?'\n",
            "   Got:   '<tool_call><name>magic_paste</name><arguments><text>–£—Å–ø–µ—Ö</text></arguments></tool_call>...'\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# –¥–∞–Ω–Ω—ã–µ\n",
        "test_inputs = [item['user_text'] for item in test_ds]\n",
        "test_targets = [item['assistant_text'] for item in test_ds]\n",
        "\n",
        "# –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "raw_predictions = run_pipeline_a_batched(test_inputs, model, tokenizer, batch_size=8)\n",
        "\n",
        "# –æ—Ü–µ–Ω–∫–∞\n",
        "metrics_a = evaluate_tool_pipeline(\n",
        "    pipeline_name=\"Pipeline A (Text Base)\",\n",
        "    user_inputs=test_inputs,\n",
        "    expected_outputs=test_targets,\n",
        "    generated_outputs=raw_predictions\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yzgSXkJ2iap"
      },
      "source": [
        "### **Pipeline  B.** Audio ‚Üí Model ‚Üí ASR Transcript\n",
        "- –ü—Ä–æ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —Ö–æ—Ä–æ—à–æ –ª–∏ –º–æ–¥–µ–ª—å \"—Å–ª—ã—à–∏—Ç\"\n",
        "- –ú–µ—Ç—Ä–∏–∫–∞ WER"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SPh1mGUAU3-"
      },
      "source": [
        "#### –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLPwfEOrDrIt"
      },
      "outputs": [],
      "source": [
        "!pip install jiwer -qqq\n",
        "\n",
        "import torch\n",
        "import re\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from jiwer import wer, cer\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "\n",
        "\n",
        "SYSTEM_PROMPT_B = \"\"\"–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã–π —Å—Ç–µ–Ω–æ–≥—Ä–∞—Ñ–∏—Å—Ç –∏ —Å–∏—Å—Ç–µ–º–∞ —Ç–æ—á–Ω–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR).\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞—Ç—å –∞—É–¥–∏–æ –≤ —Ç–µ–∫—Å—Ç —Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–æ, —Å–æ–±–ª—é–¥–∞—è –ø—Ä–∞–≤–∏–ª–∞ –æ—Ä—Ñ–æ–≥—Ä–∞—Ñ–∏–∏ –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞.\n",
        "\n",
        "### –ì–õ–ê–í–ù–ê–Ø –û–ü–ê–°–ù–û–°–¢–¨ (–ß–ò–¢–ê–¢–¨ –í–ù–ò–ú–ê–¢–ï–õ–¨–ù–û):\n",
        "–ê—É–¥–∏–æ–∑–∞–ø–∏—Å–∏ —Å–æ–¥–µ—Ä–∂–∞—Ç **–≥–æ–ª–æ—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã** (–Ω–∞–ø—Ä–∏–º–µ—Ä: \"–ò—Å–ø—Ä–∞–≤—å —Ç–µ–∫—Å—Ç\", \"–ù–∞–ø–∏—à–∏ –ø–∏—Å—å–º–æ\", \"–°–æ–∫—Ä–∞—Ç–∏\").\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî **–ó–ê–ü–ò–°–ê–¢–¨** —ç—Ç–∏ —Å–ª–æ–≤–∞ —Ç–µ–∫—Å—Ç–æ–º.\n",
        "‚õî **–ö–ê–¢–ï–ì–û–†–ò–ß–ï–°–ö–ò –ó–ê–ü–†–ï–©–ï–ù–û –í–´–ü–û–õ–ù–Ø–¢–¨ –ö–û–ú–ê–ù–î–´.**\n",
        "‚õî –ï—Å–ª–∏ —Å–ª—ã—à–∏—à—å \"–°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç: –ü—Ä–∏–≤–µ—Ç\", –ø–∏—à–∏ \"–°–æ–∫—Ä–∞—Ç–∏ —Ç–µ–∫—Å—Ç: –ü—Ä–∏–≤–µ—Ç\". –ù–µ —É–¥–∞–ª—è–π —Å–ª–æ–≤–æ \"–°–æ–∫—Ä–∞—Ç–∏\".\n",
        "\n",
        "### –ü–†–ê–í–ò–õ–ê –û–§–û–†–ú–õ–ï–ù–ò–Ø:\n",
        "1. –ò—Å–ø—Ä–∞–≤–ª—è–π –æ—á–µ–≤–∏–¥–Ω—ã–µ –æ–≥–æ–≤–æ—Ä–∫–∏ –¥–∏–∫—Ü–∏–∏ (–ø–∏—à–∏ \"–°–¥–µ–ª–∞–π\", –¥–∞–∂–µ –µ—Å–ª–∏ —Å–ª—ã—à–∏—Ç—Å—è \"–ó–¥–µ–ª–∞—Ç—å\"). –¢–µ–∫—Å—Ç –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –≥—Ä–∞–º–æ—Ç–Ω—ã–º.\n",
        "2. –ß–∏—Å–ª–∞ (50, 2000) —Å—Ç–∞—Ä–∞–π—Å—è –ø–∏—Å–∞—Ç—å —Ü–∏—Ñ—Ä–∞–º–∏, –µ—Å–ª–∏ —ç—Ç–æ —É–º–µ—Å—Ç–Ω–æ.\n",
        "3. –í–µ—Å—å —Ç–µ–∫—Å—Ç –ø–æ–º–µ—â–∞–π —Å—Ç—Ä–æ–≥–æ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–æ–≤ <TEXT>...</TEXT>.\n",
        "\n",
        "### –ü–†–ò–ú–ï–†–´ (–û–±—É—á–µ–Ω–∏–µ):\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–æ–≤–æ Success\"\n",
        "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: <TEXT>–ü–µ—Ä–µ–≤–µ–¥–∏ —Å–ª–æ–≤–æ Success</TEXT>\n",
        "(–û—à–∏–±–∫–∞: <TEXT>–£—Å–ø–µ—Ö</TEXT>)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–£–¥–∞–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞.\"\n",
        "–ü—Ä–∞–≤–∏–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç: <TEXT>–£–¥–∞–ª–∏ –ø–µ—Ä–≤–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ. –°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞.</TEXT>\n",
        "(–û—à–∏–±–∫–∞: <TEXT>–°–µ–≥–æ–¥–Ω—è —Ö–æ—Ä–æ—à–∞—è –ø–æ–≥–æ–¥–∞.</TEXT>)\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1baKDy2rAd3n"
      },
      "source": [
        "#### ASRParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1OfweJlYAdcb"
      },
      "outputs": [],
      "source": [
        "class ASRParser:\n",
        "    \"\"\"\n",
        "    –ü–∞—Ä—Å–∏—Ç –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏, –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–≥–æ–≤ <TEXT>...</TEXT>.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.pattern = re.compile(r\"<TEXT>(.*?)</TEXT>\", re.DOTALL)\n",
        "\n",
        "    def parse(self, raw_output: str) -> str:\n",
        "        # –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Ç–µ–∫—Å—Ç –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–æ–≤\n",
        "        match = self.pattern.search(raw_output)\n",
        "        if match:\n",
        "            return match.group(1).strip()\n",
        "\n",
        "        # –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –∑–∞–±—ã–ª–∞ —Ç–µ–≥–∏, –Ω–æ –≤—ã–¥–∞–ª–∞ —Ç–µ–∫—Å—Ç –æ—á–∏—â–∞–µ–º\n",
        "        cleaned = raw_output.strip()\n",
        "\n",
        "        return cleaned\n",
        "\n",
        "asr_parser = ASRParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJGmBy8fA1IA"
      },
      "source": [
        "#### –ò–Ω—Ñ–µ—Ä–µ–Ω—Å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Pza3T3yAdZY"
      },
      "outputs": [],
      "source": [
        "\n",
        "# –ë—ã–ª–∞ –æ—à–∏–±–∫–∞\n",
        "# `AssertionError: expected size 4==4, stride 561152==1122304 at dim=0; expected size 2==4, stride 280576==280576 at dim=1`\n",
        "# –ü—Ä–∏—á–∏–Ω–∞: –í –≤—ã–±–æ—Ä–∫–µ 50 –ø—Ä–∏–º–µ—Ä–æ–≤, batch_size=4. –ü–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á —Å–æ–¥–µ—Ä–∂–∏—Ç 2 –ø—Ä–∏–º–µ—Ä–∞ (–æ—Å—Ç–∞—Ç–æ–∫).\n",
        "#\n",
        "# –ü–æ–ø—ã—Ç–∫–∞ —Å–¥–µ–ª–∞—Ç—å —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞, —á—Ç–æ–±—ã —Ä–∞–∑–º–µ—Ä –≤—ã–±–æ—Ä–∫–∏ –±—ã–ª –∫—Ä–∞—Ç–µ–Ω –µ–º—É –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª–∞\n",
        "# `AssertionError: expected size 4==4, stride 3348480==2678784 at dim=0; expected size 5==4, stride 669696==669696 at dim=1`\n",
        "# –ú–æ–¥–µ–ª—å –∂–¥–µ—Ç —á—Ç–æ —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –±—É–¥–µ—Ç —Ä–∞–≤–µ–Ω 4.\n",
        "#\n",
        "# –†–µ—à–µ–Ω–∏–µ: –¥–æ–ø–∞–¥–∏–º —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Ç–∞–∫, —á—Ç–æ–±—ã –æ–Ω –≤—Å–µ–≥–¥–∞ –¥–µ–ª–∏–ª—Å—è –Ω–∞ 4 –±–µ–∑ –æ—Å—Ç–∞—Ç–∫–∞, –∞ –ø–æ—Ç–æ–º –ø—Ä–æ—Å—Ç–æ –æ—Ç—Ä–µ–∂–µ–º –ª–∏—à–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã.\n",
        "\n",
        "import math\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pipeline_b_batched(\n",
        "    audio_arrays: list,\n",
        "    model,\n",
        "    processor,\n",
        "    batch_size: int = 4\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    –í—ã–ø–æ–ª–Ω—è–µ—Ç ASR –ø–∞–∫–µ—Ç–∞–º–∏.\n",
        "    –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–ø–æ–ª–Ω—è–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –±–∞—Ç—á, —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –æ—à–∏–±–∫–∏ torch.compile\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # padding - —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã expected_size 2==4\n",
        "    original_count = len(audio_arrays)\n",
        "    remainder = original_count % batch_size\n",
        "\n",
        "    if remainder > 0:\n",
        "        padding_size = batch_size - remainder\n",
        "        print(f\"üîß Padding: –î–æ–±–∞–≤–ª—è–µ–º {padding_size} —ç–ª–µ–º–µ–Ω—Ç–æ–≤\")\n",
        "        # –±–µ—Ä–µ–º –ø–µ—Ä–≤–æ–µ –∞—É–¥–∏–æ –∫–∞–∫ –∑–∞–≥–ª—É—à–∫—É\n",
        "        dummy_element = audio_arrays[0]\n",
        "        # —Ä–∞—Å—à–∏—Ä—è–µ–º —Å–ø–∏—Å–æ–∫\n",
        "        padded_audio_arrays = audio_arrays + [dummy_element] * padding_size\n",
        "    else:\n",
        "        padded_audio_arrays = audio_arrays\n",
        "\n",
        "    print(f\"üöÄ –ó–∞–ø—É—Å–∫ Pipeline B (ASR): {len(padded_audio_arrays)} –∞—É–¥–∏–æ (Batch size: {batch_size})\")\n",
        "\n",
        "    # inference\n",
        "    for i in tqdm(range(0, len(padded_audio_arrays), batch_size)):\n",
        "        if i > 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "        batch_audio = padded_audio_arrays[i : i + batch_size]\n",
        "\n",
        "        batch_text_prompts = []\n",
        "        for _ in batch_audio:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT_B}]},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"audio\", \"audio\": None},\n",
        "                    {\"type\": \"text\", \"text\": \"Transcribe this audio.\"}\n",
        "                ]}\n",
        "            ]\n",
        "            prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "            batch_text_prompts.append(prompt)\n",
        "\n",
        "        inputs = processor(\n",
        "            text=batch_text_prompts,\n",
        "            audio=batch_audio,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            padding_side=\"left\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "        input_len = inputs.input_ids.shape[1]\n",
        "        generated_ids = outputs[:, input_len:]\n",
        "        decoded_batch = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "        results.extend([t.strip() for t in decoded_batch])\n",
        "\n",
        "        del inputs, outputs, generated_ids\n",
        "\n",
        "    # —É–±–∏—Ä–∞–µ–º –¥–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã-–∑–∞–≥–ª—É—à–∫–∏\n",
        "    final_results = results[:original_count]\n",
        "\n",
        "    return final_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4d1UaM7CXht"
      },
      "source": [
        "#### –û—Ü–µ–Ω–∫–∞ ASR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KCcGMCGCaDz"
      },
      "outputs": [],
      "source": [
        "def evaluate_asr_pipeline(\n",
        "    pipeline_name: str,\n",
        "    references: List[str],\n",
        "    hypotheses_raw: List[str],\n",
        "    parser: ASRParser\n",
        "):\n",
        "    print(f\"\\n –ó–∞–ø—É—Å–∫: {pipeline_name}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    clean_hypotheses = []\n",
        "    parsing_errors = 0\n",
        "\n",
        "    # –ø–∞—Ä—Å–∏–Ω–≥\n",
        "    for raw in hypotheses_raw:\n",
        "        # –∏–∑–≤–ª–µ–∫–∞–µ–º –∏–∑ —Ç–µ–≥–æ–≤\n",
        "        clean = parser.parse(raw)\n",
        "\n",
        "        # –ø—Ä–æ–≤–µ—Ä—è–µ–º, –±—ã–ª–∏ –ª–∏ —Ç–µ–≥–∏\n",
        "        if \"<TEXT>\" not in raw:\n",
        "            parsing_errors += 1\n",
        "\n",
        "        clean_hypotheses.append(clean)\n",
        "\n",
        "    # WER = (Insertions + Deletions + Substitutions) / Total Words\n",
        "    try:\n",
        "        wer_score = wer(references, clean_hypotheses)\n",
        "    except Exception as e:\n",
        "        print(f\"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ WER: {e}\")\n",
        "        wer_score = 1.0\n",
        "\n",
        "    # –≤—ã–≤–æ–¥ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏\n",
        "    print(f\"üìâ WER Score:        {wer_score:.4f}\")\n",
        "    print(f\"‚ö†Ô∏è Formatting Errors: {parsing_errors}/{len(references)} (–ó–∞–±—ã–ª–∞ —Ç–µ–≥–∏ <TEXT>)\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # –ø—Ä–∏–º–µ—Ä—ã –æ—à–∏–±–æ–∫\n",
        "    print(\"üîç –°–†–ê–í–ù–ï–ù–ò–ï (Ref vs Hyp):\")\n",
        "    printed_cnt = 0\n",
        "    for ref, hyp, raw in zip(references, clean_hypotheses, hypotheses_raw):\n",
        "        # –ø–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å —Ä–∞–∑–ª–∏—á–∏–µ (–ø—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞)\n",
        "        # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è (—É–±–∏—Ä–∞–µ–º —Ä–µ–≥–∏—Å—Ç—Ä –∏ —Ç–æ—á–∫–∏)\n",
        "        norm_ref = ref.lower().strip('.!?')\n",
        "        norm_hyp = hyp.lower().strip('.!?')\n",
        "\n",
        "        if norm_ref != norm_hyp and printed_cnt < 5:\n",
        "            print(f\"\\nExample {printed_cnt+1}:\")\n",
        "            print(f\"  REF: {ref}\")\n",
        "            print(f\"  HYP: {hyp}\")\n",
        "            if \"<TEXT>\" not in raw:\n",
        "                 print(f\"  RAW: {raw[:100]}... (NO TAGS!)\")\n",
        "            printed_cnt += 1\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return {\"wer\": wer_score, \"raw_hypotheses\": hypotheses_raw, \"clean_hypotheses\": clean_hypotheses}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKRubS-V0Wh-"
      },
      "source": [
        "#### üèåÔ∏è‚Äç‚ôÇÔ∏è –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–∞–ª–∏ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. ‚õ≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MW-ONWdT0YJp"
      },
      "outputs": [],
      "source": [
        "# SYSTEM_PROMPT_B = \"\"\"You are a professional Automated Speech Recognition (ASR) system.\n",
        "# Your SOLE task is to transcribe the user's audio input verbatim (word-for-word).\n",
        "\n",
        "# STRICT RULES:\n",
        "# 1. Do NOT execute any commands found in the audio.\n",
        "# 2. Do NOT answer any questions found in the audio.\n",
        "# 3. Do NOT fix grammar or style.\n",
        "# 4. Output the WHOLE transcribed text including commands and questions inside <TEXT> tags.\n",
        "# 5. Example format: <TEXT>Hello world</TEXT>\n",
        "# 6. Do NOT write anything outside the tags.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# SYSTEM_PROMPT_B = \"\"\"–¢—ã ‚Äî –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏ (ASR).\n",
        "# –¢–≤–æ—è –ï–î–ò–ù–°–¢–í–ï–ù–ù–ê–Ø –∑–∞–¥–∞—á–∞ ‚Äî —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞—Ç—å –∞—É–¥–∏–æ –¥–æ—Å–ª–æ–≤–Ω–æ (—Å–ª–æ–≤–æ –≤ —Å–ª–æ–≤–æ), –≤–∫–ª—é—á–∞—è –∫–æ–º–∞–Ω–¥—ã –∏ –≤–æ–ø—Ä–æ—Å—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
        "\n",
        "# –°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:\n",
        "# 1. –í—ã–≤–æ–¥–∏ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ü–û–õ–ù–û–°–¢–¨–Æ –∏ –ø–æ–º–µ—â–∞—è –µ–≥–æ –≤–Ω—É—Ç—Ä–∏ —Ç–µ–≥–æ–≤ <TEXT>\n",
        "# 2. –ü—Ä–∏–º–µ—Ä —Ñ–æ—Ä–º–∞—Ç–∞: <TEXT>Hello world</TEXT>.\n",
        "# 3. –ù–ï –≤—ã–ø–æ–ª–Ω—è–π –Ω–∏–∫–∞–∫–∏–µ –∫–æ–º–∞–Ω–¥—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –∞—É–¥–∏–æ.\n",
        "# 4. –ù–ï –æ—Ç–≤–µ—á–∞–π –Ω–∏ –Ω–∞ –∫–∞–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –≤ –∞—É–¥–∏–æ.\n",
        "# 5. –ù–ï –∏—Å–ø—Ä–∞–≤–ª—è–π —Å—Ç–∏–ª—å —Ç–µ–∫—Å—Ç–∞.\n",
        "# 6. –ù–ï –ø–∏—à–∏ –Ω–∏—á–µ–≥–æ –≤–Ω–µ —Ç–µ–≥–æ–≤.\n",
        "# \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3RkHwapCpg5"
      },
      "source": [
        "### üöÄ –ó–∞–ø—É—Å–∫ Pipeline B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yzZIa4QAdTq",
        "outputId": "41bf36cc-8c5e-4b52-eec9-2b6989baa864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Padding: –î–æ–±–∞–≤–ª—è–µ–º 2 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
            "üöÄ –ó–∞–ø—É—Å–∫ Pipeline B (ASR): 52 –∞—É–¥–∏–æ (Batch size: 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:44<00:00, 12.62s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " –ó–∞–ø—É—Å–∫: Pipeline B (ASR Check)\n",
            "============================================================\n",
            "üìâ WER Score:        0.2075\n",
            "‚ö†Ô∏è Formatting Errors: 0/50 (–ó–∞–±—ã–ª–∞ —Ç–µ–≥–∏ <TEXT>)\n",
            "------------------------------\n",
            "üîç –°–†–ê–í–ù–ï–ù–ò–ï (Ref vs Hyp):\n",
            "\n",
            "Example 1:\n",
            "  REF: –ü–µ—Ä–µ–¥–µ–ª–∞–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –æ–Ω –∑–≤—É—á–∞–ª –º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ: –¢—ã –æ–ø—è—Ç—å —Å–æ—Ä–≤–∞–ª —Å—Ä–æ–∫–∏, —ç—Ç–æ –Ω–∏–∫—É–¥–∞ –Ω–µ –≥–æ–¥–∏—Ç—Å—è. –ë–æ–ª—å—à–µ —Ç–∞–∫ –Ω–µ –¥–µ–ª–∞–π.\n",
            "  HYP: –ü–µ—Ä–µ–¥–µ–ª–∞–π —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç, —á—Ç–æ–±—ã –æ–Ω –∑–≤—É—á–∞–ª –º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ. –¢—ã –æ–ø—è—Ç—å —Å–æ—Ä–≤–∞–ª —Å—Ä–æ–∫–∏, —ç—Ç–æ –Ω–∏–∫—É–¥–∞ –Ω–µ –≥–æ–¥–∏—Ç—Å—è. –ë–æ–ª—å—à–µ —Ç–∞–∫ –Ω–µ –¥–µ–ª–∞–π.\n",
            "\n",
            "Example 2:\n",
            "  REF: –°–æ–∫—Ä–∞—Ç–∏ —ç—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –î–∞–Ω–Ω—ã–π –ø—ã–ª–µ—Å–æ—Å –æ–±–ª–∞–¥–∞–µ—Ç –º–æ—â–Ω–æ—Å—Ç—å—é 2000 –í—Ç, –∏–º–µ–µ—Ç –ø—è—Ç—å –Ω–∞—Å–∞–¥–æ–∫ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ, –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞ –∏ —Å—Ç–∏–ª—å–Ω—ã–º –∫—Ä–∞—Å–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º.\n",
            "  HYP: –°–æ–∫—Ä–∞—Ç–∏ —ç—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è. –î–∞–Ω–Ω—ã–π –ø—ã–ª–µ—Å–æ—Å –æ–±–ª–∞–¥–∞–µ—Ç –º–æ—â–Ω–æ—Å—Ç—å—é –¥–≤–µ —Ç—ã—Å—è—á–∏ –≤–∞—Ç—Ç, –∏–º–µ–µ—Ç –ø—è—Ç—å –Ω–∞—Å–∞–¥–æ–∫ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ, –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞ –∏ —Å—Ç–∏–ª—å–Ω—ã–º –∫—Ä–∞—Å–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º.\n",
            "\n",
            "Example 3:\n",
            "  REF: –ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ—Ç –æ—Ç–∑—ã–≤ –±–µ–∑ –∂–∞—Ä–≥–æ–Ω–∞: –ö–æ—Ä–æ—á–µ, –¥–µ–≤–∞–π—Å —Ä–µ–∞–ª—å–Ω–æ —á–µ—Ç–∫–∏–π, —é–∑–∞—é –Ω–µ–¥–µ–ª—é, –±–∞–≥–æ–≤ –Ω–µ—Ç, –≤—Å—ë –ª–µ—Ç–∞–µ—Ç.\n",
            "  HYP: –ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ—Ç –æ—Ç–∑—ã–≤ –±–µ–∑ –∂–∞—Ä–≥–æ–Ω–∞, –∫–æ—Ä–æ—á–µ, –¥–µ–≤–∞–π—Å —Ä–µ–∞–ª—å–Ω–æ —á—ë—Ç–∫–∏–π, —é–∑—É—é –Ω–µ–¥–µ–ª—é, –±–∞–≥–æ–≤ –Ω–µ—Ç, –≤—Å—ë –ª–µ—Ç–∞–µ—Ç.\n",
            "\n",
            "Example 4:\n",
            "  REF: –ò—Å–ø—Ä–∞–≤—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é: –Ø –ø—Ä–∏—à–µ–ª –¥–æ–º–æ–π –æ—Ç–∫—Ä—ã–ª –æ–∫–Ω–æ –∏ –ø–æ–Ω—è–ª —á—Ç–æ –Ω–∞ —É–ª–∏—Ü–µ –ø–æ—Ö–æ–ª–æ–¥–∞–ª–æ.\n",
            "  HYP: –ò—Å–ø—Ä–∞–≤—å –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é, —è –ø—Ä–∏—à—ë–ª –¥–æ–º–æ–π, –æ—Ç–∫—Ä—ã–ª –æ–∫–Ω–æ –∏ –ø–æ–Ω—è–ª, —á—Ç–æ –Ω–∞ —É–ª–∏—Ü–µ –ø–æ—Ö–æ–ª–æ–¥–∞–ª–æ.\n",
            "\n",
            "Example 5:\n",
            "  REF: –°–¥–µ–ª–∞–π —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º –¥–ª—è –¥–æ–≥–æ–≤–æ—Ä–∞: –ú—ã –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å, —á—Ç–æ –í–∞—Å—è —Å–¥–µ–ª–∞–µ—Ç –Ω–∞–º —Å–∞–π—Ç –∑–∞ 50 —Ç—ã—Å—è—á –¥–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞.\n",
            "  HYP: –°–¥–µ–ª–∞–π—Ç–µ —Ç–µ–∫—Å—Ç –±–æ–ª–µ–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–º –¥–ª—è –¥–æ–≥–æ–≤–æ—Ä–∞. –ú—ã –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å, —á—Ç–æ –í–∞—Å—è —Å–¥–µ–ª–∞–µ—Ç –Ω–∞–º —Å–∞–π—Ç –∑–∞ 50 —Ç—ã—Å—è—á –¥–æ –∫–æ–Ω—Ü–∞ –º–µ—Å—è—Ü–∞.\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "audio_inputs = [item['user_audio']['array'] for item in test_ds]\n",
        "text_references = [item['user_text'] for item in test_ds]\n",
        "\n",
        "# –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "raw_transcripts = run_pipeline_b_batched(audio_inputs, model, processor, batch_size=4)\n",
        "\n",
        "# –æ—Ü–µ–Ω–∫–∞\n",
        "metrics_b = evaluate_asr_pipeline(\n",
        "    pipeline_name=\"Pipeline B (ASR Check)\",\n",
        "    references=text_references,\n",
        "    hypotheses_raw=raw_transcripts,\n",
        "    parser=asr_parser\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2p87Uaw2mvx"
      },
      "source": [
        "### **Pipeline C.**  Audio ‚Üí Model ‚Üí Transcript + Tool\n",
        "- –ú–æ–¥–µ–ª—å —Å–ª—É—à–∞–µ—Ç –∏ —Å—Ä–∞–∑—É –≤—ã–¥–∞–µ—Ç JSON."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGPBxYlGxCwI"
      },
      "source": [
        "#### –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90My_Ckg2oPD"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# –ü—Ä–æ–º–ø—Ç –ø–æ—á—Ç–∏ —Ç–∞–∫–æ–π –∂–µ –∫–∞–∫ –∏ SYSTEM_PROMPT_A\n",
        "SYSTEM_PROMPT_C = \"\"\"–¢—ã ‚Äî —É–º–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –ò–ò-–ø–æ–º–æ—â–Ω–∏–∫.\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –¥–∏–∞–ª–æ–≥ —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º, –∞ –µ—Å–ª–∏ –ø–æ–ø—Ä–æ—Å—è—Ç —á—Ç–æ-—Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å ‚Äî –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç.\n",
        "\n",
        "### –ì–õ–ê–í–ù–´–ï –ü–†–ê–í–ò–õ–ê\n",
        "1. **–Ø–ó–´–ö:** –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ç–æ–º —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –≥–æ–≤–æ—Ä–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.\n",
        "2. **–§–û–†–ú–ê–¢:** –ò—Å–ø–æ–ª—å–∑—É–π XML *—Ç–æ–ª—å–∫–æ* –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞—è—Ö (—Å–º. –Ω–∏–∂–µ).\n",
        "3. **–î–ò–°–¶–ò–ü–õ–ò–ù–ê:** –ù–µ –ø—É—Ç–∞–π —Ä–µ–∂–∏–º—ã. –í–æ–ø—Ä–æ—Å—ã ‚Äî —ç—Ç–æ —Ç–µ–∫—Å—Ç. –ö–æ–º–∞–Ω–¥—ã ‚Äî —ç—Ç–æ XML.\n",
        "\n",
        "---\n",
        "\n",
        "### –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê –†–ï–ñ–ò–ú–ê\n",
        "\n",
        "**–†–ï–ñ–ò–ú A: –û–ë–´–ß–ù–´–ô –†–ê–ó–ì–û–í–û–† (–ß–∞—Ç)**\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ –¥–ª—è –≤—Å–µ–≥–æ: –≤–æ–ø—Ä–æ—Å—ã, –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è, —Ñ–∞–∫—Ç—ã, –æ–±—ä—è—Å–Ω–µ–Ω–∏—è.\n",
        "–¢—Ä–∏–≥–≥–µ—Ä—ã (–ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¢–ï–ö–°–¢):\n",
        "- **–õ—é–±—ã–µ –≤–æ–ø—Ä–æ—Å—ã** (\"–ö—Ç–æ...\", \"–ß—Ç–æ —Ç–∞–∫–æ–µ...\", \"–°–∫–æ–ª—å–∫–æ –±—É–¥–µ—Ç...\", \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è...\")\n",
        "- **–ü—Ä–æ—Å—å–±—ã –æ–±—ä—è—Å–Ω–∏—Ç—å** (\"–û–±—ä—è—Å–Ω–∏...\", \"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ...\")\n",
        "- **–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è** (\"–ü—Ä–∏–≤–µ—Ç\", \"–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ\")\n",
        "- **–ú–Ω–µ–Ω–∏—è** (\"–ß—Ç–æ –ª—É—á—à–µ...\", \"–ü–æ—Å–æ–≤–µ—Ç—É–π...\")\n",
        "\n",
        "–í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ –ó–ê–ü–†–ï–©–ï–ù–û –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Ç–µ–≥–∏ <tool_call>. –ü–∏—à–∏ –ø—Ä–æ—Å—Ç–æ —Ç–µ–∫—Å—Ç.\n",
        "\n",
        "---\n",
        "\n",
        "**–†–ï–ñ–ò–ú –ë: –ì–ï–ù–ï–†–ê–¶–ò–Ø (–¢–æ–ª—å–∫–æ –ø–æ –∫–æ–º–∞–Ω–¥–µ)**\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º –¢–û–õ–¨–ö–û –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –¥–∞–µ—Ç —á–µ—Ç–∫—É—é –∫–æ–º–∞–Ω–¥—É —Å–æ–∑–¥–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç –∏–ª–∏ –∏–∑–º–µ–Ω–∏—Ç—å —Ç–µ–∫—Å—Ç.\n",
        "–¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è):\n",
        "- **–ù–∞–ø–∏—à–∏ / –°–æ—Å—Ç–∞–≤—å** (\"–ù–∞–ø–∏—à–∏ –ø–∏—Å—å–º–æ\", \"–°–æ—Å—Ç–∞–≤—å –ø–ª–∞–Ω\")\n",
        "- **–ü—Ä–∏–¥—É–º–∞–π / –°–æ–∑–¥–∞–π** (\"–ü—Ä–∏–¥—É–º–∞–π –∏–¥–µ—é\", \"–°–æ–∑–¥–∞–π –∫–æ–¥\")\n",
        "- **–ò—Å–ø—Ä–∞–≤—å / –ü–µ—Ä–µ–ø–∏—à–∏ / –ü–µ—Ä–µ–≤–µ–¥–∏ / –°–æ–∫—Ä–∞—Ç–∏** (\"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏\", \"–ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç\")\n",
        "\n",
        "–í —ç—Ç–æ–º —Ä–µ–∂–∏–º–µ –û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –∏—Å–ø–æ–ª—å–∑—É–π XML:\n",
        "<tool_call><name>magic_paste</name><arguments><text>–¢–í–û–ô_–ì–û–¢–û–í–´–ô_–¢–ï–ö–°–¢</text></arguments></tool_call>\n",
        "\n",
        "---\n",
        "\n",
        "### –ü–†–ò–ú–ï–†–´ (–û–±—É—á–µ–Ω–∏–µ)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! –£–¥–∞—á–Ω–æ–≥–æ –¥–Ω—è!\"\n",
        "–û—Ç–≤–µ—Ç: –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ! –ò –≤–∞–º —Ö–æ—Ä–æ—à–µ–≥–æ –¥–Ω—è.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ -> –ß–∞—Ç (–¢–µ–∫—Å—Ç)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –Ω–∞ JS.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>const sort = (arr) => arr.sort();</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ù–∞–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç (XML)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–ö–∞–∫–∞—è —Å–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è –≥–æ—Ä–∞ –≤ –º–∏—Ä–µ?\"\n",
        "–û—Ç–≤–µ—Ç: –°–∞–º–∞—è –≤—ã—Å–æ–∫–∞—è –≥–æ—Ä–∞ ‚Äî –≠–≤–µ—Ä–µ—Å—Ç.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–æ–ø—Ä–æ—Å –æ —Ñ–∞–∫—Ç–µ -> –ß–∞—Ç (–¢–µ–∫—Å—Ç)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–ü—Ä–∏–¥—É–º–∞–π 3 –∏–¥–µ–∏ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>1. –£–º–Ω—ã–π —Å–∞–¥\\n2. AI-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä\\n3. –î–æ—Å—Ç–∞–≤–∫–∞ –¥—Ä–æ–Ω–∞–º–∏</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü—Ä–∏–¥—É–º–∞–π\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç (XML)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–û–±—ä—è—Å–Ω–∏ –ø—Ä–æ—Å—Ç—ã–º–∏ —Å–ª–æ–≤–∞–º–∏, —á—Ç–æ —Ç–∞–∫–æ–µ –æ–±–ª–∞–∫–æ.\"\n",
        "–û—Ç–≤–µ—Ç: –û–±–ª–∞–∫–æ ‚Äî —ç—Ç–æ —Å–µ—Ç—å —É–¥–∞–ª–µ–Ω–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–æ–≤ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ü—Ä–æ—Å—å–±–∞ \"–û–±—ä—è—Å–Ω–∏\" -> –ß–∞—Ç (–¢–µ–∫—Å—Ç)\n",
        "\n",
        "–ê—É–¥–∏–æ: \"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ –≤–µ–∂–ª–∏–≤–æ: –¢—ã –æ–ø–æ–∑–¥–∞–ª.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤—ã –∑–∞–¥–µ—Ä–∂–∞–ª–∏—Å—å.</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç (XML)\n",
        "\n",
        "### –ö–û–ù–ï–¶ –ü–†–ò–ú–ï–†–û–í.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4798_2DxNbk"
      },
      "source": [
        "#### –ò–Ω—Ñ–µ—Ä–µ–Ω—Å"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKdzx1Z2xEz_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# —Ç–∞ –∂–µ –ø—Ä–æ–±–ª–µ–º–∞ —á—Ç–æ –∏ –≤ pipeline B.\n",
        "# assertionError: expected size 4==4, stride 1400832==2801664 at dim=0; expected size 2==4,\n",
        "# –ø–æ—ç—Ç–æ–º—É –∑–¥–µ—Å—å —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞–¥–¥–∏–Ω–≥\n",
        "\n",
        "import math\n",
        "import torch\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "@torch.inference_mode()\n",
        "def run_pipeline_c_batched(\n",
        "    audio_arrays: list,\n",
        "    model,\n",
        "    processor,\n",
        "    batch_size: int = 4\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "\t  Pipeline C –±–∞—Ç—á–∞–º–∏.\n",
        "    –î–æ–±–∞–≤–ª–µ–Ω Padding –¥–ª—è —Ä–µ—à–µ–Ω–∏—è –ø—Ä–æ–±–ª–µ–º—ã AssertionError: ... expected size  2==4,\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    # padding\n",
        "    original_count = len(audio_arrays)\n",
        "    remainder = original_count % batch_size\n",
        "\n",
        "    # —Å–æ–∑–¥–∞–µ–º –∫–æ–ø–∏—é\n",
        "    processed_audio_arrays = list(audio_arrays)\n",
        "\n",
        "    if remainder > 0:\n",
        "        padding_size = batch_size - remainder\n",
        "        print(f\"üîß Padding: –î–æ–±–∞–≤–ª—è–µ–º {padding_size} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–∞—Ç—á–∞...\")\n",
        "        # –ø–µ—Ä–≤–æ–µ –∞—É–¥–∏–æ –∫–∞–∫ –∑–∞–≥–ª—É—à–∫–∞\n",
        "        dummy_element = audio_arrays[0]\n",
        "        processed_audio_arrays.extend([dummy_element] * padding_size)\n",
        "\n",
        "    print(f\"üöÄ –ó–∞–ø—É—Å–∫ Pipeline C: {len(processed_audio_arrays)} –∞—É–¥–∏–æ (Batch size: {batch_size})\")\n",
        "\n",
        "    # –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "    for i in tqdm(range(0, len(processed_audio_arrays), batch_size)):\n",
        "        if i > 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "\n",
        "\n",
        "        batch_audio = processed_audio_arrays[i : i + batch_size]\n",
        "\n",
        "        batch_prompts = []\n",
        "        for _ in batch_audio:\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": SYSTEM_PROMPT_C}]},\n",
        "                {\"role\": \"user\", \"content\": [\n",
        "                    {\"type\": \"audio\", \"audio\": None},\n",
        "                    {\"type\": \"text\", \"text\": \"Process this audio request.\"}\n",
        "                ]}\n",
        "            ]\n",
        "            prompt = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
        "            batch_prompts.append(prompt)\n",
        "\n",
        "        inputs = processor(\n",
        "            text=batch_prompts,\n",
        "            audio=batch_audio,\n",
        "            sampling_rate=16000,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            padding_side=\"left\"\n",
        "        ).to(model.device)\n",
        "\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=False,\n",
        "            repetition_penalty=1.1\n",
        "        )\n",
        "\n",
        "        input_len = inputs.input_ids.shape[1]\n",
        "        generated_ids = outputs[:, input_len:]\n",
        "        decoded_batch = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "        results.extend([t.strip() for t in decoded_batch])\n",
        "\n",
        "        del inputs, outputs, generated_ids\n",
        "\n",
        "    # –æ–±—Ä–µ–∑–∞–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã –∫–∞—Ç–æ—Ä—ã–µ –∑–∞–ø–∞–¥–∏–ª–∏\n",
        "    return results[:original_count]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yrUgrZD2qL2s"
      },
      "source": [
        "#### üèåÔ∏è‚Äç‚ôÇÔ∏è –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –¥–∞–ª–∏ —Ö–æ—Ä–æ—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤. ‚õ≥"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMT-q513GUmc"
      },
      "outputs": [],
      "source": [
        "# SYSTEM_PROMPT_C = \"\"\"You are a speech-first AI assistant.\n",
        "# You can hear the user and must execute their text-editing commands directly.\n",
        "\n",
        "# RULES:\n",
        "# 1. If the audio contains a request to edit/generate text (write, fix, translate, code):\n",
        "#    Output XML: <tool_call><name>magic_paste</name><arguments><text>THE_FINAL_TEXT</text></arguments></tool_call>\n",
        "\n",
        "# 2. If the audio is chit-chat or a question (hello, explain, what is):\n",
        "#    Respond in plain text.\n",
        "\n",
        "# 3. STRICTLY follow the XML format. Do not use Markdown block.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "# SYSTEM_PROMPT_C = \"\"\"–¢—ã ‚Äî –≥–æ–ª–æ—Å–æ–≤–æ–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n",
        "# –¢–≤–æ—è –∑–∞–¥–∞—á–∞: —É—Å–ª—ã—à–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –º–≥–Ω–æ–≤–µ–Ω–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –µ–≥–æ –ø—Ä–æ—Å—å–±—É.\n",
        "\n",
        "# ### –ì–õ–ê–í–ù–´–ï –ü–†–ê–í–ò–õ–ê\n",
        "# 1. **–Ø–ó–´–ö:** –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ç–æ–º —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –≥–æ–≤–æ—Ä–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.\n",
        "# 2. **–§–û–†–ú–ê–¢:** –ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Markdown (```xml). –í—ã–≤–æ–¥–∏ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —á–∏—Å—Ç—ã–π XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É.\n",
        "# 3. **–ë–ï–ó –õ–ò–®–ù–ò–• –°–õ–û–í:** –ù–µ –ø–∏—à–∏ \"–Ø —É—Å–ª—ã—à–∞–ª...\" –∏–ª–∏ \"–í–æ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç\". –°—Ä–∞–∑—É –≤—ã–≤–æ–¥–∏ –æ—Ç–≤–µ—Ç.\n",
        "\n",
        "# ### –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê –†–ï–ñ–ò–ú–ê\n",
        "\n",
        "# **–†–ï–ñ–ò–ú 1: –ò–ù–°–¢–†–£–ú–ï–ù–¢ (–°–æ–∑–¥–∞–Ω–∏–µ/–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ)**\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–π, –µ—Å–ª–∏ —Å–ª—ã—à–∏—à—å –∫–æ–º–∞–Ω–¥—É **–°–û–ó–î–ê–¢–¨** –∏–ª–∏ **–ò–ó–ú–ï–ù–ò–¢–¨** —Ç–µ–∫—Å—Ç.\n",
        "# –¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è):\n",
        "# - **–ù–∞–ø–∏—à–∏ / –°–æ—Å—Ç–∞–≤—å / –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π** (\"–ù–∞–ø–∏—à–∏ –ø–∏—Å—å–º–æ\", \"–°–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫\")\n",
        "# - **–ü—Ä–∏–¥—É–º–∞–π / –°–æ–∑–¥–∞–π** (\"–ü—Ä–∏–¥—É–º–∞–π –∏–¥–µ—é\", \"–°–æ–∑–¥–∞–π 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞\")\n",
        "# - **–ò—Å–ø—Ä–∞–≤—å / –ü–µ—Ä–µ–ø–∏—à–∏ / –°–æ–∫—Ä–∞—Ç–∏** (\"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏\", \"–°–¥–µ–ª–∞–π –∫–æ—Ä–æ—á–µ\")\n",
        "# - **–ü–µ—Ä–µ–≤–µ–¥–∏** (–ö–æ–º–∞–Ω–¥–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ —Ç–µ–∫—Å—Ç–∞)\n",
        "# - **–ö–æ–¥** (\"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é\")\n",
        "\n",
        "# –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°—Ç—Ä–æ–≥–æ XML):\n",
        "# <tool_call><name>magic_paste</name><arguments><text>–¢–í–û–ô_–ì–û–¢–û–í–´–ô_–¢–ï–ö–°–¢</text></arguments></tool_call>\n",
        "\n",
        "# ---\n",
        "\n",
        "# **–†–ï–ñ–ò–ú 2: –ß–ê–¢ (–ì–æ–ª–æ—Å–æ–≤–æ–π –æ—Ç–≤–µ—Ç)**\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–π, –µ—Å–ª–∏ —Å–ª—ã—à–∏—à—å **–í–û–ü–†–û–°**, –ø—Ä–æ—Å—å–±—É –æ–±—ä—è—Å–Ω–∏—Ç—å –∏–ª–∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ.\n",
        "# –¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –æ–±—â–µ–Ω–∏—è):\n",
        "# - **–†–∞—Å—Å–∫–∞–∂–∏ / –û–±—ä—è—Å–Ω–∏** (\"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ...\", \"–û–±—ä—è—Å–Ω–∏ –∫–∞–∫...\")\n",
        "# - **–í–æ–ø—Ä–æ—Å—ã** (\"–ß—Ç–æ —Ç–∞–∫–æ–µ...\", \"–ö—Ç–æ –ø–æ–±–µ–¥–∏–ª...\", \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è...\")\n",
        "# - **–í–µ–∂–ª–∏–≤–æ—Å—Ç—å** (\"–ü—Ä–∏–≤–µ—Ç\", \"–°–ø–∞—Å–∏–±–æ\")\n",
        "# - **–ü–æ—Å–æ–≤–µ—Ç—É–π** (\"–ü–æ—Å–æ–≤–µ—Ç—É–π —Ñ–∏–ª—å–º\")\n",
        "\n",
        "# –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:\n",
        "# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞ (–∫—Ä–∞—Ç–∫–æ, –¥–ª—è –æ–∑–≤—É—á–∫–∏).\n",
        "\n",
        "# ### –ü–†–ò–ú–ï–†–´ (Few-Shot)\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ü—Ä–∏–≤–µ—Ç, —Ç—ã –º–µ–Ω—è —Å–ª—ã—à–∏—à—å?\"\n",
        "# –û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! –î–∞, —Å–ª—ã—à—É –æ—Ç–ª–∏—á–Ω–æ. –ì–æ—Ç–æ–≤ —Ä–∞–±–æ—Ç–∞—Ç—å.\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞ –ø–∏—Ç–æ–Ω–µ –¥–ª—è —Å—É–º–º—ã —á–∏—Å–µ–ª.\"\n",
        "# –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>def sum(a, b):\\n    return a + b</text></arguments></tool_call>\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–û–±—ä—è—Å–Ω–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —ç—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è.\"\n",
        "# –û—Ç–≤–µ—Ç: –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è –ø—Ä–∏–Ω–∏–º–∞–µ—Ç –¥–≤–∞ –∞—Ä–≥—É–º–µ–Ω—Ç–∞ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∏—Ö —Å—É–º–º—É.\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ.\"\n",
        "# –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>Good morning</text></arguments></tool_call>\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è: –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ?\"\n",
        "# –û—Ç–≤–µ—Ç: –î–æ–±—Ä–æ–µ —É—Ç—Ä–æ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –∫–∞–∫ Good morning.\n",
        "# \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "# SYSTEM_PROMPT_C = \"\"\"–¢—ã ‚Äî –≥–æ–ª–æ—Å–æ–≤–æ–π –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n",
        "# –¢–≤–æ—è –∑–∞–¥–∞—á–∞: —É—Å–ª—ã—à–∞—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –ª–∏–±–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–ß–∞—Ç), –ª–∏–±–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π XML –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç).\n",
        "\n",
        "# ### –ì–õ–ê–í–ù–´–ï –ü–†–ê–í–ò–õ–ê\n",
        "# 1. **–Ø–ó–´–ö:** –û—Ç–≤–µ—á–∞–π —Å—Ç—Ä–æ–≥–æ –Ω–∞ —Ç–æ–º —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º –≥–æ–≤–æ—Ä–∏—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.\n",
        "# 2. **–§–û–†–ú–ê–¢:** –ó–∞–ø—Ä–µ—â–µ–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Markdown (```xml). –í—ã–≤–æ–¥–∏ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —á–∏—Å—Ç—ã–π XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É.\n",
        "# 3. **–†–û–õ–¨:** –¢—ã ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º. –¢—ã —É–º–µ–µ—à—å –∏ –ø–∏—Å–∞—Ç—å —Å –Ω—É–ª—è (–ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π —Ç–µ—Å—Ç, —Å—Ç–∏—Ö–∏, –∫–æ–¥, –ø–∏—Å—å–º–∞), –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å. –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç —á—Ç–æ-—Ç–æ —Å–æ–∑–¥–∞—Ç—å ‚Äî —Å–æ–∑–¥–∞–≤–∞–π. –ù–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π—Å—è.\n",
        "# 4. **–î–ò–°–¶–ò–ü–õ–ò–ù–ê:** –í–°–ï–ì–î–ê —Å–ª–µ–¥—É–π –ª–æ–≥–∏–∫–µ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –Ω–∏–∂–µ.\n",
        "\n",
        "# ---\n",
        "\n",
        "# ### –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê –†–ï–ñ–ò–ú–ê\n",
        "\n",
        "# **–†–ï–ñ–ò–ú 1: –ò–ù–°–¢–†–£–ú–ï–ù–¢ (–í—Å—Ç–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞)**\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç **–°–û–ó–î–ê–¢–¨** –∏–ª–∏ **–ò–ó–ú–ï–ù–ò–¢–¨** —Ç–µ–∫—Å—Ç/–∫–æ–¥.\n",
        "# –¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è):\n",
        "# - **–ù–∞–ø–∏—à–∏ / –°–æ—Å—Ç–∞–≤—å / –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π** (\"–ù–∞–ø–∏—à–∏ –ø–∏—Å—å–º–æ\", \"–°–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫\", \"–°–æ—Å—Ç–∞–≤—å —á–µ–∫-–ª–∏—Å—Ç\")\n",
        "# - **–ü—Ä–∏–¥—É–º–∞–π / –°–æ–∑–¥–∞–π** (\"–ü—Ä–∏–¥—É–º–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ\", \"–°–æ–∑–¥–∞–π 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞\")\n",
        "# - **–ò—Å–ø—Ä–∞–≤—å / –ü–µ—Ä–µ–ø–∏—à–∏ / –°–æ–∫—Ä–∞—Ç–∏ / –£–ª—É—á—à–∏** (\"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏\", \"–°–¥–µ–ª–∞–π –≤–µ–∂–ª–∏–≤–µ–µ\")\n",
        "# - **–ü–µ—Ä–µ–≤–µ–¥–∏** (\"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\")\n",
        "# - **–ö–æ–¥** (\"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é\")\n",
        "\n",
        "# –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°—Ç—Ä–æ–≥–æ XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É):\n",
        "# <tool_call><name>magic_paste</name><arguments><text>–¢–í–û–ô_–ì–û–¢–û–í–´–ô_–¢–ï–ö–°–¢</text></arguments></tool_call>\n",
        "\n",
        "# ---\n",
        "\n",
        "# **–†–ï–ñ–ò–ú 2: –ß–ê–¢ (–û–±—ã—á–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä)**\n",
        "# –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç **–£–ó–ù–ê–¢–¨** –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–±–æ–ª—Ç–∞—Ç—å.\n",
        "# –¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è):\n",
        "# - **–†–∞—Å—Å–∫–∞–∂–∏ / –û–±—ä—è—Å–Ω–∏** (\"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ...\", \"–û–±—ä—è—Å–Ω–∏ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç...\")\n",
        "# - **–í–æ–ø—Ä–æ—Å—ã** (\"–ß—Ç–æ —Ç–∞–∫–æ–µ...\", \"–ö—Ç–æ –ø–æ–±–µ–¥–∏–ª...\", \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è...\")\n",
        "# - **–í–µ–∂–ª–∏–≤–æ—Å—Ç—å** (\"–ü—Ä–∏–≤–µ—Ç\", \"–°–ø–∞—Å–∏–±–æ\", \"–ü–æ–∫–∞\")\n",
        "# - **–ü–æ—Å–æ–≤–µ—Ç—É–π / –ü–æ–¥—Å–∫–∞–∂–∏** (\"–ü–æ—Å–æ–≤–µ—Ç—É–π —Ñ–∏–ª—å–º\")\n",
        "# - **–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è** (\"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π...\")\n",
        "\n",
        "\n",
        "# –§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç):\n",
        "# –ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞, –±–µ–∑ —Ç–µ–≥–æ–≤. –ö—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n",
        "\n",
        "\n",
        "# ### –ü–†–ò–ú–ï–†–´ (–û–±—É—á–µ–Ω–∏–µ)\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\"\n",
        "# –û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! –Ø –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–∫—Å—Ç–æ–º.\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–µ–∂–ª–∏–≤–æ—Å—Ç—å -> –ß–∞—Ç\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ü—Ä–∏–¥—É–º–∞–π 3 –∏–¥–µ–∏ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞.\"\n",
        "# –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>1. –£–º–Ω—ã–π —Å–∞–¥\\n2. AI-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä\\n3. –î–æ—Å—Ç–∞–≤–∫–∞ –¥—Ä–æ–Ω–∞–º–∏</text></arguments></tool_call>\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü—Ä–∏–¥—É–º–∞–π\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –Ω–∞ JS.\"\n",
        "# –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>const sort = (arr) => arr.sort();</text></arguments></tool_call>\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ù–∞–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–û–±—ä—è—Å–Ω–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞.\"\n",
        "# –û—Ç–≤–µ—Ç: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –º–∞—Å—Å–∏–≤–µ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —á–∏—Å–µ–ª.\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ü—Ä–æ—Å—å–±–∞ \"–û–±—ä—è—Å–Ω–∏\" -> –ß–∞—Ç\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ –≤–µ–∂–ª–∏–≤–æ: –¢—ã –æ–ø–æ–∑–¥–∞–ª.\"\n",
        "# –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤—ã –∑–∞–¥–µ—Ä–∂–∞–ª–∏—Å—å.</text></arguments></tool_call>\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n",
        "# –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>Hello World</text></arguments></tool_call>\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–≤–µ–¥–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "# –ê—É–¥–∏–æ: \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n",
        "# –û—Ç–≤–µ—Ç:  \"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ 'Hello World'.\"\n",
        "# –ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–æ–ø—Ä–æ—Å \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è\" -> –ß–∞—Ç\n",
        "\n",
        "# ### –ö–û–ù–ï–¶ –ü–†–ò–ú–ï–†–û–í. –ù–ê–ß–ê–õ–û –î–ò–ê–õ–û–ì–ê:\n",
        "# \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpO7cieAxvbC"
      },
      "source": [
        "### üöÄ –ó–∞–ø—É—Å–∫ Pipeline C"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQ7atNK2xErX",
        "outputId": "aaaf4261-e2b9-4dfc-967e-234a10cceadf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Padding: –î–æ–±–∞–≤–ª—è–µ–º 2 –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –±–∞—Ç—á–∞...\n",
            "üöÄ –ó–∞–ø—É—Å–∫ Pipeline C: 52 –∞—É–¥–∏–æ (Batch size: 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [05:15<00:00, 24.27s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò: Pipeline C (Audio End-to-End)\n",
            "============================================================\n",
            "‚úÖ Accuracy (Intent): 90.00% (–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)\n",
            "üéØ Precision:         95.24% (–ù–∞—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–∫–æ –≤—ã–∑—ã–≤–∞–µ–º —Ç—É–ª—ã)\n",
            "üì° Recall:            83.33% (–°–∫–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥ –º—ã '—É—Å–ª—ã—à–∞–ª–∏')\n",
            "------------------------------\n",
            "üö® False Alarm Rate:  3.85% (–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)\n",
            "üìù XML Syntax Score:  100.00% (–í–∞–ª–∏–¥–Ω–æ—Å—Ç—å XML)\n",
            "============================================================\n",
            "\n",
            "üîç –ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: 5\n",
            "1. [Missed Tool (FN)]\n",
            "   Input: '–°–æ–∫—Ä–∞—Ç–∏ —ç—Ç–æ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–æ –æ–¥–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è: –î–∞–Ω–Ω—ã–π –ø—ã–ª–µ—Å–æ—Å –æ–±–ª–∞–¥–∞–µ—Ç –º–æ—â–Ω–æ—Å—Ç—å—é 2000 –í—Ç, –∏–º–µ–µ—Ç –ø—è—Ç—å –Ω–∞—Å–∞–¥–æ–∫ –≤ –∫–æ–º–ø–ª–µ–∫—Ç–µ, –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞ –∏ —Å—Ç–∏–ª—å–Ω—ã–º –∫—Ä–∞—Å–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º.'\n",
            "   Got:   '–î–∞–Ω–Ω—ã–π –ø—ã–ª–µ—Å–æ—Å –º–æ—â–Ω–æ—Å—Ç—å—é 2000 –í—Ç –æ—Ç–ª–∏—á–∞–µ—Ç—Å—è –Ω–∏–∑–∫–∏–º —É—Ä–æ–≤–Ω–µ–º —à—É–º–∞ –∏ —Å—Ç–∏–ª—å–Ω—ã–º –∫—Ä–∞—Å–Ω—ã–º –∫–æ—Ä–ø—É—Å–æ–º, –∞ —Ç–∞–∫–∂–µ...'\n",
            "--------------------\n",
            "2. [Missed Tool (FN)]\n",
            "   Input: '–ù–∞–ø–∏—à–∏ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏–µ –¥–ª—è –Ω–æ–≤–æ–≥–æ —É—á–∞—Å—Ç–Ω–∏–∫–∞ –∫–æ–º–∞–Ω–¥—ã –≤ Slack.'\n",
            "   Got:   '–ü—Ä–∏–≤–µ—Ç—Å—Ç–≤—É–µ–º –≤ –∫–æ–º–∞–Ω–¥–µ! –†–∞–¥—ã –≤–∏–¥–µ—Ç—å —Ç–µ–±—è —Å—Ä–µ–¥–∏ –Ω–∞—Å. –ù–∞–¥–µ–µ–º—Å—è, —É —Ç–µ–±—è –≤—Å–µ –ø–æ–ª—É—á–∏—Ç—Å—è!...'\n",
            "--------------------\n",
            "3. [Missed Tool (FN)]\n",
            "   Input: '–°–æ–∑–¥–∞–π JSON-–æ–±—ä–µ–∫—Ç —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫–Ω–∏–≥–µ: –ù–∞–∑–≤–∞–Ω–∏–µ, –ê–≤—Ç–æ—Ä, –ñ–∞–Ω—Ä, –ì–æ–¥.'\n",
            "   Got:   '–ú–Ω–µ –∂–∞–ª—å, –Ω–æ —è –Ω–µ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ –≤—ã –∏–º–µ–µ—Ç–µ –≤ –≤–∏–¥—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ —Å–≤–æ–π –∑–∞–ø—Ä–æ—Å....'\n",
            "--------------------\n",
            "4. [Missed Tool (FN)]\n",
            "   Input: '–ù–∞–ø–∏—à–∏ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è –≤–∏–¥–µ–æ –Ω–∞ YouTube –ø—Ä–æ —Ä–µ–º–æ–Ω—Ç iPhone 13.'\n",
            "   Got:   '–Ø –Ω–µ –ø–æ–Ω–∏–º–∞—é, —á—Ç–æ –≤—ã –∏–º–µ–µ—Ç–µ –≤ –≤–∏–¥—É. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É—Ç–æ—á–Ω–∏—Ç–µ –≤–∞—à –∑–∞–ø—Ä–æ—Å. –Ø –º–æ–≥—É –ø–æ–º–æ—á—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π, —Ç–µ...'\n",
            "--------------------\n",
            "5. [Hallucination (FP)]\n",
            "   Input: '–ö–∞–∫ –º–Ω–µ —Å–±—Ä–æ—Å–∏—Ç—å –≤–µ—Å?'\n",
            "   Got:   '<tool_call><name>magic_paste</name><arguments><text>–î–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π –ø–æ—Ç–µ—Ä–∏ –≤–µ—Å–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –∫–æ–º–ø–ª–µ...'\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "audio_inputs = [item['user_audio']['array'] for item in test_ds]\n",
        "test_targets = [item['assistant_text'] for item in test_ds]\n",
        "# –¥–ª—è –≤—ã–≤–æ–¥–∞ –æ—à–∏–±–æ–∫\n",
        "user_transcripts = [item['user_text'] for item in test_ds]\n",
        "\n",
        "# –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "raw_predictions_c = run_pipeline_c_batched(audio_inputs, model, processor, batch_size=4)\n",
        "\n",
        "# —Ça –∂–µ —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏, —á—Ç–æ –∏ –¥–ª—è Pipeline A\n",
        "metrics_c = evaluate_tool_pipeline(\n",
        "    pipeline_name=\"Pipeline C (Audio End-to-End)\",\n",
        "    user_inputs=user_transcripts,\n",
        "    expected_outputs=test_targets,\n",
        "    generated_outputs=raw_predictions_c\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSbPYWLm2xRw"
      },
      "source": [
        "### **Pipeline D.**  Audio ‚Üí Model ‚Üí Transcript ‚Üí Model ‚Üí Tool\n",
        "\n",
        " Pipeline D - —ç—Ç–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ **Pipeline B -> Pipeline  A**\n",
        "\n",
        "- –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≥–æ–Ω—è–µ–º *–≤—Å–µ* –∞—É–¥–∏–æ —á–µ—Ä–µ–∑ Pipeline B (–ø–æ–ª—É—á–∞–µ–º —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã).\n",
        "- –ü–æ—Ç–æ–º –ø—Ä–æ–≥–æ–Ω—è–µ–º *–≤—Å–µ* —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç—ã —á–µ—Ä–µ–∑ Pipeline A (–ø–æ–ª—É—á–∞–µ–º —Ç—É–ª—ã).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xjavFIdO2y93"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "def run_pipeline_d_cascaded(\n",
        "    audio_arrays: list,\n",
        "    model,\n",
        "    processor,  # –¥–ª—è ASR\n",
        "    tokenizer,  # –¥–ª—è Text\n",
        "    asr_batch_size: int = 4,\n",
        "    llm_batch_size: int = 8\n",
        "):\n",
        "    print(f\"\\nüöÄ –ó–∞–ø—É—Å–∫ pipeline D\")\n",
        "\n",
        "    # Pipeline B\n",
        "    print(f\"üì° –®–∞–≥ 1/2: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è (ASR)...\")\n",
        "    raw_transcripts = run_pipeline_b_batched(\n",
        "        audio_arrays,\n",
        "        model,\n",
        "        processor,\n",
        "        batch_size=asr_batch_size\n",
        "    )\n",
        "\n",
        "    # –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Ç—è–∂–µ–ª–æ–≥–æ –∞—É–¥–∏–æ-—ç–Ω–∫–æ–¥–µ—Ä–∞\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # —á–∏—Å—Ç–∏–º —Ç–µ–≥–∏ <TEXT>...</TEXT>\n",
        "    clean_transcripts = []\n",
        "    for rt in raw_transcripts:\n",
        "        # –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ä—Å–µ—Ä –∏–∑ Pipeline B\n",
        "        clean_text = asr_parser.parse(rt)\n",
        "        clean_transcripts.append(clean_text)\n",
        "\n",
        "    # Pipeline A\n",
        "    print(f\"üß† –®–∞–≥ 2/2: –õ–æ–≥–∏–∫–∞ (LLM)...\")\n",
        "    final_outputs = run_pipeline_a_batched(\n",
        "        clean_transcripts, # –ø–æ–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç ASR\n",
        "        model,\n",
        "        tokenizer,\n",
        "        batch_size=llm_batch_size\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"transcripts\": clean_transcripts,\n",
        "        \"final_outputs\": final_outputs\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xTMxJXg2ot_"
      },
      "source": [
        "### üöÄ –ó–∞–ø—É—Å–∫ Pipeline D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRsF4UTLMXwQ",
        "outputId": "f9de1e79-7837-4dbd-eea4-6f0c57b21a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ –ó–∞–ø—É—Å–∫ pipeline D\n",
            "üì° –®–∞–≥ 1/2: –¢—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏—è (ASR)...\n",
            "üîß Padding: –î–æ–±–∞–≤–ª—è–µ–º 2 —ç–ª–µ–º–µ–Ω—Ç–æ–≤\n",
            "üöÄ –ó–∞–ø—É—Å–∫ Pipeline B (ASR): 52 –∞—É–¥–∏–æ (Batch size: 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [02:24<00:00, 11.09s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß† –®–∞–≥ 2/2: –õ–æ–≥–∏–∫–∞ (LLM)...\n",
            "–î–æ–±–∞–≤–ª—è–µ–º 2 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ \n",
            "üöÄ –ó–∞–ø—É—Å–∫ Pipeline A: 52 –ø—Ä–∏–º–µ—Ä–æ–≤ (Batch size: 4)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [06:01<00:00, 27.83s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò: Pipeline D (Cascaded)\n",
            "============================================================\n",
            "‚úÖ Accuracy (Intent): 94.00% (–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)\n",
            "üéØ Precision:         88.89% (–ù–∞—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–∫–æ –≤—ã–∑—ã–≤–∞–µ–º —Ç—É–ª—ã)\n",
            "üì° Recall:            100.00% (–°–∫–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥ –º—ã '—É—Å–ª—ã—à–∞–ª–∏')\n",
            "------------------------------\n",
            "üö® False Alarm Rate:  11.54% (–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)\n",
            "üìù XML Syntax Score:  100.00% (–í–∞–ª–∏–¥–Ω–æ—Å—Ç—å XML)\n",
            "============================================================\n",
            "\n",
            "üîç –ù–∞–π–¥–µ–Ω–æ –æ—à–∏–±–æ–∫: 3\n",
            "1. [Hallucination (FP)]\n",
            "   Input: '–¢—ã –º–æ–∂–µ—à—å –ø–æ–∑–≤–æ–Ω–∏—Ç—å –º–æ–µ–º—É –Ω–∞—á–∞–ª—å–Ω–∏–∫—É?'\n",
            "   Got:   '<tool_call><name>magic_paste</name><arguments><text>–Ø –Ω–µ –º–æ–≥—É —ç—Ç–æ–≥–æ —Å–¥–µ–ª–∞—Ç—å. –Ø ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –∏ –Ω–µ –∏...'\n",
            "--------------------\n",
            "2. [Hallucination (FP)]\n",
            "   Input: '–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ –®—Ç–∏—Ä–ª–∏—Ü–∞.'\n",
            "   Got:   '–†–∞—Å—Å–∫–∞–∂–∏ –∞–Ω–µ–∫–¥–æ—Ç –ø—Ä–æ —à—Ç–∏–ª—å. –û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>–ü–æ—á–µ–º—É —à—Ç–∏–ª—å ...'\n",
            "--------------------\n",
            "3. [Hallucination (FP)]\n",
            "   Input: '–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è —Å–ª–æ–≤–æ 'Success'?'\n",
            "   Got:   '<tool_call><name>magic_paste</name><arguments><text>—É—Å–ø–µ—Ö</text></arguments></tool_call>...'\n",
            "--------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#  –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
        "audio_inputs = [item['user_audio']['array'] for item in test_ds]\n",
        "test_targets = [item['assistant_text'] for item in test_ds]\n",
        "ref_texts = [item['user_text'] for item in test_ds]\n",
        "\n",
        "# –∑–∞–ø—É—Å–∫\n",
        "results_d = run_pipeline_d_cascaded(\n",
        "    audio_inputs,\n",
        "    model,\n",
        "    processor,\n",
        "    tokenizer,\n",
        "    asr_batch_size=4,\n",
        "    llm_batch_size=4\n",
        ")\n",
        "\n",
        "\n",
        "# –æ—Ü–µ–Ω–∏–≤–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - tool call\n",
        "metrics_d = evaluate_tool_pipeline(\n",
        "    pipeline_name=\"Pipeline D (Cascaded)\",\n",
        "    user_inputs=ref_texts,\n",
        "    expected_outputs=test_targets,\n",
        "    generated_outputs=results_d['final_outputs']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MICXrf4uqdsc"
      },
      "source": [
        "## –ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –≤—ã–≤–æ–¥—ã"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMrTE0ROqurf"
      },
      "source": [
        "#### **–¢–µ–∫—Å—Ç - Pipeline A**\n",
        "- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** Accuracy = 94%, Recall = 100%.\n",
        "- –ú–æ–¥–µ–ª—å –∏–¥–µ–∞–ª—å–Ω–æ —É–ª–∞–≤–ª–∏–≤–∞–µ—Ç –Ω–∞–º–µ—Ä–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è - –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–æ–ø—É—â–µ–Ω–Ω–æ–≥–æ –≤—ã–∑–æ–≤–∞ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞. –ü—Ä–æ–º–ø—Ç-–∏–Ω–∂–∏–Ω–∏—Ä–∏–Ω–≥ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º —Ä–µ–∂–∏–º–æ–≤ Tool vs Chat —Å—Ä–∞–±–æ—Ç–∞–ª —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ.\n",
        "- **–ü—Ä–æ–±–ª–µ–º–∞:**  False Alarm Rate ~11.5%. –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º —É—Å–µ—Ä–¥–Ω–æ –ø—ã—Ç–∞–µ—Ç—Å—è –≤—ã–∑–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç –¥–∞–∂–µ —Ç–∞–º, –≥–¥–µ —ç—Ç–æ —Å–ø–æ—Ä–Ω–æ –∏–ª–∏ –Ω–µ—É–º–µ—Å—Ç–Ω–æ.\n",
        "- **–†–µ—à–µ–Ω–∏–µ:** Fine-tuning  –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω —á—Ç–æ–±—ã —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π –º–æ–¥–µ–ª—å.\n",
        "\n",
        "#### **–ö–∞—á–µ—Å—Ç–≤–æ —Å–ª—É—Ö–∞ - Pipeline B**\n",
        "- **–†–µ–∑—É–ª—å—Ç–∞—Ç:** WER = 0.2075 (20.7%).\n",
        "- –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã —Å —Å–∏—Å—Ç–µ–º–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º –ø–æ–∑–≤–æ–ª–∏–ª–∏ –≤—ã–ø—Ä–∞–≤–∏—Ç—å —Å–∏—Ç—É–∞—Ü–∏—é —Å word error rate. –ú–æ–¥–µ–ª—å –ø–µ—Ä–µ—Å—Ç–∞–ª–∞ –ø—ã—Ç–∞—Ç—å—Å—è –≤—ã–ø–æ–ª–Ω—è—Ç—å –≥–æ–ª–æ—Å–æ–≤—ã–µ –∫–æ–º–∞–Ω–¥—ã –∏ –ø–µ—Ä–µ–∫–ª—é—á–∏–ª–∞—Å—å –≤ —Ä–µ–∂–∏–º —Å—Ç–µ–Ω–æ–≥—Ä–∞—Ñ–∏—Å—Ç–∞.\n",
        "- –û—à–∏–±–∫–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å—Ç–∏–ª–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ (–ø—É–Ω–∫—Ç—É–∞—Ü–∏—è, –∑–∞–º–µ–Ω–∞ —Ü–∏—Ñ—Ä —Å–ª–æ–≤–∞–º–∏) –∏–ª–∏ —Ñ–æ–Ω–µ—Ç–∏—á–µ—Å–∫–∏–µ. –°–º—ã—Å–ª —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ–ª–Ω–æ—Å—Ç—å—é. –≠—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞.\n",
        "\n",
        "#### **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä:  Pipeline D vs Pipeline C**\n",
        "- Pipeline D. Accuracy 94%.\n",
        "- Pipeline C. Accuracy 90%.\n",
        "  - Pipeline D –ø–æ–∫–∞–∑–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —á–∏—Å—Ç–æ–º—É —Ç–µ–∫—Å—Ç—É. –ö–∞—á–µ—Å—Ç–≤–æ ASR (Pipeline B) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ, —Ä–∞–∑–¥–µ–ª—è—è –∑–∞–¥–∞—á–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –º—ã—à–ª–µ–Ω–∏—è.\n",
        "  - Pipeline C  - Recall –ø—Ä–æ—Å–µ–ª –¥–æ 83%. –ú–æ–¥–µ–ª–∏ —Å–ª–æ–∂–Ω–µ–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ —Ä–∞–∑–±–∏—Ä–∞—Ç—å –∞—É–¥–∏–æ –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ –≤—ã–∑–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏. –û–Ω–∞ —á–∞—â–µ \"–Ω–µ —Å–ª—ã—à–∏—Ç\" –∫–æ–º–∞–Ω–¥—É –∏ —Å–≤–∞–ª–∏–≤–∞–µ—Ç—Å—è –≤ –æ–±—ã—á–Ω—ã–π –æ—Ç–≤–µ—Ç.\n",
        "\n",
        "#### **–†–∞–∑—Ä—ã–≤ –º–æ–¥–∞–ª—å–Ω–æ—Å—Ç–µ–π –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç**\n",
        "- Accuracy_A (Text) = 94%.\n",
        "- Accuracy_D (Audio) = 94%.  \n",
        "\n",
        "–ù–µ—Ç –±–æ–ª—å—à–æ–π —Ä–∞–∑–Ω–∏—Ü—ã –º–µ–∂–¥—É —Ç–µ–∫—Å—Ç–æ–≤—ã–º –∏ –≥–æ–ª–æ—Å–æ–≤—ã–º –≤–≤–æ–¥–æ–º –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –∫–∞—Å–∫–∞–¥–Ω–æ–π —Å—Ö–µ–º—ã.\n",
        "\n",
        "#### **–ö—Ä–∞—Ç–∫–∏–π –ø—Ä–æ–º. –∏—Ç–æ–≥**\n",
        "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å Gemma-3n –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –≤—ã—Å–æ–∫–∏–µ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ —Å–ª–µ–¥–æ–≤–∞–Ω–∏—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è–º. –ì–ª–∞–≤–Ω–∞—è –∑–æ–Ω–∞ —Ä–æ—Å—Ç–∞ - —Å–Ω–∏–∂–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π FP.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH58gvL5zKfC"
      },
      "source": [
        "## ‚ö° LoRA fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjXe0yKZzO-8"
      },
      "source": [
        "- –î–æ–æ–±—É—á–∞—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫ (–Ω–∞–ø—Ä. CommonVoice) –±–æ–ª—å—à–æ–≥–æ —Å–º—ã—Å–ª–∞ –Ω–µ—Ç. –ú–æ–¥–µ–ª—å –Ω–µ–ø–ª–æ—Ö–æ –ø–æ–Ω–∏–º–∞–µ—Ç —Ä—É—Å—Å–∫–∏–π —è–∑—ã–∫.\n",
        "- –î–æ–æ–±—É—á–µ–Ω–∏–µ ASR (pipeline B) —Ç–æ–∂–µ –≤–µ—Ä–æ—è—Ç–Ω–æ –Ω–µ –¥–∞—Å—Ç –±–æ–ª—å—à–≥–æ –ø—Ä–∏—Ä–æ—Å—Ç–∞ –≤ –∫–∞—á–µ—Å—Ç–≤–µ, –ø–æ—Å–∫–æ–ª—å–∫—É –º–æ–¥–µ–ª—å —É–∂–µ –Ω–µ–ø–ª–æ—Ö–æ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è —Å –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç. (–µ—Å–ª–∏ –Ω–µ —Å—á–∏—Ç–∞—Ç—å –≥—Ä–∞–º–º–∞—Ç–∏–∫—É –∏ —Ñ–æ–Ω–µ—Ç–∏–∫—É)\n",
        "- Pipeline D (Cascaded B + A) –ø–æ–∫–∞–∑–∞–ª —Å–µ–±—è –ª—É—á—à–µ –¥—Ä—É–≥–∏—Ö. –û–Ω –∑–∞–≤–∏—Å–∏—Ç –æ—Ç —Ç–µ–∫—Å—Ç–æ–≤–æ–π –º–æ–¥–µ–ª–∏ (pipeline A). –£–ª—É—á—à–∏–º —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å - —É–ª—É—á—à–∏–º –∏ Pipeline D.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DNXMMZiu46CA"
      },
      "source": [
        "### LoRa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6eOJOoW3smT"
      },
      "source": [
        "#### –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUhLiZPK3q9S",
        "outputId": "c5c89c09-3c9b-4180-9ca8-87be4f17b887"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
          ]
        }
      ],
      "source": [
        "from trl import SFTTrainer #, DataCollatorForCompletionOnlyLM\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# –Ω–∞ –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è –º–∏–Ω–∏–∑–∏—É—Ä–µ–º –ø—Ä–æ–º–ø—Ç (–û–¢–ú–ï–ù–ï–ù–û. –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç)\n",
        "TRAIN_SYSTEM_PROMPT = \"\"\"–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç.\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –æ—Ç–≤–µ—á–∞—Ç—å —Ç–µ–∫—Å—Ç–æ–º (–ß–∞—Ç) –∏–ª–∏ –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å XML <tool_call> (–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç).\n",
        "\n",
        "–ü–†–ê–í–ò–õ–ê:\n",
        "1. –û—Ç–≤–µ—á–∞–π –Ω–∞ —è–∑—ã–∫–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è.\n",
        "2. –§–æ—Ä–º–∞—Ç: —Ç–æ–ª—å–∫–æ —á–∏—Å—Ç—ã–π XML –∏–ª–∏ —Ç–µ–∫—Å—Ç. –ë–µ–∑ Markdown.\n",
        "3. –õ–û–ì–ò–ö–ê:\n",
        "   - \"–ù–∞–ø–∏—à–∏/–ò—Å–ø—Ä–∞–≤—å/–ü–µ—Ä–µ–≤–µ–¥–∏/–ö–æ–¥\" -> <tool_call>...\n",
        "   - \"–û–±—ä—è—Å–Ω–∏/–ü–æ—Å–æ–≤–µ—Ç—É–π/–ü—Ä–∏–≤–µ—Ç/–í–æ–ø—Ä–æ—Å\" -> –¢–µ–∫—Å—Ç.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏\n",
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    r =  64, #64, # –±—ã–ª–æ 16,\n",
        "    lora_alpha = 128, # –±—ã–ª–æ 16,\n",
        "    lora_dropout = 0.0, # –¥–æ–±–∞–≤–ª—è–µ–º —à—É–º, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –Ω–µ –ø–µ—Ä–µ–æ–±—É—á–∞–ª–∞—Å—å\n",
        "\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "\n",
        "\n",
        "    # ‚ö†Ô∏è –ù–ï –ê–ö–¢–£–ê–õ–¨–ù–û - –æ—à–∏–±–∫–∞ OOM –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞ (gradient_checkpointing = True)\n",
        "    # –û–±—É—á–µ–Ω–∏–µ –≤—Å–µ—Ö –º–æ–¥—É–ª–µ–π (gate, up, down) –≤ —Ä–µ–∂–∏–º–µ FP32 - —ç—Ç–æ —Ç—É –º–∞—á –¥–ª—è T4\n",
        "    # –û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –≤–∞–∂–Ω—ã–µ –¥–ª—è –≤–Ω–∏–º–∞–Ω–∏—è (q, v).\n",
        "    # target_modules =  [\"q_proj\", \"v_proj\"],\n",
        "\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    max_seq_length = 2048,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç (SYSTEM_PROMPT_A, –∫–æ—Ç–æ—Ä—ã–π –¥–∞–ª Acc = 94%)\n",
        "INFERENCE_SYSTEM_PROMPT = \"\"\"–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
        "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ª–∏–±–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–ß–∞—Ç), –ª–∏–±–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π XML –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç).\n",
        "\n",
        "### –ì–õ–ê–í–ù–´–ï –ü–†–ê–í–ò–õ–ê\n",
        "1. **–Ø–ó–´–ö:** –í—Å–µ–≥–¥–∞ –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ç–æ–º –∂–µ —è–∑—ã–∫–µ, –Ω–∞ –∫–æ—Ç–æ—Ä–æ–º —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å.\n",
        "2. **–§–û–†–ú–ê–¢:** –ù–∏–∫–æ–≥–¥–∞ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–π Markdown (```xml –∏–ª–∏ ```json). –í—ã–≤–æ–¥–∏ —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç –∏–ª–∏ —á–∏—Å—Ç—ã–π XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É.\n",
        "3. **–†–û–õ–¨:** –¢—ã ‚Äî –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç —Ä–∞–±–æ—Ç—ã —Å —Ç–µ–∫—Å—Ç–æ–º. –¢—ã —É–º–µ–µ—à—å –∏ –ø–∏—Å–∞—Ç—å —Å –Ω—É–ª—è (–ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã–π —Ç–µ—Å—Ç, —Å—Ç–∏—Ö–∏, –∫–æ–¥, –ø–∏—Å—å–º–∞), –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞—Ç—å. –ï—Å–ª–∏ –ø—Ä–æ—Å—è—Ç —á—Ç–æ-—Ç–æ —Å–æ–∑–¥–∞—Ç—å ‚Äî —Å–æ–∑–¥–∞–≤–∞–π. –ù–µ –æ—Ç–∫–∞–∑—ã–≤–∞–π—Å—è.\n",
        "4. **–î–ò–°–¶–ò–ü–õ–ò–ù–ê:** –í–°–ï–ì–î–ê —Å–ª–µ–¥—É–π –ª–æ–≥–∏–∫–µ –≤—ã–±–æ—Ä–∞ —Ä–µ–∂–∏–º–∞ –Ω–∏–∂–µ.\n",
        "\n",
        "---\n",
        "\n",
        "### –õ–û–ì–ò–ö–ê –í–´–ë–û–†–ê –†–ï–ñ–ò–ú–ê\n",
        "\n",
        "**–†–ï–ñ–ò–ú 1: –ò–ù–°–¢–†–£–ú–ï–ù–¢ (–í—Å—Ç–∞–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞)**\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç **–°–û–ó–î–ê–¢–¨** –∏–ª–∏ **–ò–ó–ú–ï–ù–ò–¢–¨** —Ç–µ–∫—Å—Ç/–∫–æ–¥.\n",
        "–¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –¥–µ–π—Å—Ç–≤–∏—è):\n",
        "- **–ù–∞–ø–∏—à–∏ / –°–æ—Å—Ç–∞–≤—å / –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π** (\"–ù–∞–ø–∏—à–∏ –ø–∏—Å—å–º–æ\", \"–°–æ—Å—Ç–∞–≤—å —Å–ø–∏—Å–æ–∫\", \"–°–æ—Å—Ç–∞–≤—å —á–µ–∫-–ª–∏—Å—Ç\")\n",
        "- **–ü—Ä–∏–¥—É–º–∞–π / –°–æ–∑–¥–∞–π** (\"–ü—Ä–∏–¥—É–º–∞–π –Ω–∞–∑–≤–∞–Ω–∏–µ\", \"–°–æ–∑–¥–∞–π 3 –≤–∞—Ä–∏–∞–Ω—Ç–∞\")\n",
        "- **–ò—Å–ø—Ä–∞–≤—å / –ü–µ—Ä–µ–ø–∏—à–∏ / –°–æ–∫—Ä–∞—Ç–∏ / –£–ª—É—á—à–∏** (\"–ò—Å–ø—Ä–∞–≤—å –æ—à–∏–±–∫–∏\", \"–°–¥–µ–ª–∞–π –≤–µ–∂–ª–∏–≤–µ–µ\")\n",
        "- **–ü–µ—Ä–µ–≤–µ–¥–∏** (\"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π\")\n",
        "- **–ö–æ–¥** (\"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é\")\n",
        "\n",
        "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê (–°—Ç—Ä–æ–≥–æ XML –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É –±–µ–∑ –ª–∏—à–Ω–∏—Ö —Å–ª–æ–≤):\n",
        "<tool_call><name>magic_paste</name><arguments><text>–¢–í–û–ô_–ì–û–¢–û–í–´–ô_–¢–ï–ö–°–¢</text></arguments></tool_call>\n",
        "\n",
        "---\n",
        "\n",
        "**–†–ï–ñ–ò–ú 2: –ß–ê–¢ (–û–±—ã—á–Ω—ã–π —Ä–∞–∑–≥–æ–≤–æ—Ä)**\n",
        "–ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–æ—Ç —Ä–µ–∂–∏–º, –µ—Å–ª–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Ö–æ—á–µ—Ç **–£–ó–ù–ê–¢–¨** –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é, –ø–æ–±–ª–∞–≥–æ–¥–∞—Ä–∏—Ç—å –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –ø–æ–±–æ–ª—Ç–∞—Ç—å.\n",
        "–¢—Ä–∏–≥–≥–µ—Ä—ã (–ì–ª–∞–≥–æ–ª—ã –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è):\n",
        "- **–†–∞—Å—Å–∫–∞–∂–∏ / –û–±—ä—è—Å–Ω–∏** (\"–†–∞—Å—Å–∫–∞–∂–∏ –ø—Ä–æ...\", \"–û–±—ä—è—Å–Ω–∏ –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç...\")\n",
        "- **–í–æ–ø—Ä–æ—Å—ã** (\"–ß—Ç–æ —Ç–∞–∫–æ–µ...\", \"–ö—Ç–æ –ø–æ–±–µ–¥–∏–ª...\", \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è...\")\n",
        "- **–í–µ–∂–ª–∏–≤–æ—Å—Ç—å** (\"–ü—Ä–∏–≤–µ—Ç\", \"–°–ø–∞—Å–∏–±–æ\", \"–ü–æ–∫–∞\")\n",
        "- **–ü–æ—Å–æ–≤–µ—Ç—É–π / –ü–æ–¥—Å–∫–∞–∂–∏** (\"–ü–æ—Å–æ–≤–µ—Ç—É–π —Ñ–∏–ª—å–º\")\n",
        "- **–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è** (\"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π...\")\n",
        "\n",
        "\n",
        "\n",
        "–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:\n",
        "–ü—Ä–æ—Å—Ç–æ–π —Ç–µ–∫—Å—Ç –æ—Ç–≤–µ—Ç–∞. –ö—Ä–∞—Ç–∫–æ –∏ –ø–æ –¥–µ–ª—É.\n",
        "\n",
        "### –ü–†–ò–ú–ï–†–´ (–û–±—É—á–µ–Ω–∏–µ)\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞?\"\n",
        "–û—Ç–≤–µ—Ç: –ü—Ä–∏–≤–µ—Ç! –Ø –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ —Å —Ç–µ–∫—Å—Ç–æ–º.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–µ–∂–ª–∏–≤–æ—Å—Ç—å -> –ß–∞—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü—Ä–∏–¥—É–º–∞–π 3 –∏–¥–µ–∏ –¥–ª—è —Å—Ç–∞—Ä—Ç–∞–ø–∞.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>1. –£–º–Ω—ã–π —Å–∞–¥\\n2. AI-—Ä–µ–ø–µ—Ç–∏—Ç–æ—Ä\\n3. –î–æ—Å—Ç–∞–≤–∫–∞ –¥—Ä–æ–Ω–∞–º–∏</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü—Ä–∏–¥—É–º–∞–π\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ù–∞–ø–∏—à–∏ —Ñ—É–Ω–∫—Ü–∏—é —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ –Ω–∞ JS.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>const sort = (arr) => arr.sort();</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ù–∞–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–û–±—ä—è—Å–Ω–∏, –∫–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞.\"\n",
        "–û—Ç–≤–µ—Ç: –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ —É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç —ç–ª–µ–º–µ–Ω—Ç—ã –≤ –º–∞—Å—Å–∏–≤–µ –ø–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–º—É –∫—Ä–∏—Ç–µ—Ä–∏—é, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ –≤–æ–∑—Ä–∞—Å—Ç–∞–Ω–∏—é —á–∏—Å–µ–ª.\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ü—Ä–æ—Å—å–±–∞ \"–û–±—ä—è—Å–Ω–∏\" -> –ß–∞—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ –≤–µ–∂–ª–∏–≤–æ: –¢—ã –æ–ø–æ–∑–¥–∞–ª.\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –≤—ã –∑–∞–¥–µ—Ä–∂–∞–ª–∏—Å—å.</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–ø–∏—à–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ü–µ—Ä–µ–≤–µ–¥–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n",
        "–û—Ç–≤–µ—Ç: <tool_call><name>magic_paste</name><arguments><text>Hello World</text></arguments></tool_call>\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –ö–æ–º–∞–Ω–¥–∞ \"–ü–µ—Ä–µ–≤–µ–¥–∏\" -> –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç\n",
        "\n",
        "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è: –ü—Ä–∏–≤–µ—Ç –º–∏—Ä\"\n",
        "–û—Ç–≤–µ—Ç:  \"–ü—Ä–∏–≤–µ—Ç –º–∏—Ä –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∫–∞–∫ 'Hello World'.\"\n",
        "–ü–æ—è—Å–Ω–µ–Ω–∏–µ: –í–æ–ø—Ä–æ—Å \"–ö–∞–∫ –ø–µ—Ä–µ–≤–æ–¥–∏—Ç—Å—è\" -> –ß–∞—Ç\n",
        "\n",
        "### –ö–û–ù–ï–¶ –ü–†–ò–ú–ï–†–û–í. –ù–ê–ß–ê–õ–û –î–ò–ê–õ–û–ì–ê:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufj1oIsX40we"
      },
      "source": [
        "#### –î–∞–Ω–Ω—ã–µ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "f8w3KQ8-3Sob",
        "outputId": "e4ea2031-8477-44d1-d614-052f6288dd00"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "38d768dd51a04536821f87822e37d05d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/150 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "—è. –ü—Ä–µ–¥–ª–∞–≥–∞—é —Å–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–µ –≤—Ä–µ–º—è –¥–ª—è –æ–±—Å—É–∂–¥–µ–Ω–∏—è.</text>\n",
            "</arguments>\n",
            "</tool_call><end_of_turn>\n",
            "\n",
            "‚úÖ –î–∞–Ω–Ω—ã–µ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã. –ü—Ä–∏–º–µ—Ä:\n",
            "<bos><start_of_turn>user\n",
            "–¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ª–∏–±–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–ß–∞—Ç), –ª–∏–±–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–π XML –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏ –≤ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ (–ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç).\n",
            "\n",
            "### –ì\n"
          ]
        }
      ],
      "source": [
        "def format_prompts(examples):\n",
        "    texts = []\n",
        "\n",
        "    for user, assistant in zip(examples['user_text'], examples['assistant_text']):\n",
        "\n",
        "        # —Ñ–æ—Ä–º–∏—Ä—É–µ–º –¥–∏–∞–ª–æ–≥ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –∫—Ä–∞—Ç–∫–∏–π –ø—Ä–æ–º–ø—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": INFERENCE_SYSTEM_PROMPT }, # TRAIN_SYSTEM_PROMPT},\n",
        "            {\"role\": \"user\", \"content\": user},\n",
        "            {\"role\": \"assistant\", \"content\": assistant}\n",
        "        ]\n",
        "\n",
        "        # apply_chat_template –ø—Ä–µ–≤—Ä–∞—â–∞–µ—Ç —ç—Ç–æ –≤ —Å—Ç—Ä–æ–∫—É c–æ —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω–∞–º–∏\n",
        "        # <bos><start_of_turn>system...<end_of_turn>...\n",
        "        text = tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=False\n",
        "        )\n",
        "        texts.append(text)\n",
        "\n",
        "    return { \"text\" : texts }\n",
        "\n",
        "\n",
        "# –ø—Ä–∏–º–µ–Ω—è–µ–º –∏ —á–∏—Å—Ç–∏–º –¥–∞—Ç–∞—Å–µ—Ç –æ—Ç –≤—Å–µ–≥–æ –ª–∏—à–Ω–µ–≥–æ (–∞—É–¥–∏–æ –∏ –ø—Ä–æ—á–µ–≥–æ)\n",
        "train_ds_clean = train_ds.map(format_prompts, batched=True)\n",
        "# –æ—Å—Ç–∞–≤–ª—è–µ–º –¢–û–õ–¨–ö–û –∫–æ–ª–æ–Ω–∫—É text\n",
        "train_ds_clean = train_ds_clean.select_columns([\"text\"])\n",
        "\n",
        "# –ø—Ä–æ–≤–µ—Ä–∏–º –µ—Å—Ç—å –ª–∏ –≤ –∫–æ–Ω—Ü–µ —Ç–æ–∫–µ–Ω –∫–æ–Ω—Ü–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏\n",
        "print(train_ds_clean[0]['text'][-100:])\n",
        "\n",
        "print(\"‚úÖ –î–∞–Ω–Ω—ã–µ –æ—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω—ã. –ü—Ä–∏–º–µ—Ä:\")\n",
        "print(train_ds_clean[0]['text'][:200])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qzd7NZT96SHR"
      },
      "source": [
        "#### –ö–∞—Å—Ç–æ–º–Ω—ã–π –∫–æ–ª–ª–∞—Ç–æ—Ä\n",
        "- –∏–º–ø–æ—Ä—Ç DataCollatorForCompletionOnlyLM –∏–∑ trl –≤—ã–¥–∞–µ—Ç –æ—à–∏–±–∫—É\n",
        "- –≤–æ–∑–º–æ–∂–Ω–æ –≤–µ—Ä—Å–∏—è trl –Ω–µ –ø–æ–¥—Ö–æ–¥—è—â–∞—è\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KbbDQI1e6RlJ",
        "outputId": "37f30a57-bb28-4e8f-9fcd-ce4b68c1b351"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "—Ç–æ–∫–µ–Ω–∞ —à–∞–±–ª–æ–Ω–∞: [105, 4368, 107]\n"
          ]
        }
      ],
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "import numpy as np\n",
        "text_tokenizer = tokenizer.tokenizer\n",
        "class CustomMaskingCollator(DataCollatorForLanguageModeling):\n",
        "    def __init__(self, tokenizer, resp_temp, mlm=False):\n",
        "        super().__init__(tokenizer=tokenizer, mlm=mlm)\n",
        "        # –∫–æ–¥–∏—Ä—É–µ–º —à–∞–±–ª–æ–Ω –≤ —Ç–æ–∫–µ–Ω—ã –æ–¥–∏–Ω —Ä–∞–∑\n",
        "        self.resp_temp_ids = tokenizer.encode(resp_temp, add_special_tokens=False)\n",
        "        print(f\"—Ç–æ–∫–µ–Ω–∞ —à–∞–±–ª–æ–Ω–∞: {self.resp_temp_ids}\")\n",
        "\n",
        "    def torch_call(self, examples):\n",
        "        # –±–∞–∑–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø–∞–¥–¥–∏–Ω–≥ –∏ —Ç.–¥.\n",
        "        batch = super().torch_call(examples)\n",
        "\n",
        "        # –ø—Ä–æ—Ö–æ–¥–∏–º –ø–æ –∫–∞–∂–¥–æ–º—É –ø—Ä–∏–º–µ—Ä—É\n",
        "        for i in range(len(batch[\"labels\"])):\n",
        "            # –∏—â–µ–º —à–∞–±–ª–æ–Ω –≤ input_ids\n",
        "            input_ids = batch[\"input_ids\"][i].tolist()\n",
        "\n",
        "            # –Ω–∞—Ö–æ–¥–∏–º –Ω–∞—á–∞–ª–æ —à–∞–±–ª–æ–Ω–∞\n",
        "            found_idx = -1\n",
        "            n = len(self.resp_temp_ids)\n",
        "            for j in range(len(input_ids) - n):\n",
        "                if input_ids[j:j+n] == self.resp_temp_ids:\n",
        "                    found_idx = j + n # –∏–Ω–¥–µ–∫—Å –ü–ï–†–í–û–ì–û —Ç–æ–∫–µ–Ω–∞ –æ—Ç–≤–µ—Ç–∞\n",
        "                    break\n",
        "\n",
        "            if found_idx != -1:\n",
        "                # –º–∞—Å–∫–∏—Ä—É–µ–º –≤—Å—ë –î–û –æ—Ç–≤–µ—Ç–∞ - –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é –∏ –≤–æ–ø—Ä–æ—Å\n",
        "                # -100 = –Ω–µ —Å—á–∏—Ç–∞—Ç—å –ª–æ—Å—Å –¥–ª—è —ç—Ç–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤\n",
        "                batch[\"labels\"][i, :found_idx] = -100\n",
        "            else:\n",
        "                # –µ—Å–ª–∏ —à–∞–±–ª–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –º–∞—Å–∫–∏—Ä—É–µ–º –≤–æ–æ–±—â–µ –≤—Å—ë\n",
        "                print(f\"‚ö†Ô∏è –®–∞–±–ª–æ–Ω –Ω–µ –Ω–∞–π–¥–µ–Ω {i}\")\n",
        "                batch[\"labels\"][i, :] = -100\n",
        "\n",
        "        return batch\n",
        "\n",
        "# gemma 3 –æ–±—ã—á–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ç–∞–∫–æ–π —Ñ–æ—Ä–º–∞—Ç –ø–µ—Ä–µ–¥ –æ—Ç–≤–µ—Ç–æ–º\n",
        "resp_temp = \"<start_of_turn>model\\n\"\n",
        "collator = CustomMaskingCollator(tokenizer=text_tokenizer, resp_temp=resp_temp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhXlbks64jEC"
      },
      "source": [
        "#### trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "3fdfe2fa4515429f97c77d1d658eabfc"
          ]
        },
        "collapsed": true,
        "id": "DPyI2Wa94gVj",
        "outputId": "ff72c5cf-b333-47a8-d2c2-6e3b205a47f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Switching to float32 training since model cannot work with float16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3fdfe2fa4515429f97c77d1d658eabfc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/150 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "max_seq_length = 2048\n",
        "\n",
        "# –∫–æ–Ω—Ñ–∏–≥\n",
        "args = SFTConfig(\n",
        "    output_dir = \"outputs\",\n",
        "    per_device_train_batch_size = 2,\n",
        "    gradient_accumulation_steps = 4,\n",
        "    max_steps = 60,    # –ª–æ—Å—Å —Å—Ç–∞–±–∏–ª—å–Ω–æ –Ω–∞—á–∏–Ω–∞–µ—Ç —Ä–∞—Å—Ç–∏ –ø–æ—Å–ª–µ 20-–≥–æ —à–∞–≥–∞\n",
        "    warmup_steps = 5,     # –±—ã—Å—Ç—Ä—ã–π —Ä–∞–∑–æ–≥—Ä–µ–≤. –±—ã–ª–æ 10\n",
        "    lr_scheduler_type = \"cosine\",  # –ø–ª–∞–≤–Ω–æ–µ –∑–∞—Ç—É—Ö–∞–Ω–∏–µ\n",
        "    learning_rate = 5e-5,\n",
        "\n",
        "    save_strategy = \"steps\",     # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç—ã\n",
        "    save_steps = 5,\n",
        "    completion_only_loss=True,\n",
        "\n",
        "    # ‚ö†Ô∏è –†–ï–®–ï–ù–ò–ï –ü–†–û–ë–õ–ï–ú–´ Out Of Memory (VRAM)\n",
        "    gradient_checkpointing = True,\n",
        "    neftune_noise_alpha = 5,  # —à—É–º–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –æ—Ç –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è\n",
        "\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    packing = False,\n",
        "    remove_unused_columns = True,\n",
        "\n",
        "    report_to = \"none\",\n",
        "    fp16 = not is_bfloat16_supported(),\n",
        "    bf16 = is_bfloat16_supported(),\n",
        "    logging_steps = 1,\n",
        "    optim = \"adamw_8bit\",\n",
        "    weight_decay = 0.01,\n",
        "    seed = 3407,\n",
        ")\n",
        "\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = text_tokenizer,\n",
        "    train_dataset = train_ds_clean,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    data_collator = collator,\n",
        "\n",
        "    args = args,\n",
        "\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs-xkBa44BK-"
      },
      "source": [
        "### üöÄ –ó–∞–ø—É—Å–∫ LoRa (In progress ...)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1I4nj0x64Bpl",
        "outputId": "b2b29272-b037-44be-9e2b-55498b424442"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 150 | Num Epochs = 4 | Total steps = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 160,759,808 of 8,010,738,000 (2.01% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [60/60 35:12, Epoch 3/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.480500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>8.149000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.099600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>3.116600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.394600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.131000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.941800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.590100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.604900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.789300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.505400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.911600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.736700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.780800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.038600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.034000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.151500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>1.282200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.108700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.835700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>1.309200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>1.032900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.984500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>1.237200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>1.204700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>1.171200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.948400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>1.156000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.038200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.919200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>1.290000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.981400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.836400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.859200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.844900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.920500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.607600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.642800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.502600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.787100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.971200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.708300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.495500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.682200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.730700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.776100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.862300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.742500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.415100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.448600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\n",
            "–í—Ä–µ–º—è: 2303.51 —Å–µ–∫\n",
            "–ü—Ä–æ–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É —Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º\n",
            "INFERENCE_SYSTEM_PROMPT: –¢—ã ‚Äî –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞.\n",
            "–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –ª–∏–±–æ –ø—Ä–æ—Å—Ç–æ –æ—Ç–≤–µ—Ç–∏—Ç—å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é (–ß–∞—Ç), –ª–∏–±–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ ...\n",
            "–î–æ–±–∞–≤–ª—è–µ–º 6 —Ç–µ–∫—Å—Ç–æ–≤ –¥–ª—è –ø–∞–¥–¥–∏–Ω–≥–∞ \n",
            "üöÄ –ó–∞–ø—É—Å–∫ Pipeline A: 56 –ø—Ä–∏–º–µ—Ä–æ–≤ (Batch size: 8)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 7/7 [05:28<00:00, 46.95s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä –ó–ê–ü–£–°–ö –û–¶–ï–ù–ö–ò: Pipeline A (LoRa Tuned)\n",
            "============================================================\n",
            "‚úÖ Accuracy (Intent): 100.00% (–û–±—â–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å)\n",
            "üéØ Precision:         100.00% (–ù–∞—Å–∫–æ–ª—å–∫–æ –º–µ—Ç–∫–æ –≤—ã–∑—ã–≤–∞–µ–º —Ç—É–ª—ã)\n",
            "üì° Recall:            100.00% (–°–∫–æ–ª—å–∫–æ –∫–æ–º–∞–Ω–¥ –º—ã '—É—Å–ª—ã—à–∞–ª–∏')\n",
            "------------------------------\n",
            "üö® False Alarm Rate:  0.00% (–õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è)\n",
            "üìù XML Syntax Score:  100.00% (–í–∞–ª–∏–¥–Ω–æ—Å—Ç—å XML)\n",
            "============================================================\n",
            "\n",
            "üéâ –û—à–∏–±–æ–∫ –Ω–µ—Ç.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['lora_model_magic_paste/processor_config.json']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"\\n üöÄ –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è\")\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(\"\\n ‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "print(f\"–í—Ä–µ–º—è: {trainer_stats.metrics['train_runtime']:.2f} —Å–µ–∫\")\n",
        "\n",
        "# –∏–Ω—Ñ–µ—Ä–µ–Ω—Å\n",
        "FastModel.for_inference(model)\n",
        "\n",
        "# –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ –∂–µ —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ\n",
        "test_inputs = [item['user_text'] for item in test_ds]\n",
        "test_targets = [item['assistant_text'] for item in test_ds]\n",
        "\n",
        "# –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–Ω—ã–π –ø—Ä–æ–º–ø—Ç\n",
        "print('–ü—Ä–æ–≤–æ–¥–∏–º –æ—Ü–µ–Ω–∫—É —Å –ø–æ–ª–Ω—ã–º –ø—Ä–æ–º–ø—Ç–æ–º')\n",
        "print(f'INFERENCE_SYSTEM_PROMPT: {INFERENCE_SYSTEM_PROMPT[:120]} ...' )\n",
        "lora_predictions_isp = run_pipeline_a_batched(\n",
        "    test_inputs,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    batch_size=8,\n",
        "    system_prompt=INFERENCE_SYSTEM_PROMPT\n",
        ")\n",
        "\n",
        "# –æ—Ü–µ–Ω–∫–∞\n",
        "metrics_lora_isp = evaluate_tool_pipeline(\n",
        "    pipeline_name=\"Pipeline A (LoRa Tuned)\",\n",
        "    user_inputs=test_inputs,\n",
        "    expected_outputs=test_targets,\n",
        "    generated_outputs=lora_predictions_isp\n",
        ")\n",
        "\n",
        "\n",
        "# —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ\n",
        "model.save_pretrained(\"lora_model_magic_paste\")\n",
        "tokenizer.save_pretrained(\"lora_model_magic_paste\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIG1RVmPAEc-"
      },
      "source": [
        "### üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 267
        },
        "id": "vBfK4Iy6DISw",
        "outputId": "a0ea9594-a6bc-4b9a-faee-6b058119111a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ –õ–æ–∫–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ lora_gemma_fixed\n",
            "Mounted at /content/drive\n",
            "‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ Google –î–∏—Å–∫–µ: /content/drive/MyDrive/lora_gemma_fixed\n",
            "  adding: lora_gemma_fixed/ (stored 0%)\n",
            "  adding: lora_gemma_fixed/tokenizer.model (deflated 52%)\n",
            "  adding: lora_gemma_fixed/README.md (deflated 65%)\n",
            "  adding: lora_gemma_fixed/adapter_model.safetensors (deflated 11%)\n",
            "  adding: lora_gemma_fixed/tokenizer_config.json (deflated 97%)\n",
            "  adding: lora_gemma_fixed/special_tokens_map.json (deflated 77%)\n",
            "  adding: lora_gemma_fixed/adapter_config.json (deflated 57%)\n",
            "  adding: lora_gemma_fixed/processor_config.json (deflated 20%)\n",
            "  adding: lora_gemma_fixed/preprocessor_config.json (deflated 55%)\n",
            "  adding: lora_gemma_fixed/tokenizer.json (deflated 83%)\n",
            "  adding: lora_gemma_fixed/chat_template.jinja (deflated 71%)\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "download(\"download_a6ab4985-5c43-47b5-9c74-8d7166249ccf\", \"lora_gemma_3n.zip\", 580927155)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∞–¥–∞–ø—Ç–µ—Ä—ã (LoRa) –ª–æ–∫–∞–ª—å–Ω–æ\n",
        "output_dir = \"lora_gemma_fixed\"\n",
        "model.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "print(f\"‚úÖ –õ–æ–∫–∞–ª—å–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_dir}\")\n",
        "\n",
        "# —Å–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–∞ drive\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"/content/drive\"):\n",
        "    print(\"Drive —É–∂–µ –ø—Ä–∏–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω\")\n",
        "else:\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# –∫–æ–ø–∏—Ä—É–µ–º –ø–∞–ø–∫—É —Å –º–æ–¥–µ–ª—å—é –Ω–∞ –¥–∏—Å–∫\n",
        "dest_path = f\"/content/drive/MyDrive/{output_dir}\"\n",
        "!cp -r {output_dir} \"{dest_path}\"\n",
        "\n",
        "print(f\"‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –Ω–∞ Google –î–∏—Å–∫–µ: {dest_path}\")\n",
        "\n",
        "# c–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤–æ–º\n",
        "!zip -r lora_gemma_3n.zip {output_dir}\n",
        "from google.colab import files\n",
        "files.download('lora_gemma_3n.zip')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## –ò—Ç–æ–≥–æ–≤—ã–µ –≤—ã–≤–æ–¥—ã\n"
      ],
      "metadata": {
        "id": "4iS75AG-DSdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "–ë—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç–∏ –º—É–ª—å—Ç–∏–º–æ–¥–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Gemma-3n-4B –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞, —É–ø—Ä–∞–≤–ª—è—é—â–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–º –≤–≤–æ–¥–æ–º.\n",
        "\n",
        "#### **–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä**\n",
        "- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–¥–∞—á ASR –∏ LLM –≤ Pipeline D –æ–∫–∞–∑–∞–ª–æ—Å—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ —á–µ–º –ø–æ–¥—Ö–æ–¥ End-to-End –≤ Pipeline C.\n",
        "- Pipeline D –ø–æ–∫–∞–∑–∞–ª —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ —á–∏—Å—Ç–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–π –∑–∞–¥–∞—á–µ (p. A). –ö–∞—á–µ—Å—Ç–≤–æ ASR (p. B) –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ. –ú–æ–¥–µ–ª—å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞–¥–µ–∂–Ω–æ, —Ä–∞–∑–¥–µ–ª—è—è –∑–∞–¥–∞—á–∏ –≤–æ—Å–ø—Ä–∏—è—Ç–∏—è –∏ –º—ã—à–ª–µ–Ω–∏—è.\n",
        "- Pipeline C –ø–æ–∫–∞–∑–∞–ª –ø–∞–¥–µ–Ω–∏–µ Recall –¥–æ 83%. –ú–æ–¥–µ–ª–∏ —Å–ª–æ–∂–Ω–µ–µ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∞—É–¥–∏–æ–ø–æ—Ç–æ–∫ –∏ –ø—Ä–∏–Ω–∏–º–∞—Ç—å —Ä–µ—à–µ–Ω–∏–µ –æ –≤—ã–∑–æ–≤–µ —Ñ—É–Ω–∫—Ü–∏–∏.\n",
        "\n",
        "#### **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã Fine-Tuning**\n",
        "–ë–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å (p. A) –ø–æ–∫–∞–∑–∞–ª–∞ –≤—ã—Å–æ–∫—É—é —Ç–æ—á–Ω–æ—Å—Ç—å ~94%, –Ω–æ —Å—Ç—Ä–∞–¥–∞–ª–∞ –æ—Ç –ª–æ–∂–Ω—ã—Ö —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–π (False Alarm Rate ~11.5%). –û–Ω–∞ –ø—ã—Ç–∞–ª–∞—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –∫–æ–º–∞–Ω–¥—É (–≤—ã–∑–≤–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç) –¥–∞–∂–µ –Ω–∞ —Ä–∞–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã.\n",
        "\n",
        "–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è LoRA –Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ —É–¥–∞–ª–æ—Å—å –¥–æ—Å—Ç–∏—á—å —Ö–æ—Ä–æ—à–∏—Ö –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π –Ω–∞ (–Ω–µ–±–æ–ª—å—à–æ–π) —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ :\n",
        "- Accuracy: 94% ‚Üí 100%\n",
        "- False Alarm Rate: 11.5% ‚Üí 0.00%\n",
        "\n",
        "Fine-tuning –ø–æ–∑–≤–æ–ª–∏–ª —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å —á–µ—Ç–∫—É—é –≥—Ä–∞–Ω–∏—Ü—É –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏–π. –ú–æ–¥–µ–ª—å –Ω–∞—É—á–∏–ª–∞—Å—å —Ö–æ—Ä–æ—à–æ –æ—Ç–ª–∏—á–∞—Ç—å –Ω–∞–º–µ—Ä–µ–Ω–∏—è \"–ù–∞–ø–∏—à–∏/–ò–∑–º–µ–Ω–∏\" (–¥–µ–π—Å—Ç–≤–∏–µ) –æ—Ç \"–†–∞—Å—Å–∫–∞–∂–∏/–û–±—ä—è—Å–Ω–∏\" (–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è/—á–∞—Ç).\n",
        "\n",
        "#### **–ü—Ä–æ–≥—Ä–∞–º–º–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è**\n",
        "–†–∞–±–æ—Ç–∞ –¥–æ–≤–µ–¥–µ–Ω–∞ –¥–æ —Å—Ç–∞–¥–∏–∏ MVP –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, —Ä–∞–±–æ—Ç–∞—é—â–µ–≥–æ –≤ —É—Å–ª–æ–≤–∏—è—Ö –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã—Ö —Ä–µ—Å—É—Ä—Å–æ–≤:\n",
        "- –í—ã–ø–æ–ª–Ω–µ–Ω–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤–µ—Å–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç GGUF —Å —Ä–µ—à–µ–Ω–∏–µ–º –ø—Ä–æ–±–ª–µ–º—ã OOM –Ω–∞ T4 (Google Colab) –∏–∑-–∑–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π FP32.\n",
        "- –ü—Ä–æ–≤–µ–¥–µ–Ω–æ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–æ Q4_K_M, –ø–æ–∑–≤–æ–ª–∏–≤—à–µ–µ —É–º–µ—Å—Ç–∏—Ç—å –º–æ–¥–µ–ª—å –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤ –ø–∞–º—è—Ç—å –≤–∏–¥–µ–æ–∫–∞—Ä—Ç—ã RTX 3050 4 GB (laptop).\n",
        "- –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω —É–Ω–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–µ—Ä–≤–∏—Å –Ω–∞ –±–∞–∑–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ Strategy/Factory –¥–ª—è –∑–∞–º–µ–Ω—ã –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ ASR (Whisper/Gemma) –∏ TTS (Edge/Silero).\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω UI –±–µ–∑ –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ —Å–∏—Å—Ç–µ–º–Ω–æ–≥–æ —Ñ–æ–∫—É—Å–∞ –Ω–∞ –±–∞–∑–µ PySide6 —Å –∫–∞—Å—Ç–æ–º–Ω–æ–π –æ—Ç—Ä–∏—Å–æ–≤–∫–æ–π –∏ –∞–Ω–∏–º–∞—Ü–∏—è–º–∏ —Å–æ—Å—Ç–æ—è–Ω–∏—è.\n",
        "- –í–Ω–µ–¥—Ä–µ–Ω–æ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞–º–∏ —á–µ—Ä–µ–∑ Windows Job Objects, –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—â–µ–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ C++ —Å–µ—Ä–≤–µ—Ä–∞ –∏ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ VRAM –ø—Ä–∏ —Å–±–æ—è—Ö.\n",
        "- –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ –Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –±—É—Ñ–µ—Ä–æ–º –æ–±–º–µ–Ω–∞ –∏ —ç–º—É–ª—è—Ü–∏—è –≤–≤–æ–¥–∞ –¥–ª—è —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–ª–ª–∏–∑–∏–π —Ñ–æ–∫—É—Å–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ —Ç–µ–∫—Å—Ç–∞.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "5Q9XH_hJDOtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "8jYsmQkuI3c4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "DGfMVvooI3VZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "."
      ],
      "metadata": {
        "id": "pZF9HhFeI3M_"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeBjJkkdwjMG"
      },
      "source": [
        "\n",
        "---\n",
        "---\n",
        "---\n",
        "## –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ .GGUF —Ñ–æ—Ä–º–∞—Ç.\n",
        "- –Ω–µ—É–¥–∞—á–Ω—ã–µ –ø–æ–ø—ã—Ç–∫–∏ —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å –≤ .GGUF –≤ —Å—Ä–µ–¥–µ Google Colab\n",
        "- –æ—Å—Ç–∞–≤–ª–µ–Ω–æ –∏—Å–∫–ª—é—á–∏—Ç–µ–ª—å–Ω–æ –¥–ª—è –∏—Å—Ç–æ—Ä–∏–∏\n",
        "- –≤ –∫–æ–Ω—á–µ–Ω–æ–º —Å—á–µ—Ç–µ –ø—Ä–æ—Ü–µ—Å—Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –±—ã–ª –ø–µ—Ä–µ–Ω–µ—Å–µ–Ω –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—ã–π –ü–ö (–ø–æ—Å–ª–µ  \n",
        "—Å—Ç–∞–¥–∏–∏ —Å–ª–∏—è–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å LoRa –∞–¥–∞–ø—Ç–µ—Ä–∞–º–∏), –∏–∑-–∑–∞ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –Ω–∞ T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4d010c2e831a48d6bbed014ac853671c",
            "5df884fad7d14dacb9ed4e09d08b69ba",
            "a7f41acf77f04e0cb2c012e1141d440e",
            "3ad2c5d5bd96486aa4b4f426c9cb2744",
            "11145f76ab1f4ea78f73fed254b6d039"
          ]
        },
        "id": "W1A3pC8cs86D",
        "outputId": "daf4d24b-7fa6-4b31-8a9e-7f70be4c2295"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑: lora_gemma_fixed ...\n",
            "==((====))==  Unsloth 2026.2.1: Fast Gemma3N patching. Transformers: 4.56.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3n won't work! Using float32.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d010c2e831a48d6bbed014ac853671c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: ##### The current model auto adds a BOS token.\n",
            "Unsloth: ##### Your chat template has a BOS token. We shall remove it temporarily.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –≤ GGUF...\n",
            "Unsloth: Merging model weights to 16-bit format...\n",
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5df884fad7d14dacb9ed4e09d08b69ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Preparing safetensor model files:  25%|‚ñà‚ñà‚ñå       | 1/4 [00:55<02:46, 55.46s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a7f41acf77f04e0cb2c012e1141d440e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Preparing safetensor model files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [03:29<03:47, 113.66s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ad2c5d5bd96486aa4b4f426c9cb2744",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rUnsloth: Preparing safetensor model files:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [06:29<02:23, 143.93s/it]"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11145f76ab1f4ea78f73fed254b6d039",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [07:29<00:00, 112.29s/it]\n",
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [09:21<00:00, 140.36s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/gemma_3n_magic_paste_q4_k_m`\n",
            "Unsloth: Converting to GGUF format...\n",
            "==((====))==  Unsloth: Conversion from HF to GGUF information\n",
            "   \\\\   /|    [0] Installing llama.cpp might take 3 minutes.\n",
            "O^O/ \\_/ \\    [1] Converting HF to GGUF f16 might take 3 minutes.\n",
            "\\        /    [2] Converting GGUF f16 to ['q4_k_m'] might take 10 minutes each.\n",
            " \"-____-\"     In total, you will have to wait at least 16 minutes.\n",
            "\n",
            "Unsloth: Installing llama.cpp. This might take 3 minutes...\n",
            "Unsloth: Updating system package directories\n",
            "Unsloth: All required system packages already installed!\n",
            "Unsloth: Install llama.cpp and building - please wait 1 to 3 minutes\n",
            "Unsloth: Cloning llama.cpp repository\n",
            "Unsloth: Install GGUF and other packages\n",
            "Unsloth: Successfully installed llama.cpp!\n",
            "Unsloth: Preparing converter script...\n",
            "Unsloth: [1] Converting model into f16 GGUF format.\n",
            "This might take 3 minutes...\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Unsloth: GGUF conversion failed: Unsloth: Failed to convert text model to GGUF: Command 'python llama.cpp/unsloth_convert_hf_to_gguf.py --outfile gemma-3n-e4b-it.F16.gguf --outtype f16 --split-max-size 50G gemma_3n_magic_paste_q4_k_m' returned non-zero exit status 137.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/llama_cpp.py\u001b[0m in \u001b[0;36mconvert_to_gguf\u001b[0;34m(model_name, input_folder, model_dtype, quantization_type, converter_location, supported_text_archs, supported_vision_archs, is_vlm, is_gpt_oss, max_shard_size, print_output, print_outputs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1038\u001b[0;31m                 \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1039\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCalledProcessError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             raise CalledProcessError(retcode, process.args,\n\u001b[0m\u001b[1;32m    572\u001b[0m                                      output=stdout, stderr=stderr)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'python llama.cpp/unsloth_convert_hf_to_gguf.py --outfile gemma-3n-e4b-it.F16.gguf --outtype f16 --split-max-size 50G gemma_3n_magic_paste_q4_k_m' returned non-zero exit status 137.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1986\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1987\u001b[0;31m         all_file_locations, want_full_precision, is_vlm_update = save_to_gguf(\n\u001b[0m\u001b[1;32m   1988\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36msave_to_gguf\u001b[0;34m(model_name, model_type, model_dtype, is_sentencepiece, model_directory, quantization_method, first_conversion, is_vlm, is_gpt_oss)\u001b[0m\n\u001b[1;32m   1228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         initial_files, is_vlm_update = convert_to_gguf(\n\u001b[0m\u001b[1;32m   1230\u001b[0m             \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth_zoo/llama_cpp.py\u001b[0m in \u001b[0;36mconvert_to_gguf\u001b[0;34m(model_name, input_folder, model_dtype, quantization_type, converter_location, supported_text_archs, supported_vision_archs, is_vlm, is_gpt_oss, max_shard_size, print_output, print_outputs)\u001b[0m\n\u001b[1;32m   1041\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsloth: Failed to convert {description} to GGUF: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: Failed to convert text model to GGUF: Command 'python llama.cpp/unsloth_convert_hf_to_gguf.py --outfile gemma-3n-e4b-it.F16.gguf --outtype f16 --split-max-size 50G gemma_3n_magic_paste_q4_k_m' returned non-zero exit status 137.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1795072905.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m# –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (Q4_K_M)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m model.save_pretrained_gguf(\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;34m\"gemma_3n_magic_paste_q4_k_m\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/unsloth/save.py\u001b[0m in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   2005\u001b[0m             )\n\u001b[1;32m   2006\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2007\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unsloth: GGUF conversion failed: {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m     \u001b[0;31m# Step 9: Create Ollama modelfile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Unsloth: GGUF conversion failed: Unsloth: Failed to convert text model to GGUF: Command 'python llama.cpp/unsloth_convert_hf_to_gguf.py --outfile gemma-3n-e4b-it.F16.gguf --outtype f16 --split-max-size 50G gemma_3n_magic_paste_q4_k_m' returned non-zero exit status 137."
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "\n",
        "saved_model_path = \"lora_gemma_fixed\"\n",
        "\n",
        "print(f\"–ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑: {saved_model_path} ...\")\n",
        "\n",
        "# –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = saved_model_path,\n",
        "    max_seq_length = 2048,\n",
        "    load_in_4bit = True,\n",
        "    dtype = torch.float16,\n",
        "    device_map = \"auto\",\n",
        "\n",
        ")\n",
        "\n",
        "# –ø–µ—Ä–µ–≤–æ–¥–∏–º –≤ —Ä–µ–∂–∏–º –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞\")\n",
        "\n",
        "\n",
        "import gc\n",
        "\n",
        "# –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ —Ç—è–∂–µ–ª–æ–π –æ–ø–µ—Ä–∞—Ü–∏–µ–π\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"‚è≥ –ù–∞—á–∏–Ω–∞–µ–º –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—é –≤ GGUF...\")\n",
        "\n",
        "# –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ (Q4_K_M)\n",
        "model.save_pretrained_gguf(\n",
        "    \"gemma_3n_magic_paste_q4_k_m\",\n",
        "    tokenizer,\n",
        "    quantization_method = \"q4_k_m\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYh8grXA5B8e"
      },
      "source": [
        "### ‚ö†Ô∏è –í–µ—Å–∞ —Å–æ—Ö—Ä–∞–Ω–∏–ª–∏—Å—å, –Ω–æ –ø—Ä–æ—Ü–µ—Å—Å –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–∏–ª—Å—è –æ—à–∏–±–∫–æ–π...  \n",
        "- –∑–∞–Ω–æ–≤–æ –∑–∞–ø—É—Å–∫–∞–µ–º –±–ª–æ–∫ –∫–æ–¥–∞ \"—É—Å—Ç–∞–Ω–æ–≤–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫\"\n",
        "- –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –∫–æ–ø–∏—é –≤–µ—Å–æ–≤ —Å –≥—É–≥–ª –¥–∏—Å–∫–∞\n",
        "- –°–ù–ê–ß–ê–õ–ê –¥–µ–ª–∞–µ–º —Å–ª–∏—è–Ω–∏–µ–º LoRa –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ —Å —á–∏—Å—Ç–æ–π –º–æ–¥–µ–ª—å—é, —á—Ç–æ–±—ã –º–∏–Ω–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ –ø–∞–º—è—Ç–∏ –Ω–∞ —ç—Ç–∞–ø–µ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ GGUF\n",
        "- –ø–æ—Ç–æ–º –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ GGUF –∏ –∫–≤–∞–Ω—Ç—É–µ–º\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVdCTVg4MuFJ"
      },
      "source": [
        "Swap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7GKHXa-RLSlT",
        "outputId": "0b70e23b-cdf1-4117-bc99-ec391ee66d55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "4294967296 bytes (4.3 GB, 4.0 GiB) copied, 28 s, 156 MB/s\n",
            "4+0 records in\n",
            "4+0 records out\n",
            "4294967296 bytes (4.3 GB, 4.0 GiB) copied, 27.5198 s, 156 MB/s\n",
            "^C\n",
            "Setting up swapspace version 1, size = 4 GiB (4294963200 bytes)\n",
            "no label, UUID=b9795fa8-adde-41d9-834b-e74456a91815\n",
            "swapon: /content/drive/MyDrive/swapfile: swapon failed: Invalid argument\n",
            "‚úÖ Swap file created and enabled (8GB) in Google Drive\n"
          ]
        }
      ],
      "source": [
        "# # –º–æ–Ω—Ç–∏—Ä—É–µ–º –¥–∏—Å–∫\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # —Å–æ–∑–¥–∞–µ–º SWAP 8GB —á—Ç–æ–±—ã –∏–∑–±–µ–∂–∞—Ç—å –≤—ã–ª–µ—Ç–∞ Colab –ø–æ –ø–∞–º—è—Ç–∏\n",
        "# !fallocate -l 8G /swapfile\n",
        "# !chmod 600 /swapfile\n",
        "# !mkswap /swapfile\n",
        "# !swapon /swapfile\n",
        "# print(\"‚úÖ Swap file created and enabled (8GB)\")\n",
        "\n",
        "\n",
        "# # –º–æ–Ω—Ç–∏—Ä—É–µ–º –¥–∏—Å–∫\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # —Å–æ–∑–¥–∞–µ–º SWAP 8GB —á–µ—Ä–µ–∑ dd (–º–µ–¥–ª–µ–Ω–Ω–µ–µ fallocate, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤—Å–µ–≥–¥–∞)\n",
        "# !dd if=/dev/zero of=/swapfile bs=1G count=8 status=progress\n",
        "# !chmod 600 /swapfile\n",
        "# !mkswap /swapfile\n",
        "# !swapon /swapfile\n",
        "# print(\"‚úÖ Swap file created and enabled (8GB) via dd\")\n",
        "\n",
        "\n",
        "# –º–æ–Ω—Ç–∏—Ä—É–µ–º –¥–∏—Å–∫\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# # —Å–æ–∑–¥–∞–µ–º SWAP 8GB –≤ Google Drive (ext4 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç swap)\n",
        "# !dd if=/dev/zero of=/content/drive/MyDrive/swapfile bs=1G count=8 status=progress\n",
        "# !chmod 600 /content/drive/MyDrive/swapfile\n",
        "# !mkswap /content/drive/MyDrive/swapfile\n",
        "# !swapon /content/drive/MyDrive/swapfile\n",
        "# print(\"‚úÖ Swap file created and enabled (8GB) in Google Drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i0Q8oQgU2PJ",
        "outputId": "66a1c472-331a-48bc-c9fd-3ff643cc3eaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Swap file already exists\n",
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       994Mi       2.3Gi       4.0Mi       9.4Gi        11Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "# –º–æ–Ω—Ç–∏—Ä—É–µ–º –¥–∏—Å–∫\n",
        "from google.colab import drive\n",
        "import os\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# —Å–æ–∑–¥–∞–µ–º SWAP 8GB —á–µ—Ä–µ–∑ dd (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –Ω–∞–¥–µ–∂–Ω–æ)\n",
        "if not os.path.exists('/swapfile'):\n",
        "    print(\"–°–æ–∑–¥–∞–µ–º Swap-—Ñ–∞–π–ª\")\n",
        "    !dd if=/dev/zero of=/swapfile bs=1G count=8 status=progress\n",
        "    !chmod 600 /swapfile\n",
        "    !mkswap /swapfile\n",
        "    !swapon /swapfile\n",
        "    print(\"‚úÖ Swap file created and enabled (8GB)\")\n",
        "else:\n",
        "    print(\"‚úÖ Swap file already exists\")\n",
        "\n",
        "# –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏\n",
        "!free -h\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install unsloth\n",
        "!pip install --no-deps \"xformers<0.0.27\" \"trl<0.9.0\" peft accelerate bitsandbytes\n",
        "!pip install \"huggingface_hub>=0.24.0\"\n",
        "!pip install \"transformers>=4.43.0\""
      ],
      "metadata": {
        "id": "4dhrZAG4nGKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-yHTLSRlyKYR",
        "outputId": "1a5c5084-e951-4bba-ae04-b11fbe914991"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               total        used        free      shared  buff/cache   available\n",
            "Mem:            12Gi       780Mi       9.1Gi       5.0Mi       2.8Gi        11Gi\n",
            "Swap:             0B          0B          0B\n"
          ]
        }
      ],
      "source": [
        "!free -h\n",
        "!swapon --show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCTkwVgWMvCF"
      },
      "source": [
        "#### –°–ª–∏—è–Ω–∏–µ LoRa –∏ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLASWO8CL_Kv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "b50ac657bd7848db9969c647f364854e",
            "062590f3776843db80ed44fdc3cc3172",
            "14fb10a1376f460ca8a60c2a39583703",
            "8cbf15c6f1bb412dbe05781b388ad27a",
            "ee4f6bf36b324e2fbfe301ce367bfa35",
            "94e3ff65e7a24c568b8d6717c62686ae",
            "f8f0cafd634c420db0e52421922da3df",
            "c9fa4ece813a46eab3d5eb6b32d268e1",
            "fb76967345114dd7aa655018275b1251",
            "068be1900ccc4b29b3d44785bb321a32",
            "db4ab5e47d5945bda061930877e82cd3",
            "3188129ddcc54b22a58f95d273b07bca",
            "9058b56241ae4a29a41fc678dca46477",
            "7082cca5bdcc47a197ae3c56f474635c",
            "1aded047af7b4239935790258321383c",
            "d800da40261b405ea2e0f4b08b9d57b4",
            "d8e5fc0c9d304d4c84c7e9dee9b8d37e",
            "cdbce73e904c4604aa45054e4ae44df3",
            "94063b867d6b47fc871238fe8e0bfa81",
            "9e45deed2cfc4340a6958c58fecd9de5",
            "3db997eff69b4fb89a3707d8dd566501",
            "b10df76796c24478b9b9c66c228b769e",
            "b27f7bffb5dd40e0a1374dd05c88901d",
            "2816aadcabc04171aec80edd533cbb71",
            "0b225bd4e44e41a09678a385bc0e5e62",
            "bde32e0898f84cfb8bc1864978423da1",
            "ddc00b8f44514d02acf52cdbedd0144d",
            "f22c21b130a8452a9e4cc3fc979c7eea",
            "b9595b893e7b4c06a99380f1a23bc3bc",
            "9964b21af35f4389988a50301a24ac2b",
            "fa4761fd2b6b4dccb3dc1c775b9beff2",
            "61f25ba220624998858b286b36c35b24",
            "e22d72e3c05d488890aefa3f94d72a41",
            "fdd0b3945c8245c6ade27e98ead02334",
            "9c943f7b3c614f5c9ef0ce2078ec5965",
            "e369d09753a442e5a325017a6395e1ca",
            "34e6e32aabf1492aa6f986bf4417af97",
            "4d76dce8462a42e892c69049312ed3bb",
            "08dfbcdc92464fdd8b94c079306bc185",
            "7b4fb488bf744c97ab5debc69980cc2f",
            "6ffe902acc2b49ad8f555a7861b1b12f",
            "b2685a01b17e47e29bf5dc80efec3b80",
            "e4f175fdae264610a541bcc81788c713",
            "29940884369b4595a1038c4cb7450844",
            "6df7de7827284e0982dd5dd88824a309",
            "415e4adff8f04bae848512a5ec77c05c",
            "af158ee9538849418306eca9d6865470",
            "20a2c9acc980416c9a066331a817ad8a",
            "72ecd370cdd04d5096db7baab1a1f65b",
            "d4256aeec48e45ff8b8c28e1df5cab4b",
            "243f22d3e8954ad49de2e7de4104014b",
            "36fabc6d0dac48f6b0c7f061ad31e03c",
            "ab079e6c8020496b979db34e608c4192",
            "bc11d58fcfb64cfea628a33ab20ff228",
            "d4066ed306b04abdbd00ca55857d27ee",
            "cd928ca1cf05479c8f620de3792ccfb2",
            "e99d9ffa12ff4f5792f6435169d0b901",
            "5fe23b2479264013ba326c3213110a45",
            "5c5761561e67426f9e02f1874cda10f1",
            "5efbe5baca934d5da95520ad395fb8c0",
            "a6aa50b6858e4869b30e8c92987d34e8",
            "59b028f5e37f4ea0878c57a7bc6ab7f1",
            "b0b79847c4ce4265969b058dbba37b60",
            "c426a901bdea47e8b0b72ce517a2e681",
            "b38e5977fee04b1aadac18008689aac0",
            "9a2f85d12a854a6faa5a8f450b9fc97e",
            "989f8623c1f44159aaa093093faecac4",
            "f36012e991114623929a36d001b6d04c",
            "ec8c96eeca2f4be7aaea3ca16507fade",
            "fc1b07ec1c5f4d79bdad01ed6de7d5e5",
            "62bfcd76e5a94257b00f1fada4930140",
            "56db82b0fe7e4fdeab78fe1d98ea7794",
            "dad8f526ca4d449caa9c201767fd2d98",
            "91fac95d702b435ea66327fdb26dc0fb",
            "b5c3e3b7227542d0a7966a5f2157bbc8",
            "447bd7d28c73467182c7c3c069682a8a",
            "04b833e37b484a97a24d72265c4b914a",
            "ac8c06ccae7a4ad7b04febf8eb87ae9e",
            "e9cba4c3004e4daaa1000f6342808318",
            "f6d6fcfbceed4473b984e3fdc0ca2f0c",
            "c10937ccf14a4051bb8f3bf4184774ec",
            "305e1358dcae4bc89e047bdbac0c69e4",
            "cc546d70aaef4c15abdc7606b4057bd9",
            "726f201fe289464382057b7717821466",
            "a5b9cd169f70432ea6c4b0b22686b6de",
            "ad165276183046b8bfe5598bbcde678c",
            "112531bded4f4dbd942f53bb3b842782",
            "5a2c5979d65c4af3bf30e41db3b381dd",
            "8935921a7d174f829133f36d3323ac61",
            "5ccd80cf8c954f109b3116238acecd66",
            "dd23adb0360c45e69f97cfad3d805cfd",
            "d5972f21d2b045b8b0d726a975798b4b",
            "eb40716f6e9543058b90c99b181b2ce9",
            "a5a268473df5409fa8c7f928d8cc64c4",
            "807a1b1302074522842c6e95dd684024",
            "8fee2fc439454d4c9b563a87eb55ce0f",
            "9263c148bc7a4edeb7bfec04aa8d4819",
            "a1d32886d2434db2be797f0eb5c0d834",
            "478ed749673045d097d7335717e5fa35",
            "5be77fe360c34ddc931c22251d2f12ce",
            "63837f4f9ca04af088a43c173f4b3796",
            "003636ef937a4e4c909e0400ca1f54bf",
            "d7fa80fe05424038a1992cfadad8844f",
            "1ccf7bad6b2c4b2d804371ed1d0bafff",
            "fa8cc2a5091f410992f8e593bcc6ed46",
            "01b1fb495bc548b182c493f5c4e28840",
            "658d9514c1544e34a84e20fad8ca42f8",
            "e1e02b60a018470fac8b2b5174e13b1e",
            "5bf0add0526b42ac833a964cea2c25cb",
            "0bd8f576eaca4f71a0b9c8c7576da471",
            "68af71c7d8384349bf9deeb31c6db8a5",
            "2933df4e36b8467b932bc1fdf88f1c5d",
            "46823449e6fd4e19b700dd270e3b2f0b",
            "66bdb644305040e08dca7b897c8e6cf6",
            "5e434fb4556346029dfbcdc9bdbb6085",
            "c27b0ebd011a4395ade0a503225aae05",
            "885e02b835694d43b9fb2838e0083767",
            "56519b1506e54b35bc148d5acf3f8ff0",
            "a729a5a1bf614d00a0c23276566956e9",
            "47591f48444a43f4be1dfbef22753007",
            "1adb76b10d6f45389909bf9f53f8fd10",
            "e45b057c4a724f949e59da20c8d2708d",
            "83b12a5c712e4337a82ae532b3ff470c",
            "85f8fb6af99f4a7d91699d625a8abbaf",
            "756e574d780c44b2b7f2248e7c0d6b3f",
            "2411ba645082439681ee45799b9bfc69",
            "b68d5dc3fd46432aa81013e29fafaee4",
            "6ef6578764a54b0d9c811ce68ad1acd7",
            "e8943f35c5d44cfc9001a4fc429bc765",
            "45811cbf4a35413598215840bbda951c",
            "813474887c264c58a9821a63ba2d2b5a",
            "16c8d403216f480a88126768f49249aa"
          ]
        },
        "outputId": "8fab1594-e1a2-4eae-e4d3-4ccbec647221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "–ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∏–∑: /content/drive/MyDrive/lora_gemma_fixed\n",
            "==((====))==  Unsloth 2026.2.1: Fast Gemma3N patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.34. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Using float16 precision for gemma3n won't work! Using float32.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b50ac657bd7848db9969c647f364854e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3188129ddcc54b22a58f95d273b07bca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b27f7bffb5dd40e0a1374dd05c88901d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fdd0b3945c8245c6ade27e98ead02334"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6df7de7827284e0982dd5dd88824a309"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd928ca1cf05479c8f620de3792ccfb2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–ò–¥–µ—Ç merge\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "989f8623c1f44159aaa093093faecac4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac8c06ccae7a4ad7b04febf8eb87ae9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking cache directory for required files...\n",
            "Cache check failed: model-00001-of-00004.safetensors not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n",
            "Checking cache directory for required files...\n",
            "Cache check failed: tokenizer.model not found in local cache.\n",
            "Not all required files found in cache. Will proceed with downloading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Preparing safetensor model files:   0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/3.08G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8935921a7d174f829133f36d3323ac61"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Preparing safetensor model files:  25%|‚ñà‚ñà‚ñå       | 1/4 [01:20<04:01, 80.44s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5be77fe360c34ddc931c22251d2f12ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Preparing safetensor model files:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 2/4 [03:45<03:57, 118.65s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68af71c7d8384349bf9deeb31c6db8a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rUnsloth: Preparing safetensor model files:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 3/4 [05:17<01:46, 106.51s/it]"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/2.66G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e45b057c4a724f949e59da20c8d2708d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Unsloth: Preparing safetensor model files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [05:48<00:00, 87.23s/it]\n",
            "Unsloth: Merging weights into 16bit: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4/4 [14:02<00:00, 210.73s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: Merge process complete. Saved to `/content/merged_model_hf`\n",
            "‚úÖ –°–ª–∏—è–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: /content/merged_model_hf\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "92263"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import gc\n",
        "\n",
        "# –ø—É—Ç—å –∫ –∞–¥–∞–ø—Ç–µ—Ä–∞–º –Ω–∞ –¥–∏—Å–∫–µ\n",
        "adapter_path = \"/content/drive/MyDrive/lora_gemma_fixed\"\n",
        "output_merged_path = \"/content/merged_model_hf\"\n",
        "\n",
        "print(f\"–ó–∞–≥—Ä—É–∑–∫–∞ –∞–¥–∞–ø—Ç–µ—Ä–æ–≤ –∏–∑: {adapter_path}\")\n",
        "\n",
        "# –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = adapter_path,\n",
        "    max_seq_length = 2048,\n",
        "    dtype = None,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "# —Å–ª–∏–≤–∞–µ–º –≤–µ—Å–∞ –∏ –≤—ã–≥—Ä—É–∂–∞–µ–º LoRa\n",
        "print(\"–ò–¥–µ—Ç merge\")\n",
        "model.save_pretrained_merged(\n",
        "    output_merged_path,\n",
        "    tokenizer,\n",
        "    save_method = \"merged_16bit\",\n",
        ")\n",
        "print(f\"‚úÖ –°–ª–∏—è–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ. –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_merged_path}\")\n",
        "\n",
        "# —á–∏—Å—Ç–∏–º –ø–∞–º—è—Ç—å\n",
        "del model\n",
        "del tokenizer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–°–∫–æ–ø–∏—Ä—É–µ–º –Ω–∞ google drive, —á—Ç–æ–±—ã –Ω–µ —Ç–µ—Ä—è—Ç—å –≤—Ä–µ–º—è –Ω–∞ merge –≤ —Å–ª–µ–¥. —Ä–∞–∑."
      ],
      "metadata": {
        "id": "lub1vlfAs580"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r /content/merged_model_hf /content/drive/MyDrive/merged_model_hf"
      ],
      "metadata": {
        "id": "7svFwnLgs5Op"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eO0fyKPSM0nY"
      },
      "source": [
        "–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∏ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4  # —Ñ–∏–∫—Å–∏—Ç numpy conflicts\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRb9qD13rx6c",
        "outputId": "28b208e0-fb00-4a7c-b4ed-91943bf8c92d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.12/dist-packages (1.26.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# –∫–ª–æ–Ω–∏—Ä—É–µ–º –∏ —Å–æ–±–∏—Ä–∞–µ–º llama.cpp\n",
        "# if not os.path.exists(\"llama.cpp\"):\n",
        "#     !git clone https://github.com/ggerganov/llama.cpp\n",
        "#     !cd llama.cpp && make clean && make -j 4\n",
        "# –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—É—é –ø–∞–ø–∫—É –∏ –∫–ª–æ–Ω–∏—Ä—É–µ–º –∑–∞–Ω–æ–≤–æ\n",
        "%cd /content\n",
        "!rm -rf llama.cpp\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "\n",
        "# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º CMake –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
        "!apt update -qq && apt install -qq cmake build-essential -y\n",
        "\n",
        "# –°–æ–±–∏—Ä–∞–µ–º —Å CUDA –¥–ª—è Colab T4\n",
        "!cmake -B build -DGGML_CUDA=ON\n",
        "!cmake --build build --config Release -j2\n",
        "\n",
        "# –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∏–Ω–∞—Ä–Ω–∏–∫–∏\n",
        "!ls -la build/bin/\n",
        "\n",
        "# –§–∏–∫—Å–∏–º –æ–∫—Ä—É–∂–µ–Ω–∏–µ\n",
        "import os\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/llama.cpp:\" + os.environ.get(\"PYTHONPATH\", \"\")\n",
        "\n",
        "# # —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏\n",
        "# !pip install -r llama.cpp/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_vYLf7TspBL",
        "outputId": "78ad3390-7d18-4b29-fdd4-722ef15607e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 80064, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 80064 (delta 1), reused 1 (delta 1), pack-reused 80054 (from 2)\u001b[K\n",
            "Receiving objects: 100% (80064/80064), 294.68 MiB | 17.34 MiB/s, done.\n",
            "Resolving deltas: 100% (57887/57887), done.\n",
            "Updating files: 100% (2272/2272), done.\n",
            "/content/llama.cpp\n",
            "48 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 48 not upgraded.\n",
            "-- The C compiler identification is GNU 11.4.0\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- The ASM compiler identification is GNU\n",
            "-- Found assembler: /usr/bin/cc\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- Found OpenMP_C: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\")\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- Found CUDAToolkit: /usr/local/cuda/targets/x86_64-linux/include (found version \"12.8.93\")\n",
            "-- CUDA Toolkit found\n",
            "-- The CUDA compiler identification is NVIDIA 12.8.93 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Using CMAKE_CUDA_ARCHITECTURES=75-real CMAKE_CUDA_ARCHITECTURES_NATIVE=75-real\n",
            "-- CUDA host compiler is GNU 11.4.0\n",
            "-- Including CUDA backend\n",
            "-- ggml version: 0.9.5\n",
            "-- ggml commit:  01d8eaa28\n",
            "-- Found OpenSSL: /usr/lib/x86_64-linux-gnu/libcrypto.so (found version \"3.0.2\")\n",
            "-- Performing Test OPENSSL_VERSION_SUPPORTED\n",
            "-- Performing Test OPENSSL_VERSION_SUPPORTED - Success\n",
            "-- OpenSSL found: 3.0.2\n",
            "-- Generating embedded license file for target: common\n",
            "-- Configuring done (31.8s)\n",
            "-- Generating done (0.5s)\n",
            "-- Build files have been written to: /content/llama.cpp/build\n",
            "[  0%] \u001b[32mBuilding CXX object common/CMakeFiles/build_info.dir/build-info.cpp.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml.c.o\u001b[0m\n",
            "[  0%] Built target build_info\n",
            "[  0%] \u001b[32mBuilding CXX object vendor/cpp-httplib/CMakeFiles/cpp-httplib.dir/httplib.cpp.o\u001b[0m\n",
            "[  0%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-alloc.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-backend.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-opt.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/ggml-threading.cpp.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-base.dir/ggml-quants.c.o\u001b[0m\n",
            "[  1%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-base.dir/gguf.cpp.o\u001b[0m\n",
            "[  2%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-base.so\u001b[0m\n",
            "[  2%] Built target ggml-base\n",
            "[  2%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha256.dir/deps/sha256/sha256.c.o\u001b[0m\n",
            "[  2%] Built target sha256\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/xxhash.dir/deps/xxhash/xxhash.c.o\u001b[0m\n",
            "[  3%] Built target xxhash\n",
            "[  3%] \u001b[32mBuilding C object examples/gguf-hash/CMakeFiles/sha1.dir/deps/sha1/sha1.c.o\u001b[0m\n",
            "[  3%] Built target sha1\n",
            "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-llava-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  3%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-llava-cli\u001b[0m\n",
            "[  3%] Built target llama-llava-cli\n",
            "[  3%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-gemma3-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gemma3-cli\u001b[0m\n",
            "[  4%] Built target llama-gemma3-cli\n",
            "[  4%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-minicpmv-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  4%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-minicpmv-cli\u001b[0m\n",
            "[  4%] Built target llama-minicpmv-cli\n",
            "[  5%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-qwen2vl-cli.dir/deprecation-warning.cpp.o\u001b[0m\n",
            "[  5%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-qwen2vl-cli\u001b[0m\n",
            "[  5%] Built target llama-qwen2vl-cli\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/acc.cu.o\u001b[0m\n",
            "[  6%] \u001b[32m\u001b[1mLinking CXX static library libcpp-httplib.a\u001b[0m\n",
            "[  6%] Built target cpp-httplib\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/add-id.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/arange.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argmax.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/argsort.cu.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.c.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ggml-cpu.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/repack.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/hbm.cpp.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/quants.c.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/traits.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/amx.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/amx/mmq.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/binary-ops.cpp.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/unary-ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/vec.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/ops.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/llamafile/sgemm.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding C object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/quants.c.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object ggml/src/CMakeFiles/ggml-cpu.dir/ggml-cpu/arch/x86/repack.cpp.o\u001b[0m\n",
            "[  8%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libggml-cpu.so\u001b[0m\n",
            "[  8%] Built target ggml-cpu\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/binbcast.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/clamp.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/concat.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv-transpose-1d.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-dw.cu.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d-transpose.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/conv2d.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/convert.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/count-equal.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cpy.cu.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cross-entropy-loss.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/cumsum.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diag.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/diagmask.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-tile.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn-wmma-f16.cu.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fattn.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/fill.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/getrows.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ggml-cuda.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/gla.cu.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/im2col.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mean.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmf.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmid.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmq.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvf.cu.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/mmvq.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/norm.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-adamw.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/opt-step-sgd.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/out-prod.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad.cu.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pad_reflect_1d.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/pool2d.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/quantize.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/roll.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/rope.cu.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/scale.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set-rows.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/set.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softcap.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/softmax.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/solve_tri.cu.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-conv.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/ssm-scan.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sum.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/sumrows.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/top-k.cu.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/topk-moe.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tri.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/tsembd.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/unary.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/upscale.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/wkv.cu.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq112-dv112.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq128-dv128.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq256-dv256.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq40-dv40.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq576-dv512.cu.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq64-dv64.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq72-dv72.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq80-dv80.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-tile-instance-dkq96-dv96.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_16.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_32.cu.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_1-ncols2_8.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_1.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_2.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_16-ncols2_4.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_16.cu.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_32.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_4.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_2-ncols2_8.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_1.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_32-ncols2_2.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_16.cu.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_2.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_4.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_4-ncols2_8.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_64-ncols2_1.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_1.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_2.cu.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_4.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/fattn-mma-f16-instance-ncols1_8-ncols2_8.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq1_s.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_s.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xs.cu.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq2_xxs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_s.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq3_xxs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_nl.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-iq4_xs.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-mxfp4.cu.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q2_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q3_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_0.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_1.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q4_k.cu.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_0.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_1.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CUDA object ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o\u001b[0m\n",
            "nvcc error   : 'fatbinary' died due to signal 2 \n",
            "nvcc error   : '\"$CICC_PATH/cicc\"' died due to signal 2 \n",
            "gmake[2]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1685: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q6_k.cu.o] Interrupt\n",
            "gmake[2]: *** [ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/build.make:1670: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/template-instances/mmq-instance-q5_k.cu.o] Interrupt\n",
            "gmake[1]: *** [CMakeFiles/Makefile2:2342: ggml/src/ggml-cuda/CMakeFiles/ggml-cuda.dir/all] Interrupt\n",
            "gmake: *** [Makefile:146: all] Interrupt\n",
            "^C\n",
            "total 1956\n",
            "drwxr-xr-x  2 root root    4096 Feb 14 20:26 .\n",
            "drwxr-xr-x 13 root root    4096 Feb 14 20:23 ..\n",
            "lrwxrwxrwx  1 root root      17 Feb 14 20:24 libggml-base.so -> libggml-base.so.0\n",
            "lrwxrwxrwx  1 root root      21 Feb 14 20:24 libggml-base.so.0 -> libggml-base.so.0.9.5\n",
            "-rwxr-xr-x  1 root root  738168 Feb 14 20:24 libggml-base.so.0.9.5\n",
            "lrwxrwxrwx  1 root root      16 Feb 14 20:26 libggml-cpu.so -> libggml-cpu.so.0\n",
            "lrwxrwxrwx  1 root root      20 Feb 14 20:26 libggml-cpu.so.0 -> libggml-cpu.so.0.9.5\n",
            "-rwxr-xr-x  1 root root 1167592 Feb 14 20:26 libggml-cpu.so.0.9.5\n",
            "-rwxr-xr-x  1 root root   16904 Feb 14 20:24 llama-gemma3-cli\n",
            "-rwxr-xr-x  1 root root   16904 Feb 14 20:24 llama-llava-cli\n",
            "-rwxr-xr-x  1 root root   16904 Feb 14 20:24 llama-minicpmv-cli\n",
            "-rwxr-xr-x  1 root root   16904 Feb 14 20:24 llama-qwen2vl-cli\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# –ø—Ä–∞–≤–∏–ª—å–Ω–∞—è —É—Å—Ç–∞–Ω–æ–≤–∫–∞\n",
        "!pip install -q numpy==1.26.4 transformers torch huggingface_hub\n",
        "\n",
        "# –∫–ª–æ–Ω–∏—Ä—É–µ–º llama.cpp (—Ç–æ–ª—å–∫–æ –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä!)\n",
        "!git clone https://github.com/ggerganov/llama.cpp\n",
        "%cd llama.cpp\n",
        "\n",
        "# —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Python –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è –∫–æ–Ω–≤–µ—Ä—Ç–µ—Ä–∞\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HF ‚Üí F16 GGUF (–ï–î–ò–ù–°–¢–í–ï–ù–ù–´–ô –†–ê–ë–û–ß–ò–ô –°–ü–û–°–û–ë)\n",
        "!python convert_hf_to_gguf.py /content/merged_model_hf \\\n",
        "    --outfile /content/temp_f16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "# cPU-only —Å–±–æ—Ä–∫–∞ (3 –º–∏–Ω—É—Ç—ã –≤–º–µ—Å—Ç–æ 37!)\n",
        "!cmake -B build\n",
        "!cmake --build build --config Release -j2\n",
        "\n",
        "# –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ\n",
        "!./build/bin/llama-quantize /content/temp_f16.gguf /content/gemma_3n_q4_k_m.gguf Q4_K_M\n",
        "\n",
        "# –ø—Ä–æ–≤–µ—Ä–∫–∞\n",
        "!ls -lh /content/gemma_3n_q4_k_m.gguf\n",
        "\n",
        "!cp -r /content/gemma_3n_q4_k_m.gguf /content/drive/MyDrive/gemma_3n_q4_k_m.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5RFe8r29zww",
        "outputId": "067d7860-4e13-4ae1-85f7-09deb99375b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 80064, done.\u001b[K\n",
            "remote: Counting objects: 100% (10/10), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "^C\n",
            "[Errno 2] No such file or directory: 'llama.cpp'\n",
            "/content/llama.cpp/llama.cpp\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly, https://download.pytorch.org/whl/cpu, https://download.pytorch.org/whl/nightly\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Ignoring torch: markers 'platform_machine == \"s390x\"' don't match your environment\n",
            "Requirement already satisfied: numpy~=1.26.4 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: sentencepiece<0.3.0,>=0.1.98 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 2)) (0.2.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.57.1 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.57.6)\n",
            "Requirement already satisfied: gguf>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 6)) (0.17.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_legacy_llama.txt (line 7)) (4.25.8)\n",
            "Requirement already satisfied: torch~=2.6.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2.6.0+cpu)\n",
            "Requirement already satisfied: aiohttp~=3.9.3 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 1)) (3.9.5)\n",
            "Requirement already satisfied: pytest~=8.3.3 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 2)) (8.3.5)\n",
            "Requirement already satisfied: huggingface_hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 3)) (0.36.2)\n",
            "Requirement already satisfied: matplotlib~=3.10.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 4)) (3.10.0)\n",
            "Requirement already satisfied: openai~=2.14.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 6)) (2.14.0)\n",
            "Requirement already satisfied: pandas~=2.2.3 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 7)) (2.2.3)\n",
            "Requirement already satisfied: prometheus-client~=0.20.0 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 8)) (0.20.0)\n",
            "Requirement already satisfied: requests~=2.32.3 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 9)) (2.32.4)\n",
            "Requirement already satisfied: wget~=3.2 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 10)) (3.2)\n",
            "Requirement already satisfied: typer~=0.15.1 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 11)) (0.15.4)\n",
            "Requirement already satisfied: seaborn~=0.13.2 in /usr/local/lib/python3.12/dist-packages (from -r ./requirements/requirements-tool_bench.txt (line 12)) (0.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (3.20.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.57.1->-r ./requirements/requirements-convert_legacy_llama.txt (line 4)) (4.67.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (2025.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (6.7.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (1.22.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.12/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (2.3.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest~=8.3.3->-r ./requirements/requirements-tool_bench.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1.0,>=0.34.0->-r ./requirements/requirements-tool_bench.txt (line 3)) (1.2.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (2.9.0.post0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.13.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.12.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas~=2.2.3->-r ./requirements/requirements-tool_bench.txt (line 7)) (2025.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests~=2.32.3->-r ./requirements/requirements-tool_bench.txt (line 9)) (2026.1.4)\n",
            "Requirement already satisfied: click<8.2,>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (13.9.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai~=2.14.0->-r ./requirements/requirements-tool_bench.txt (line 6)) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib~=3.10.0->-r ./requirements/requirements-tool_bench.txt (line 4)) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (2.19.2)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.0->aiohttp~=3.9.3->-r ./requirements/requirements-tool_bench.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch~=2.6.0->-r ./requirements/requirements-convert_hf_to_gguf.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer~=0.15.1->-r ./requirements/requirements-tool_bench.txt (line 11)) (0.1.2)\n",
            "INFO:hf-to-gguf:Loading model: merged_model_hf\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/merged_model_hf: Could not import module 'Gemma3nConfig'. Are this object's requirements defined correctly?\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/merged_model_hf: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00004.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> F16, shape = {2048, 262400}\n",
            "^C\n",
            "\u001b[0mCMAKE_BUILD_TYPE=Release\u001b[0m\n",
            "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
            "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
            "-- GGML_SYSTEM_ARCH: x86\n",
            "-- Including CPU backend\n",
            "-- x86 detected\n",
            "-- Adding CPU backend variant ggml-cpu: -march=native \n",
            "-- ggml version: 0.9.5\n",
            "-- ggml commit:  01d8eaa28\n",
            "-- OpenSSL found: 3.0.2\n",
            "-- Generating embedded license file for target: common\n",
            "-- Configuring done (0.7s)\n",
            "-- Generating done (0.5s)\n",
            "-- Build files have been written to: /content/llama.cpp/llama.cpp/build\n",
            "[  0%] Built target build_info\n",
            "[  2%] Built target ggml-base\n",
            "[  2%] Built target cpp-httplib\n",
            "[  2%] Built target sha256\n",
            "[  3%] Built target xxhash\n",
            "[  3%] Built target sha1\n",
            "[  4%] Built target llama-llava-cli\n",
            "[  5%] Built target llama-gemma3-cli\n",
            "[  6%] Built target llama-minicpmv-cli\n",
            "[  6%] Built target llama-qwen2vl-cli\n",
            "[ 10%] Built target ggml-cpu\n",
            "[ 11%] Built target ggml\n",
            "[ 11%] Built target llama-gguf-hash\n",
            "[ 12%] Built target llama-gguf\n",
            "[ 44%] Built target llama\n",
            "[ 44%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/clip.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/cogvlm.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/conformer.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-parser-xml-toolcall.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/glm4v.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/internvl.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimivl.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/kimik25.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/nemotron-v2-vl.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llama4.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/llava.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/minicpmv.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/pixtral.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen2vl.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/qwen3vl.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/siglip.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/whisper-enc.cpp.o\u001b[0m\n",
            "[ 49%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/mobilenetv5.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/mtmd.dir/models/youtuvl.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/chat.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking CXX shared library ../../bin/libmtmd.so\u001b[0m\n",
            "[ 50%] Built target mtmd\n",
            "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/common.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding C object tests/CMakeFiles/test-c.dir/test-c.c.o\u001b[0m\n",
            "[ 50%] \u001b[32m\u001b[1mLinking C executable ../bin/test-c\u001b[0m\n",
            "[ 50%] Built target test-c\n",
            "[ 50%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/console.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object examples/simple/CMakeFiles/llama-simple.dir/simple.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple\u001b[0m\n",
            "[ 51%] Built target llama-simple\n",
            "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/debug.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object examples/simple-chat/CMakeFiles/llama-simple-chat.dir/simple-chat.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-simple-chat\u001b[0m\n",
            "[ 52%] Built target llama-simple-chat\n",
            "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/download.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-partial.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/llguidance.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/log.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-cache.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-map.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/ngram-mod.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/peg-parser.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/preset.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/regex-partial.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/sampling.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/speculative.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/unicode.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/lexer.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/parser.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/runtime.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/value.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/string.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/jinja/caps.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object common/CMakeFiles/common.dir/__/license.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX static library libcommon.a\u001b[0m\n",
            "[ 57%] Built target common\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-0.dir/test-tokenizer-0.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/test-sampling.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-sampling.dir/get-model.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-sampling\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-0\u001b[0m\n",
            "[ 57%] Built target test-sampling\n",
            "[ 57%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/test-grammar-parser.cpp.o\u001b[0m\n",
            "[ 57%] Built target test-tokenizer-0\n",
            "[ 58%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/test-grammar-integration.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-parser\u001b[0m\n",
            "[ 59%] Built target test-grammar-parser\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-grammar-integration.dir/get-model.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/test-llama-grammar.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-llama-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-llama-grammar\u001b[0m\n",
            "[ 60%] Built target test-llama-grammar\n",
            "[ 60%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/test-chat.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-grammar-integration\u001b[0m\n",
            "[ 60%] Built target test-grammar-integration\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat.dir/get-model.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/test-json-schema-to-grammar.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-schema-to-grammar.dir/get-model.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-schema-to-grammar\u001b[0m\n",
            "[ 62%] Built target test-json-schema-to-grammar\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-stats.dir/test-quantize-stats.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-stats\u001b[0m\n",
            "[ 62%] Built target test-quantize-stats\n",
            "[ 62%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gbnf-validator.dir/test-gbnf-validator.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gbnf-validator\u001b[0m\n",
            "[ 63%] Built target test-gbnf-validator\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-bpe.dir/test-tokenizer-1-bpe.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-bpe\u001b[0m\n",
            "[ 64%] Built target test-tokenizer-1-bpe\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-tokenizer-1-spm.dir/test-tokenizer-1-spm.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-tokenizer-1-spm\u001b[0m\n",
            "[ 64%] Built target test-tokenizer-1-spm\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/test-chat-parser.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-parser\u001b[0m\n",
            "[ 65%] Built target test-chat-parser\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/test-chat-peg-parser.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat\u001b[0m\n",
            "[ 65%] Built target test-chat\n",
            "[ 65%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/test-chat-template.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-template.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-template\u001b[0m\n",
            "[ 66%] Built target test-chat-template\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-jinja.dir/test-jinja.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-chat-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-chat-peg-parser\u001b[0m\n",
            "[ 66%] Built target test-chat-peg-parser\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/test-json-partial.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-json-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-json-partial\u001b[0m\n",
            "[ 67%] Built target test-json-partial\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-jinja.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/test-log.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-log.dir/get-model.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-log\u001b[0m\n",
            "[ 67%] Built target test-log\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/test-peg-parser.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/simple-tokenize.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-basic.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-jinja\u001b[0m\n",
            "[ 68%] Built target test-jinja\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/test-regex-partial.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-gbnf-generation.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-regex-partial.dir/get-model.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-regex-partial\u001b[0m\n",
            "[ 69%] Built target test-regex-partial\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-parser.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/test-thread-safety.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-thread-safety.dir/get-model.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-thread-safety\u001b[0m\n",
            "[ 71%] Built target test-thread-safety\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-json-serialization.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/test-arg-parser.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-arg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-arg-parser\u001b[0m\n",
            "[ 72%] Built target test-arg-parser\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/peg-parser/test-unicode.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-peg-parser.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/test-opt.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-opt.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-opt\u001b[0m\n",
            "[ 72%] Built target test-opt\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/test-gguf.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-peg-parser\u001b[0m\n",
            "[ 72%] Built target test-peg-parser\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-gguf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/test-backend-ops.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-gguf\u001b[0m\n",
            "[ 72%] Built target test-gguf\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-ops.dir/get-model.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/test-model-load-cancel.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-model-load-cancel.dir/get-model.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-model-load-cancel\u001b[0m\n",
            "[ 73%] Built target test-model-load-cancel\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/test-autorelease.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-autorelease.dir/get-model.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-autorelease\u001b[0m\n",
            "[ 74%] Built target test-autorelease\n",
            "[ 74%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/test-backend-sampler.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-backend-sampler.dir/get-model.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-sampler\u001b[0m\n",
            "[ 75%] Built target test-backend-sampler\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/test-state-restore-fragmented.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-state-restore-fragmented.dir/get-model.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-state-restore-fragmented\u001b[0m\n",
            "[ 76%] Built target test-state-restore-fragmented\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/test-barrier.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-barrier.dir/get-model.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-barrier\u001b[0m\n",
            "[ 77%] Built target test-barrier\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/test-quantize-fns.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-fns.dir/get-model.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-fns\u001b[0m\n",
            "[ 78%] Built target test-quantize-fns\n",
            "[ 78%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/test-quantize-perf.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-quantize-perf.dir/get-model.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-quantize-perf\u001b[0m\n",
            "[ 79%] Built target test-quantize-perf\n",
            "[ 79%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/test-rope.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-rope.dir/get-model.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-rope\u001b[0m\n",
            "[ 80%] Built target test-rope\n",
            "[ 80%] \u001b[32mBuilding C object tests/CMakeFiles/test-mtmd-c-api.dir/test-mtmd-c-api.c.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-mtmd-c-api.dir/get-model.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-mtmd-c-api\u001b[0m\n",
            "[ 81%] Built target test-mtmd-c-api\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/test-alloc.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object tests/CMakeFiles/test-alloc.dir/get-model.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-alloc\u001b[0m\n",
            "[ 82%] Built target test-alloc\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/batched/CMakeFiles/llama-batched.dir/batched.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched\u001b[0m\n",
            "[ 82%] Built target llama-batched\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/debug/CMakeFiles/llama-debug.dir/debug.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-debug\u001b[0m\n",
            "[ 82%] Built target llama-debug\n",
            "[ 82%] \u001b[32mBuilding CXX object examples/embedding/CMakeFiles/llama-embedding.dir/embedding.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-embedding\u001b[0m\n",
            "[ 82%] Built target llama-embedding\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/eval-callback/CMakeFiles/llama-eval-callback.dir/eval-callback.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-eval-callback\u001b[0m\n",
            "[ 83%] Built target llama-eval-callback\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/idle/CMakeFiles/llama-idle.dir/idle.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../bin/test-backend-ops\u001b[0m\n",
            "[ 83%] Built target test-backend-ops\n",
            "[ 83%] \u001b[32mBuilding CXX object examples/lookahead/CMakeFiles/llama-lookahead.dir/lookahead.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-idle\u001b[0m\n",
            "[ 83%] Built target llama-idle\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup.dir/lookup.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookahead\u001b[0m\n",
            "[ 84%] Built target llama-lookahead\n",
            "[ 84%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-create.dir/lookup-create.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup\u001b[0m\n",
            "[ 84%] Built target llama-lookup\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-merge.dir/lookup-merge.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-create\u001b[0m\n",
            "[ 85%] Built target llama-lookup-create\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/lookup/CMakeFiles/llama-lookup-stats.dir/lookup-stats.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-merge\u001b[0m\n",
            "[ 85%] Built target llama-lookup-merge\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/parallel/CMakeFiles/llama-parallel.dir/parallel.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-lookup-stats\u001b[0m\n",
            "[ 85%] Built target llama-lookup-stats\n",
            "[ 85%] \u001b[32mBuilding CXX object examples/passkey/CMakeFiles/llama-passkey.dir/passkey.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-parallel\u001b[0m\n",
            "[ 86%] Built target llama-parallel\n",
            "[ 86%] \u001b[32mBuilding CXX object examples/retrieval/CMakeFiles/llama-retrieval.dir/retrieval.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-passkey\u001b[0m\n",
            "[ 86%] Built target llama-passkey\n",
            "[ 87%] \u001b[32mBuilding CXX object examples/save-load-state/CMakeFiles/llama-save-load-state.dir/save-load-state.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-retrieval\u001b[0m\n",
            "[ 87%] Built target llama-retrieval\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/speculative/CMakeFiles/llama-speculative.dir/speculative.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-save-load-state\u001b[0m\n",
            "[ 88%] Built target llama-save-load-state\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/speculative-simple/CMakeFiles/llama-speculative-simple.dir/speculative-simple.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative\u001b[0m\n",
            "[ 88%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-speculative-simple\u001b[0m\n",
            "[ 88%] Built target llama-speculative\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/gen-docs/CMakeFiles/llama-gen-docs.dir/gen-docs.cpp.o\u001b[0m\n",
            "[ 88%] Built target llama-speculative-simple\n",
            "[ 88%] \u001b[32mBuilding CXX object examples/training/CMakeFiles/llama-finetune.dir/finetune.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-finetune\u001b[0m\n",
            "[ 89%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gen-docs\u001b[0m\n",
            "[ 89%] Built target llama-finetune\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/diffusion/CMakeFiles/llama-diffusion-cli.dir/diffusion-cli.cpp.o\u001b[0m\n",
            "[ 90%] Built target llama-gen-docs\n",
            "[ 90%] \u001b[32mBuilding CXX object examples/convert-llama2c-to-ggml/CMakeFiles/llama-convert-llama2c-to-ggml.dir/convert-llama2c-to-ggml.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-convert-llama2c-to-ggml\u001b[0m\n",
            "[ 90%] Built target llama-convert-llama2c-to-ggml\n",
            "[ 90%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-vdot.dir/vdot.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-diffusion-cli\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-vdot\u001b[0m\n",
            "[ 91%] Built target llama-vdot\n",
            "[ 91%] \u001b[32mBuilding CXX object pocs/vdot/CMakeFiles/llama-q8dot.dir/q8dot.cpp.o\u001b[0m\n",
            "[ 91%] Built target llama-diffusion-cli\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/batched-bench/CMakeFiles/llama-batched-bench.dir/batched-bench.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-q8dot\u001b[0m\n",
            "[ 91%] Built target llama-q8dot\n",
            "[ 91%] \u001b[32mBuilding CXX object tools/gguf-split/CMakeFiles/llama-gguf-split.dir/gguf-split.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-batched-bench\u001b[0m\n",
            "[ 92%] Built target llama-batched-bench\n",
            "[ 92%] \u001b[32mBuilding CXX object tools/imatrix/CMakeFiles/llama-imatrix.dir/imatrix.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-gguf-split\u001b[0m\n",
            "[ 93%] Built target llama-gguf-split\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/llama-bench/CMakeFiles/llama-bench.dir/llama-bench.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-imatrix\u001b[0m\n",
            "[ 93%] Built target llama-imatrix\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/completion/CMakeFiles/llama-completion.dir/completion.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-bench\u001b[0m\n",
            "[ 93%] Built target llama-bench\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/perplexity/CMakeFiles/llama-perplexity.dir/perplexity.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-completion\u001b[0m\n",
            "[ 93%] Built target llama-completion\n",
            "[ 93%] \u001b[32mBuilding CXX object tools/quantize/CMakeFiles/llama-quantize.dir/quantize.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-quantize\u001b[0m\n",
            "[ 94%] Built target llama-quantize\n",
            "[ 94%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-task.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-perplexity\u001b[0m\n",
            "[ 95%] Built target llama-perplexity\n",
            "[ 95%] \u001b[32mBuilding CXX object tools/tokenize/CMakeFiles/llama-tokenize.dir/tokenize.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tokenize\u001b[0m\n",
            "[ 96%] Built target llama-tokenize\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-queue.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object tools/tts/CMakeFiles/llama-tts.dir/tts.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-common.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-tts\u001b[0m\n",
            "[ 97%] Built target llama-tts\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/server-context.dir/server-context.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object tools/mtmd/CMakeFiles/llama-mtmd-cli.dir/mtmd-cli.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-mtmd-cli\u001b[0m\n",
            "[ 97%] Built target llama-mtmd-cli\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cvector-generator/CMakeFiles/llama-cvector-generator.dir/cvector-generator.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cvector-generator\u001b[0m\n",
            "[ 98%] Built target llama-cvector-generator\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/export-lora/CMakeFiles/llama-export-lora.dir/export-lora.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-export-lora\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library libserver-context.a\u001b[0m\n",
            "[ 98%] Built target server-context\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/fit-params/CMakeFiles/llama-fit-params.dir/fit-params.cpp.o\u001b[0m\n",
            "[ 98%] Built target llama-export-lora\n",
            "[ 98%] \u001b[32mBuilding CXX object tools/cli/CMakeFiles/llama-cli.dir/cli.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-fit-params\u001b[0m\n",
            "[ 98%] Built target llama-fit-params\n",
            "[ 98%] \u001b[34m\u001b[1mGenerating loading.html.hpp\u001b[0m\n",
            "[ 98%] \u001b[34m\u001b[1mGenerating index.html.gz.hpp\u001b[0m\n",
            "[ 99%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-cli\u001b[0m\n",
            "[100%] Built target llama-cli\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-http.cpp.o\u001b[0m\n",
            "[100%] \u001b[32mBuilding CXX object tools/server/CMakeFiles/llama-server.dir/server-models.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/llama-server\u001b[0m\n",
            "[100%] Built target llama-server\n",
            "main: build = 8054 (01d8eaa28)\n",
            "main: built with GNU 11.4.0 for Linux x86_64\n",
            "main: quantizing '/content/temp_f16.gguf' to '/content/gemma_3n_q4_k_m.gguf' as Q4_K_M\n",
            "gguf_init_from_file: failed to open GGUF file '/content/temp_f16.gguf' (No such file or directory)\n",
            "llama_model_quantize: failed to quantize: llama_model_loader: failed to load model from /content/temp_f16.gguf\n",
            "main: failed to quantize model from '/content/temp_f16.gguf'\n",
            "ls: cannot access '/content/gemma_3n_q4_k_m.gguf': No such file or directory\n",
            "cp: cannot stat '/content/gemma_3n_q4_k_m.gguf': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i5enT8ZGE6GX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. –ü–†–û–í–ï–†–¨–¢–ï —á—Ç–æ –µ—Å—Ç—å\n",
        "!ls -la /content/merged_model_hf/\n",
        "!ls -la /content/llama.cpp/build/bin/llama-quantize\n",
        "\n",
        "# 2. –î–æ–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–π—Ç–µ (5-10 –º–∏–Ω—É—Ç, –Ω–µ –ø—Ä–µ—Ä—ã–≤–∞–π—Ç–µ!)\n",
        "%cd /content/llama.cpp\n",
        "!python convert_hf_to_gguf.py /content/merged_model_hf \\\n",
        "    --outfile /content/temp_f16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "# 3. –ö–≤–∞–Ω—Ç—É–π—Ç–µ (2-3 –º–∏–Ω—É—Ç—ã)\n",
        "!./build/bin/llama-quantize /content/temp_f16.gguf /content/gemma_3n_q4_k_m.gguf Q4_K_M\n",
        "\n",
        "# 4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç\n",
        "!ls -lh /content/gemma_3n_q4_k_m.gguf\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aqcSxz_D3wq",
        "outputId": "efb220a7-e6c1-44e9-c5e3-8ffe5b10b3ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 15370852\n",
            "drwxr-xr-x 3 root root       4096 Feb 14 19:44 .\n",
            "drwxr-xr-x 1 root root       4096 Feb 14 20:21 ..\n",
            "drwxr-xr-x 3 root root       4096 Feb 14 19:38 .cache\n",
            "-rw-r--r-- 1 root root       1626 Feb 14 19:38 chat_template.jinja\n",
            "-rw-r--r-- 1 root root       5675 Feb 14 19:38 config.json\n",
            "-rw-r--r-- 1 root root 3077103824 Feb 14 19:45 model-00001-of-00004.safetensors\n",
            "-rw-r--r-- 1 root root 4966792808 Feb 14 19:55 model-00002-of-00004.safetensors\n",
            "-rw-r--r-- 1 root root 4992870216 Feb 14 19:57 model-00003-of-00004.safetensors\n",
            "-rw-r--r-- 1 root root 2663414864 Feb 14 19:58 model-00004-of-00004.safetensors\n",
            "-rw-r--r-- 1 root root     171459 Feb 14 19:38 model.safetensors.index.json\n",
            "-rw-r--r-- 1 root root        777 Feb 14 19:38 special_tokens_map.json\n",
            "-rw-r--r-- 1 root root    1204203 Feb 14 19:38 tokenizer_config.json\n",
            "-rw-r--r-- 1 root root   33442807 Feb 14 19:38 tokenizer.json\n",
            "-rw-r--r-- 1 root root    4696020 Feb 14 19:38 tokenizer.model\n",
            "ls: cannot access '/content/llama.cpp/build/bin/llama-quantize': No such file or directory\n",
            "/content/llama.cpp\n",
            "INFO:hf-to-gguf:Loading model: merged_model_hf\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/merged_model_hf: Could not import module 'Gemma3nConfig'. Are this object's requirements defined correctly?\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/merged_model_hf: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00004.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> F16, shape = {2048, 262400}\n",
            "INFO:hf-to-gguf:Padding per-layer embeddings shape [262144, 8960] from 262144 to 262400 (adding 256 vision/audio token slots)\n",
            "^C\n",
            "/bin/bash: line 1: ./build/bin/llama-quantize: No such file or directory\n",
            "ls: cannot access '/content/gemma_3n_q4_k_m.gguf': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vTVjN2taMk3B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2534711a-341e-4b37-a299-3c9a9cb7b794"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/llama.cpp/llama.cpp/convert_hf_to_gguf.py': [Errno 2] No such file or directory\n",
            "\n",
            " ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤  GGUF –∑–∞–≤–µ—Ä—à–µ–Ω–∞\n",
            "‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª FP16 –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω.\n",
            "‚ùå –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.\n"
          ]
        }
      ],
      "source": [
        "# –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è HF -> GGUF\n",
        "# –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—É—é –Ω–∞ –ø—Ä–æ—à–ª–æ–º —à–∞–≥–µ –ø–∞–ø–∫—É merged_model_hf\n",
        "\n",
        "!python llama.cpp/convert_hf_to_gguf.py /content/merged_model_hf \\\n",
        "    --outfile /content/temp_f16.gguf \\\n",
        "    --outtype f16\n",
        "\n",
        "print(\"\\n ‚úÖ –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤  GGUF –∑–∞–≤–µ—Ä—à–µ–Ω–∞\")\n",
        "\n",
        "# –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –¥–æ Q4_K_M\n",
        "if os.path.exists(\"/content/temp_f16.gguf\"):\n",
        "\n",
        "    !./llama.cpp/llama-quantize /content/temp_f16.gguf /content/gemma_3n_q4_k_m.gguf q4_k_m\n",
        "\n",
        "    print(\"\\n‚úÖ –ö–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!\")\n",
        "\n",
        "    # —É–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª\n",
        "    !rm /content/temp_f16.gguf\n",
        "else:\n",
        "    print(\"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª FP16 –Ω–µ –±—ã–ª —Å–æ–∑–¥–∞–Ω.\")\n",
        "\n",
        "# —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –Ω–∞ Google Drive\n",
        "if os.path.exists(\"/content/gemma_3n_q4_k_m.gguf\"):\n",
        "    dest_path = \"/content/drive/MyDrive/gemma_3n_q4_k_m.gguf\"\n",
        "    !cp /content/gemma_3n_q4_k_m.gguf \"{dest_path}\"\n",
        "    print(f\"üéâ –ì–æ—Ç–æ–≤–æ! –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω –Ω–∞ Google drive: {dest_path}\")\n",
        "else:\n",
        "    print(\"‚ùå –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. –ù–ê–ô–î–ò–¢–ï llama-quantize (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –ø—É—Ç—å)\n",
        "!find /content/llama.cpp -name \"llama-quantize\" -type f\n",
        "!ls -la /content/llama.cpp/bin/llama-quantize || echo \"–ù–ï–¢ –≤ bin/\"\n",
        "!ls -la /content/llama.cpp/llama.cpp/build/bin/llama-quantize || echo \"–ù–ï–¢ –≤ llama.cpp/build/bin/\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3QFdaHTHFCRC",
        "outputId": "a5eb98da-0ede-4441-c2fd-add3cec6a375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp/llama.cpp/build/bin/llama-quantize\n",
            "ls: cannot access '/content/llama.cpp/bin/llama-quantize': No such file or directory\n",
            "–ù–ï–¢ –≤ bin/\n",
            "-rwxr-xr-x 1 root root 442512 Feb 14 21:26 /content/llama.cpp/llama.cpp/build/bin/llama-quantize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è\n",
        "%cd /content/llama.cpp\n",
        "!python convert_hf_to_gguf.py /content/merged_model_hf \\\n",
        "    --outfile /content/temp_f16.gguf \\\n",
        "    --outtype f16\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2MeNlhXF2LB",
        "outputId": "2214d573-a44d-43eb-ab46-e559f9cc9f1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/llama.cpp\n",
            "INFO:hf-to-gguf:Loading model: merged_model_hf\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/merged_model_hf: Could not import module 'Gemma3nConfig'. Are this object's requirements defined correctly?\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:Model architecture: Gemma3nForConditionalGeneration\n",
            "WARNING:hf-to-gguf:Failed to load model config from /content/merged_model_hf: partially initialized module 'torchvision' has no attribute 'extension' (most likely due to a circular import)\n",
            "WARNING:hf-to-gguf:Trying to load config.json instead\n",
            "INFO:hf-to-gguf:gguf: loading model weight map from 'model.safetensors.index.json'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00001-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00002-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00003-of-00004.safetensors'\n",
            "INFO:hf-to-gguf:gguf: indexing model part 'model-00004-of-00004.safetensors'\n",
            "INFO:gguf.gguf_writer:gguf: This GGUF file is for Little Endian only\n",
            "INFO:hf-to-gguf:Exporting model...\n",
            "INFO:hf-to-gguf:altup_proj.weight,                 torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:altup_unembd_proj.weight,          torch.bfloat16 --> F16, shape = {2048, 2048, 3}\n",
            "INFO:hf-to-gguf:token_embd.weight,                 torch.bfloat16 --> F16, shape = {2048, 262400}\n",
            "INFO:hf-to-gguf:Padding per-layer embeddings shape [262144, 8960] from 262144 to 262400 (adding 256 vision/audio token slots)\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "# –°–∫–∞—á–∏–≤–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã\n",
        "files_list = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
        "\n",
        "for filename in files_list:\n",
        "    full_path = f'/content/merged_model_hf/{filename}'\n",
        "    print(f\"–°–∫–∞—á–∏–≤–∞—é: {filename}\")\n",
        "    files.download(full_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "mP30H_FBH4nZ",
        "outputId": "21ec6e8c-16f8-479f-cb49-6079d0ac426f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "–°–∫–∞—á–∏–≤–∞—é: model-00001-of-00004.safetensors\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_8e09eb52-f7d9-4c0f-856c-3c3dadbff90a\", \"model-00001-of-00004.safetensors\", 3077103824)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –õ–æ–∫–∞–ª—å–Ω–æ\n",
        "git clone https://github.com/ggerganov/llama.cpp\n",
        "cd llama.cpp\n",
        "cmake -B build -DGGML_CUDA=1\n",
        "cmake --build build --config Release\n",
        "python convert_hf_to_gguf.py merged_model_hf --outfile gemma_lora_f16.gguf --outtype f16\n",
        "./build/bin/llama-quantize gemma_lora_f16.gguf gemma_lora_q4_k_m.gguf Q4_K_M\n"
      ],
      "metadata": {
        "id": "MQUFOc72Gxdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "1kQKMuJMstIz",
        "outputId": "226f53ae-0877-47ee-a49a-ebd649a5e03d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2364305434.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000_000_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "for x in range(1000_000_000):\n",
        "    print(x)\n",
        "    time.sleep(10)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "crOIizp8nNI-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cbu7FWxpKI5F",
        "duBX6ZuEzqPU",
        "Kp8SmJ_Q4ODb",
        "G-sx7duG4oHZ",
        "YTunD9Ag5NxR",
        "gMOQTd3oETPc",
        "p9lHo4DTzUQf",
        "4P3il7LrIxU7",
        "gWe6JK4HDs39",
        "8ex25OVXEOaY",
        "5oM2xecvERu6",
        "l8pchxNNwol0",
        "Ggd2in7wJiPP",
        "9yzgSXkJ2iap",
        "3SPh1mGUAU3-",
        "1baKDy2rAd3n",
        "YJGmBy8fA1IA",
        "a4d1UaM7CXht",
        "fKRubS-V0Wh-",
        "L3RkHwapCpg5",
        "x2p87Uaw2mvx",
        "MGPBxYlGxCwI",
        "L4798_2DxNbk",
        "yrUgrZD2qL2s",
        "bpO7cieAxvbC",
        "rSbPYWLm2xRw",
        "1xTMxJXg2ot_",
        "MICXrf4uqdsc",
        "zH58gvL5zKfC",
        "DNXMMZiu46CA",
        "T6eOJOoW3smT",
        "ufj1oIsX40we",
        "Qzd7NZT96SHR",
        "Hs-xkBa44BK-",
        "OIG1RVmPAEc-",
        "4iS75AG-DSdN",
        "KeBjJkkdwjMG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b50ac657bd7848db9969c647f364854e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_062590f3776843db80ed44fdc3cc3172",
              "IPY_MODEL_14fb10a1376f460ca8a60c2a39583703",
              "IPY_MODEL_8cbf15c6f1bb412dbe05781b388ad27a"
            ],
            "layout": "IPY_MODEL_ee4f6bf36b324e2fbfe301ce367bfa35"
          }
        },
        "062590f3776843db80ed44fdc3cc3172": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94e3ff65e7a24c568b8d6717c62686ae",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_f8f0cafd634c420db0e52421922da3df",
            "value": "model.safetensors.index.json:‚Äá"
          }
        },
        "14fb10a1376f460ca8a60c2a39583703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c9fa4ece813a46eab3d5eb6b32d268e1",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb76967345114dd7aa655018275b1251",
            "value": 1
          }
        },
        "8cbf15c6f1bb412dbe05781b388ad27a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_068be1900ccc4b29b3d44785bb321a32",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_db4ab5e47d5945bda061930877e82cd3",
            "value": "‚Äá370k/?‚Äá[00:00&lt;00:00,‚Äá23.9MB/s]"
          }
        },
        "ee4f6bf36b324e2fbfe301ce367bfa35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e3ff65e7a24c568b8d6717c62686ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8f0cafd634c420db0e52421922da3df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9fa4ece813a46eab3d5eb6b32d268e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "fb76967345114dd7aa655018275b1251": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "068be1900ccc4b29b3d44785bb321a32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db4ab5e47d5945bda061930877e82cd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3188129ddcc54b22a58f95d273b07bca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9058b56241ae4a29a41fc678dca46477",
              "IPY_MODEL_7082cca5bdcc47a197ae3c56f474635c",
              "IPY_MODEL_1aded047af7b4239935790258321383c"
            ],
            "layout": "IPY_MODEL_d800da40261b405ea2e0f4b08b9d57b4"
          }
        },
        "9058b56241ae4a29a41fc678dca46477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8e5fc0c9d304d4c84c7e9dee9b8d37e",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_cdbce73e904c4604aa45054e4ae44df3",
            "value": "model-00001-of-00003.safetensors:‚Äá100%"
          }
        },
        "7082cca5bdcc47a197ae3c56f474635c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94063b867d6b47fc871238fe8e0bfa81",
            "max": 3723417614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e45deed2cfc4340a6958c58fecd9de5",
            "value": 3723417614
          }
        },
        "1aded047af7b4239935790258321383c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3db997eff69b4fb89a3707d8dd566501",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b10df76796c24478b9b9c66c228b769e",
            "value": "‚Äá3.72G/3.72G‚Äá[00:36&lt;00:00,‚Äá168MB/s]"
          }
        },
        "d800da40261b405ea2e0f4b08b9d57b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8e5fc0c9d304d4c84c7e9dee9b8d37e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdbce73e904c4604aa45054e4ae44df3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "94063b867d6b47fc871238fe8e0bfa81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e45deed2cfc4340a6958c58fecd9de5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3db997eff69b4fb89a3707d8dd566501": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b10df76796c24478b9b9c66c228b769e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b27f7bffb5dd40e0a1374dd05c88901d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2816aadcabc04171aec80edd533cbb71",
              "IPY_MODEL_0b225bd4e44e41a09678a385bc0e5e62",
              "IPY_MODEL_bde32e0898f84cfb8bc1864978423da1"
            ],
            "layout": "IPY_MODEL_ddc00b8f44514d02acf52cdbedd0144d"
          }
        },
        "2816aadcabc04171aec80edd533cbb71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f22c21b130a8452a9e4cc3fc979c7eea",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b9595b893e7b4c06a99380f1a23bc3bc",
            "value": "model-00002-of-00003.safetensors:‚Äá100%"
          }
        },
        "0b225bd4e44e41a09678a385bc0e5e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9964b21af35f4389988a50301a24ac2b",
            "max": 4987233092,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fa4761fd2b6b4dccb3dc1c775b9beff2",
            "value": 4987233092
          }
        },
        "bde32e0898f84cfb8bc1864978423da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_61f25ba220624998858b286b36c35b24",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e22d72e3c05d488890aefa3f94d72a41",
            "value": "‚Äá4.99G/4.99G‚Äá[01:34&lt;00:00,‚Äá191MB/s]"
          }
        },
        "ddc00b8f44514d02acf52cdbedd0144d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f22c21b130a8452a9e4cc3fc979c7eea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9595b893e7b4c06a99380f1a23bc3bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9964b21af35f4389988a50301a24ac2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa4761fd2b6b4dccb3dc1c775b9beff2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "61f25ba220624998858b286b36c35b24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e22d72e3c05d488890aefa3f94d72a41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdd0b3945c8245c6ade27e98ead02334": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c943f7b3c614f5c9ef0ce2078ec5965",
              "IPY_MODEL_e369d09753a442e5a325017a6395e1ca",
              "IPY_MODEL_34e6e32aabf1492aa6f986bf4417af97"
            ],
            "layout": "IPY_MODEL_4d76dce8462a42e892c69049312ed3bb"
          }
        },
        "9c943f7b3c614f5c9ef0ce2078ec5965": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08dfbcdc92464fdd8b94c079306bc185",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_7b4fb488bf744c97ab5debc69980cc2f",
            "value": "model-00003-of-00003.safetensors:‚Äá100%"
          }
        },
        "e369d09753a442e5a325017a6395e1ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ffe902acc2b49ad8f555a7861b1b12f",
            "max": 1148535480,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2685a01b17e47e29bf5dc80efec3b80",
            "value": 1148535480
          }
        },
        "34e6e32aabf1492aa6f986bf4417af97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4f175fdae264610a541bcc81788c713",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_29940884369b4595a1038c4cb7450844",
            "value": "‚Äá1.15G/1.15G‚Äá[00:16&lt;00:00,‚Äá161MB/s]"
          }
        },
        "4d76dce8462a42e892c69049312ed3bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08dfbcdc92464fdd8b94c079306bc185": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b4fb488bf744c97ab5debc69980cc2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ffe902acc2b49ad8f555a7861b1b12f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2685a01b17e47e29bf5dc80efec3b80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e4f175fdae264610a541bcc81788c713": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "29940884369b4595a1038c4cb7450844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6df7de7827284e0982dd5dd88824a309": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_415e4adff8f04bae848512a5ec77c05c",
              "IPY_MODEL_af158ee9538849418306eca9d6865470",
              "IPY_MODEL_20a2c9acc980416c9a066331a817ad8a"
            ],
            "layout": "IPY_MODEL_72ecd370cdd04d5096db7baab1a1f65b"
          }
        },
        "415e4adff8f04bae848512a5ec77c05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4256aeec48e45ff8b8c28e1df5cab4b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_243f22d3e8954ad49de2e7de4104014b",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "af158ee9538849418306eca9d6865470": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36fabc6d0dac48f6b0c7f061ad31e03c",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ab079e6c8020496b979db34e608c4192",
            "value": 3
          }
        },
        "20a2c9acc980416c9a066331a817ad8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc11d58fcfb64cfea628a33ab20ff228",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d4066ed306b04abdbd00ca55857d27ee",
            "value": "‚Äá3/3‚Äá[00:46&lt;00:00,‚Äá13.34s/it]"
          }
        },
        "72ecd370cdd04d5096db7baab1a1f65b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4256aeec48e45ff8b8c28e1df5cab4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "243f22d3e8954ad49de2e7de4104014b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36fabc6d0dac48f6b0c7f061ad31e03c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab079e6c8020496b979db34e608c4192": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc11d58fcfb64cfea628a33ab20ff228": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4066ed306b04abdbd00ca55857d27ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cd928ca1cf05479c8f620de3792ccfb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e99d9ffa12ff4f5792f6435169d0b901",
              "IPY_MODEL_5fe23b2479264013ba326c3213110a45",
              "IPY_MODEL_5c5761561e67426f9e02f1874cda10f1"
            ],
            "layout": "IPY_MODEL_5efbe5baca934d5da95520ad395fb8c0"
          }
        },
        "e99d9ffa12ff4f5792f6435169d0b901": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6aa50b6858e4869b30e8c92987d34e8",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_59b028f5e37f4ea0878c57a7bc6ab7f1",
            "value": "generation_config.json:‚Äá100%"
          }
        },
        "5fe23b2479264013ba326c3213110a45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0b79847c4ce4265969b058dbba37b60",
            "max": 210,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c426a901bdea47e8b0b72ce517a2e681",
            "value": 210
          }
        },
        "5c5761561e67426f9e02f1874cda10f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b38e5977fee04b1aadac18008689aac0",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_9a2f85d12a854a6faa5a8f450b9fc97e",
            "value": "‚Äá210/210‚Äá[00:00&lt;00:00,‚Äá22.8kB/s]"
          }
        },
        "5efbe5baca934d5da95520ad395fb8c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a6aa50b6858e4869b30e8c92987d34e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59b028f5e37f4ea0878c57a7bc6ab7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0b79847c4ce4265969b058dbba37b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c426a901bdea47e8b0b72ce517a2e681": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b38e5977fee04b1aadac18008689aac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2f85d12a854a6faa5a8f450b9fc97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "989f8623c1f44159aaa093093faecac4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f36012e991114623929a36d001b6d04c",
              "IPY_MODEL_ec8c96eeca2f4be7aaea3ca16507fade",
              "IPY_MODEL_fc1b07ec1c5f4d79bdad01ed6de7d5e5"
            ],
            "layout": "IPY_MODEL_62bfcd76e5a94257b00f1fada4930140"
          }
        },
        "f36012e991114623929a36d001b6d04c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56db82b0fe7e4fdeab78fe1d98ea7794",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_dad8f526ca4d449caa9c201767fd2d98",
            "value": "config.json:‚Äá"
          }
        },
        "ec8c96eeca2f4be7aaea3ca16507fade": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91fac95d702b435ea66327fdb26dc0fb",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5c3e3b7227542d0a7966a5f2157bbc8",
            "value": 1
          }
        },
        "fc1b07ec1c5f4d79bdad01ed6de7d5e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_447bd7d28c73467182c7c3c069682a8a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_04b833e37b484a97a24d72265c4b914a",
            "value": "‚Äá4.58k/?‚Äá[00:00&lt;00:00,‚Äá388kB/s]"
          }
        },
        "62bfcd76e5a94257b00f1fada4930140": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56db82b0fe7e4fdeab78fe1d98ea7794": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad8f526ca4d449caa9c201767fd2d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91fac95d702b435ea66327fdb26dc0fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "b5c3e3b7227542d0a7966a5f2157bbc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "447bd7d28c73467182c7c3c069682a8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b833e37b484a97a24d72265c4b914a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ac8c06ccae7a4ad7b04febf8eb87ae9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e9cba4c3004e4daaa1000f6342808318",
              "IPY_MODEL_f6d6fcfbceed4473b984e3fdc0ca2f0c",
              "IPY_MODEL_c10937ccf14a4051bb8f3bf4184774ec"
            ],
            "layout": "IPY_MODEL_305e1358dcae4bc89e047bdbac0c69e4"
          }
        },
        "e9cba4c3004e4daaa1000f6342808318": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc546d70aaef4c15abdc7606b4057bd9",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_726f201fe289464382057b7717821466",
            "value": "model.safetensors.index.json:‚Äá"
          }
        },
        "f6d6fcfbceed4473b984e3fdc0ca2f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5b9cd169f70432ea6c4b0b22686b6de",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad165276183046b8bfe5598bbcde678c",
            "value": 1
          }
        },
        "c10937ccf14a4051bb8f3bf4184774ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_112531bded4f4dbd942f53bb3b842782",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5a2c5979d65c4af3bf30e41db3b381dd",
            "value": "‚Äá171k/?‚Äá[00:00&lt;00:00,‚Äá9.54MB/s]"
          }
        },
        "305e1358dcae4bc89e047bdbac0c69e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc546d70aaef4c15abdc7606b4057bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726f201fe289464382057b7717821466": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5b9cd169f70432ea6c4b0b22686b6de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "ad165276183046b8bfe5598bbcde678c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "112531bded4f4dbd942f53bb3b842782": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a2c5979d65c4af3bf30e41db3b381dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8935921a7d174f829133f36d3323ac61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5ccd80cf8c954f109b3116238acecd66",
              "IPY_MODEL_dd23adb0360c45e69f97cfad3d805cfd",
              "IPY_MODEL_d5972f21d2b045b8b0d726a975798b4b"
            ],
            "layout": "IPY_MODEL_eb40716f6e9543058b90c99b181b2ce9"
          }
        },
        "5ccd80cf8c954f109b3116238acecd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a5a268473df5409fa8c7f928d8cc64c4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_807a1b1302074522842c6e95dd684024",
            "value": "model-00001-of-00004.safetensors:‚Äá100%"
          }
        },
        "dd23adb0360c45e69f97cfad3d805cfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fee2fc439454d4c9b563a87eb55ce0f",
            "max": 3077103824,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9263c148bc7a4edeb7bfec04aa8d4819",
            "value": 3077103824
          }
        },
        "d5972f21d2b045b8b0d726a975798b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d32886d2434db2be797f0eb5c0d834",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_478ed749673045d097d7335717e5fa35",
            "value": "‚Äá3.08G/3.08G‚Äá[01:20&lt;00:00,‚Äá34.1MB/s]"
          }
        },
        "eb40716f6e9543058b90c99b181b2ce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5a268473df5409fa8c7f928d8cc64c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "807a1b1302074522842c6e95dd684024": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fee2fc439454d4c9b563a87eb55ce0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9263c148bc7a4edeb7bfec04aa8d4819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a1d32886d2434db2be797f0eb5c0d834": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "478ed749673045d097d7335717e5fa35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5be77fe360c34ddc931c22251d2f12ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63837f4f9ca04af088a43c173f4b3796",
              "IPY_MODEL_003636ef937a4e4c909e0400ca1f54bf",
              "IPY_MODEL_d7fa80fe05424038a1992cfadad8844f"
            ],
            "layout": "IPY_MODEL_1ccf7bad6b2c4b2d804371ed1d0bafff"
          }
        },
        "63837f4f9ca04af088a43c173f4b3796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fa8cc2a5091f410992f8e593bcc6ed46",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_01b1fb495bc548b182c493f5c4e28840",
            "value": "model-00002-of-00004.safetensors:‚Äá100%"
          }
        },
        "003636ef937a4e4c909e0400ca1f54bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658d9514c1544e34a84e20fad8ca42f8",
            "max": 4966792808,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e1e02b60a018470fac8b2b5174e13b1e",
            "value": 4966792808
          }
        },
        "d7fa80fe05424038a1992cfadad8844f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bf0add0526b42ac833a964cea2c25cb",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_0bd8f576eaca4f71a0b9c8c7576da471",
            "value": "‚Äá4.97G/4.97G‚Äá[02:25&lt;00:00,‚Äá41.7MB/s]"
          }
        },
        "1ccf7bad6b2c4b2d804371ed1d0bafff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa8cc2a5091f410992f8e593bcc6ed46": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01b1fb495bc548b182c493f5c4e28840": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "658d9514c1544e34a84e20fad8ca42f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e1e02b60a018470fac8b2b5174e13b1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5bf0add0526b42ac833a964cea2c25cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bd8f576eaca4f71a0b9c8c7576da471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68af71c7d8384349bf9deeb31c6db8a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2933df4e36b8467b932bc1fdf88f1c5d",
              "IPY_MODEL_46823449e6fd4e19b700dd270e3b2f0b",
              "IPY_MODEL_66bdb644305040e08dca7b897c8e6cf6"
            ],
            "layout": "IPY_MODEL_5e434fb4556346029dfbcdc9bdbb6085"
          }
        },
        "2933df4e36b8467b932bc1fdf88f1c5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27b0ebd011a4395ade0a503225aae05",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_885e02b835694d43b9fb2838e0083767",
            "value": "model-00003-of-00004.safetensors:‚Äá100%"
          }
        },
        "46823449e6fd4e19b700dd270e3b2f0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56519b1506e54b35bc148d5acf3f8ff0",
            "max": 4992870216,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a729a5a1bf614d00a0c23276566956e9",
            "value": 4992870216
          }
        },
        "66bdb644305040e08dca7b897c8e6cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_47591f48444a43f4be1dfbef22753007",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_1adb76b10d6f45389909bf9f53f8fd10",
            "value": "‚Äá4.99G/4.99G‚Äá[01:31&lt;00:00,‚Äá28.7MB/s]"
          }
        },
        "5e434fb4556346029dfbcdc9bdbb6085": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27b0ebd011a4395ade0a503225aae05": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "885e02b835694d43b9fb2838e0083767": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56519b1506e54b35bc148d5acf3f8ff0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a729a5a1bf614d00a0c23276566956e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "47591f48444a43f4be1dfbef22753007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1adb76b10d6f45389909bf9f53f8fd10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e45b057c4a724f949e59da20c8d2708d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83b12a5c712e4337a82ae532b3ff470c",
              "IPY_MODEL_85f8fb6af99f4a7d91699d625a8abbaf",
              "IPY_MODEL_756e574d780c44b2b7f2248e7c0d6b3f"
            ],
            "layout": "IPY_MODEL_2411ba645082439681ee45799b9bfc69"
          }
        },
        "83b12a5c712e4337a82ae532b3ff470c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b68d5dc3fd46432aa81013e29fafaee4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_6ef6578764a54b0d9c811ce68ad1acd7",
            "value": "model-00004-of-00004.safetensors:‚Äá100%"
          }
        },
        "85f8fb6af99f4a7d91699d625a8abbaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8943f35c5d44cfc9001a4fc429bc765",
            "max": 2663414864,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_45811cbf4a35413598215840bbda951c",
            "value": 2663414864
          }
        },
        "756e574d780c44b2b7f2248e7c0d6b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_813474887c264c58a9821a63ba2d2b5a",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_16c8d403216f480a88126768f49249aa",
            "value": "‚Äá2.66G/2.66G‚Äá[00:30&lt;00:00,‚Äá81.1MB/s]"
          }
        },
        "2411ba645082439681ee45799b9bfc69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b68d5dc3fd46432aa81013e29fafaee4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ef6578764a54b0d9c811ce68ad1acd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e8943f35c5d44cfc9001a4fc429bc765": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45811cbf4a35413598215840bbda951c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "813474887c264c58a9821a63ba2d2b5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16c8d403216f480a88126768f49249aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}